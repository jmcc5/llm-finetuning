{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment\\nLogic: train model: 1 epoch, training dataset: 4096, eval datasets obtained thru select random subset'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"experiment\n",
    "Logic: train model: 1 epoch, training dataset: 4096, eval datasets obtained thru select random subset\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In domain:\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 261802\n",
      "})\n",
      "{'premise': 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him', 'hypothesis': 'You lose the things to the following level if the people recall.', 'label': 0, 'idx': 1}\n",
      "Out of domain:\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'premise': 'The president avoided the athlete .', 'hypothesis': 'The athlete avoided the president .', 'label': 1, 'parse_premise': '(ROOT (S (NP (DT The) (NN president)) (VP (VBD avoided) (NP (DT the) (NN athlete))) (. .)))', 'parse_hypothesis': '(ROOT (S (NP (DT The) (NN athlete)) (VP (VBD avoided) (NP (DT the) (NN president))) (. .)))', 'binary_parse_premise': '( ( The president ) ( ( avoided ( the athlete ) ) . ) )', 'binary_parse_hypothesis': '( ( The athlete ) ( ( avoided ( the president ) ) . ) )', 'heuristic': 'lexical_overlap', 'subcase': 'ln_subject/object_swap', 'template': 'temp1'}\n"
     ]
    }
   ],
   "source": [
    "from src.data.data import get_in_domain, get_out_domain\n",
    "\n",
    "in_domain_train, in_domain_test = get_in_domain()\n",
    "out_domain = get_out_domain()\n",
    "\n",
    "print(f\"In domain:\\n{in_domain_train}\")\n",
    "print(in_domain_train[0])\n",
    "\n",
    "print(f\"Out of domain:\\n{out_domain}\")\n",
    "print(out_domain[10])\n",
    "\n",
    "# get_random_subsets(in_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe0c99f21ca4c81bd8b2ef1220659c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 16-shot:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baa9f7d642b4d3f91e77f9aeb214678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 4096-shot:   0%|          | 0/2049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m      5\u001b[0m train_datasets, eval_dataset_in, eval_dataset_out \u001b[38;5;241m=\u001b[39m get_random_subsets(train_dataset\u001b[38;5;241m=\u001b[39min_domain_train, \n\u001b[0;32m      6\u001b[0m                                                                        eval_dataset_in\u001b[38;5;241m=\u001b[39min_domain_test, \n\u001b[0;32m      7\u001b[0m                                                                        eval_dataset_out\u001b[38;5;241m=\u001b[39mout_domain, \n\u001b[0;32m      8\u001b[0m                                                                        train_sample_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m16\u001b[39m],\n\u001b[0;32m      9\u001b[0m                                                                        num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,   \u001b[38;5;66;03m# 5\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                                                                        eval_sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     12\u001b[0m context_distillation_datasets \u001b[38;5;241m=\u001b[39m get_context_distillation_datasets(dataset\u001b[38;5;241m=\u001b[39min_domain_train,\n\u001b[0;32m     13\u001b[0m                                                                   train_datasets\u001b[38;5;241m=\u001b[39mtrain_datasets,\n\u001b[0;32m     14\u001b[0m                                                                   fewshot_sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     15\u001b[0m                                                                   large_sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mbatch_context_distillation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopt-350m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                           \u001b[49m\u001b[43min_domain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_domain_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_distillation_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                           \u001b[49m\u001b[43meval_dataset_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                           \u001b[49m\u001b[43meval_dataset_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mexp_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\context_distillation.py:45\u001b[0m, in \u001b[0;36mbatch_context_distillation\u001b[1;34m(model_names, in_domain_dataset, train_datasets, eval_dataset_in, eval_dataset_out, batch_size, exp_label)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m teacher_model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m     43\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m metrics_trial \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_distillation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_domain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43meval_dataset_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43meval_dataset_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m metrics_trial \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name,\n\u001b[0;32m     57\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m'\u001b[39m: sample_size,\n\u001b[0;32m     58\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics_trial}\n\u001b[0;32m     59\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(metrics_trial)\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\context_distillation.py:122\u001b[0m, in \u001b[0;36mcontext_distillation\u001b[1;34m(student_model, teacher_model, tokenizer, dataset, train_dataset, num_epochs, eval_dataset_in, eval_dataset_out, model_name, batch_size)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_dataset\u001b[39m(model, tokenizer, dataset, batch_size):\n\u001b[1;32m--> 122\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    123\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    124\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.data.data import get_random_subsets, get_context_distillation_datasets\n",
    "from src.finetuners.context_distillation_recursive import batch_recursive_context_distillation\n",
    "from src.finetuners.context_distillation import batch_context_distillation\n",
    "\n",
    "train_datasets, eval_dataset_in, eval_dataset_out = get_random_subsets(train_dataset=in_domain_train, \n",
    "                                                                       eval_dataset_in=in_domain_test, \n",
    "                                                                       eval_dataset_out=out_domain, \n",
    "                                                                       train_sample_sizes=[16],\n",
    "                                                                       num_trials=10,   # 5\n",
    "                                                                       eval_sample_size=50)\n",
    "\n",
    "context_distillation_datasets = get_context_distillation_datasets(dataset=in_domain_train,\n",
    "                                                                  train_datasets=train_datasets,\n",
    "                                                                  fewshot_sample_size=16,\n",
    "                                                                  large_sample_size=4096)\n",
    "\n",
    "batch_context_distillation(model_names=['opt-350m'],\n",
    "                           in_domain_dataset=in_domain_train,\n",
    "                           train_datasets=context_distillation_datasets,\n",
    "                           eval_dataset_in=eval_dataset_in,\n",
    "                           eval_dataset_out=eval_dataset_out,\n",
    "                           batch_size=2,\n",
    "                           exp_label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.model import get_model, download_model\n",
    "\n",
    "# Get student models\n",
    "student_model_opt125_causal, tokenizer_opt125_causal = get_model(model_name='opt-125m', model_type='CausalLM', pretrained=True)\n",
    "student_model_opt350_causal, tokenizer_opt350_causal = get_model(model_name='opt-350m', model_type='CausalLM', pretrained=True)\n",
    "\n",
    "# Get teacher models\n",
    "teacher_model_opt125_causal, tokenizer_opt125_causal = get_model(model_name='opt-125m', model_type='CausalLM', pretrained=True)\n",
    "teacher_model_opt350_causal, tokenizer_opt350_causal = get_model(model_name='opt-350m', model_type='CausalLM', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.finetuners.fewshot import batch_fine_tune\n",
    "from src.data.data import get_random_subsets\n",
    "\n",
    "# Generate training and evaluation datasets. These should be used for all fine-tuning methods to ensure consistency. np.random should be seeded before this.\n",
    "train_datasets, eval_dataset_in, eval_dataset_out = get_random_subsets(train_dataset=in_domain_train, \n",
    "                                                                       eval_dataset_in=in_domain_test, \n",
    "                                                                       eval_dataset_out=out_domain, \n",
    "                                                                       train_sample_sizes=[4096],\n",
    "                                                                       num_trials=10,   # 5\n",
    "                                                                       eval_sample_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model_opt125_causal.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt 125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5589fac2fddf4657b9417bd14a4beed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aae4062cc524c04a0a5d835bf8ccd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In domain eval metrics:\n",
      "{'loss': 0.6931471824645996, 'accuracy': 0.58, 'runtime': 37.45100116729736, 'samples_per_second': 1.3350777934252012, 'peak_memory_gb': 9.791648864746094}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a8ab849c6c403f8799dac200539dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of domain eval metrics:\n",
      "{'loss': 0.6931471824645996, 'accuracy': 0.4, 'runtime': 33.18849849700928, 'samples_per_second': 1.506546010344688, 'peak_memory_gb': 9.791648864746094}\n",
      "Metrics:\n",
      "{'eval_in_accuracy': 0.58,\n",
      " 'eval_in_loss': 0.6931471824645996,\n",
      " 'eval_in_peak_memory_gb': 9.791648864746094,\n",
      " 'eval_in_runtime': 37.45100116729736,\n",
      " 'eval_in_samples_per_second': 1.3350777934252012,\n",
      " 'eval_out_accuracy': 0.4,\n",
      " 'eval_out_loss': 0.6931471824645996,\n",
      " 'eval_out_peak_memory_gb': 9.791648864746094,\n",
      " 'eval_out_runtime': 33.18849849700928,\n",
      " 'eval_out_samples_per_second': 1.506546010344688,\n",
      " 'model_name': 'opt-125m'}\n"
     ]
    }
   ],
   "source": [
    "from src.finetuners.context_distillation import context_distillation\n",
    "import pprint\n",
    "print(\"opt 125\")\n",
    "metrics = context_distillation(\n",
    "    student_model_opt125_causal,\n",
    "    teacher_model_opt125_causal, \n",
    "    tokenizer_opt125_causal, \n",
    "    train_datasets[4096][0], \n",
    "    num_epochs = 1,\n",
    "    eval_dataset_in=eval_dataset_in, \n",
    "    eval_dataset_out=eval_dataset_out, \n",
    "    batch_size=2,\n",
    "    model_name='opt-125m')\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.context_distillation import context_distillation\n",
    "import pprint\n",
    "print(\"opt 350\")\n",
    "metrics = context_distillation(\n",
    "    student_model_opt350_causal,\n",
    "    teacher_model_opt350_causal, \n",
    "    tokenizer_opt350_causal, \n",
    "    train_datasets[4096][0], \n",
    "    num_epochs = 1,\n",
    "    eval_dataset_in=eval_dataset_in, \n",
    "    eval_dataset_out=eval_dataset_out, \n",
    "    batch_size=2,\n",
    "    model_name='opt-350m')\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt 125 recursive distillation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joel\\anaconda3\\envs\\fine-tuning2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b37d2f5c6941d6adac32d1c6129a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b7466dcbbc4684a925243320da0dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a545a376e08547e2930611f88f786ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joel\\Documents\\fall23\\dl\\proj2\\llm-finetuning\\experiments\\context_distillation_example.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpprint\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mopt 125 recursive distillation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m metrics \u001b[39m=\u001b[39m recursive_context_distillation(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     student_model_opt125_causal,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokenizer_opt125_causal, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_datasets[\u001b[39m4096\u001b[39;49m][\u001b[39m0\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     num_epochs \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     eval_dataset_in\u001b[39m=\u001b[39;49meval_dataset_in, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     eval_dataset_out\u001b[39m=\u001b[39;49meval_dataset_out, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMetrics:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pprint\u001b[39m.\u001b[39mpprint(metrics)\n",
      "File \u001b[1;32mc:\\users\\joel\\documents\\fall23\\dl\\proj2\\llm-finetuning\\src\\finetuners\\context_distillation_recursive.py:63\u001b[0m, in \u001b[0;36mrecursive_context_distillation\u001b[1;34m(student_model, tokenizer, dataset, num_epochs, eval_dataset_in, eval_dataset_out, batch_size)\u001b[0m\n\u001b[0;32m     61\u001b[0m teacher_input_ids = torch.tensor(teacher_dataset['input_ids'])\n\u001b[0;32m     62\u001b[0m teacher_mask = torch.tensor(teacher_dataset['attention_mask'])\n\u001b[1;32m---> 63\u001b[0m teacher_logits = student_model(teacher_input_ids.to(device), teacher_mask.to(device)).logits\n\u001b[0;32m     64\u001b[0m #get student logits\n\u001b[0;32m     65\u001b[0m student_batch = Dataset.from_dict(student_batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.finetuners.context_distillation_recursive import recursive_context_distillation\n",
    "import pprint\n",
    "print(\"opt 125 recursive distillation\")\n",
    "metrics = recursive_context_distillation(\n",
    "    student_model_opt125_causal,\n",
    "    tokenizer_opt125_causal, \n",
    "    train_datasets[4096][0], \n",
    "    num_epochs = 1,\n",
    "    eval_dataset_in=eval_dataset_in, \n",
    "    eval_dataset_out=eval_dataset_out, \n",
    "    batch_size=2,\n",
    "    model_name='opt-125m')\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt 350 recursive distillation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9562c1154ad04f4bab942175749486af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 207.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joel\\Documents\\fall23\\dl\\proj2\\llm-finetuning\\experiments\\context_distillation_example.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpprint\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mopt 350 recursive distillation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m metrics \u001b[39m=\u001b[39m recursive_context_distillation(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     student_model_opt350_causal,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tokenizer_opt350_causal, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_datasets[\u001b[39m4096\u001b[39;49m][\u001b[39m0\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     num_epochs \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     eval_dataset_in\u001b[39m=\u001b[39;49meval_dataset_in, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     eval_dataset_out\u001b[39m=\u001b[39;49meval_dataset_out, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMetrics:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/fall23/dl/proj2/llm-finetuning/experiments/context_distillation_example.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pprint\u001b[39m.\u001b[39mpprint(metrics)\n",
      "File \u001b[1;32mc:\\users\\joel\\documents\\fall23\\dl\\proj2\\llm-finetuning\\src\\finetuners\\context_distillation_recursive.py:73\u001b[0m, in \u001b[0;36mrecursive_context_distillation\u001b[1;34m(student_model, tokenizer, dataset, num_epochs, eval_dataset_in, eval_dataset_out, batch_size)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m loss = distillation_loss(teacher_logits, student_logits)\n\u001b[1;32m---> 73\u001b[0m loss.backward()\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m optimizer.step()\n",
      "File \u001b[1;32mc:\\Users\\Joel\\anaconda3\\envs\\fine-tuning2\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Joel\\anaconda3\\envs\\fine-tuning2\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 207.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from src.finetuners.context_distillation_recursive import recursive_context_distillation\n",
    "import pprint\n",
    "print(\"opt 350 recursive distillation\")\n",
    "metrics = recursive_context_distillation(\n",
    "    student_model_opt350_causal,\n",
    "    tokenizer_opt350_causal, \n",
    "    train_datasets[4096][0], \n",
    "    num_epochs = 1,\n",
    "    eval_dataset_in=eval_dataset_in, \n",
    "    eval_dataset_out=eval_dataset_out, \n",
    "    batch_size=2,\n",
    "    model_name='opt-350m')\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
