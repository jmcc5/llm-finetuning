{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment notebook for \"Context is All You Need\"\n",
    "In this notebook we conduct experiments involving few-shot fine-tuning, in-context learning (ICL), and a novel implementation of context distillation.\n",
    "\n",
    "To install the development environment, run the following:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate fine-tuning\n",
    "```\n",
    "\n",
    "Run ```pip install -e .``` if module importing isn't working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the check below fails, verify your pytorch installation by following the steps at https://pytorch.org/get-started/locally/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "from src.utils import cuda_check\n",
    "\n",
    "cuda_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets\n",
    "\n",
    "Import datasets using methods from `src/data/data.py`. Datasets are downloaded from huggingface and stored in `/data`. Once downloaded, datasets are loaded locally.\n",
    "\n",
    "Our in domain dataset is [MNLI](https://huggingface.co/datasets/glue). Our out of domain dataset is [HANS](https://huggingface.co/datasets/hans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In domain (MNLI):\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 261802\n",
      "})\n",
      "{'premise': 'One of our number will carry out your instructions minutely.', 'hypothesis': 'A member of my team will execute your orders with immense precision.', 'label': 0, 'idx': 2}\n",
      "\n",
      "Out of domain (HANS):\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'premise': 'The president avoided the athlete .', 'hypothesis': 'The athlete avoided the president .', 'label': 1, 'parse_premise': '(ROOT (S (NP (DT The) (NN president)) (VP (VBD avoided) (NP (DT the) (NN athlete))) (. .)))', 'parse_hypothesis': '(ROOT (S (NP (DT The) (NN athlete)) (VP (VBD avoided) (NP (DT the) (NN president))) (. .)))', 'binary_parse_premise': '( ( The president ) ( ( avoided ( the athlete ) ) . ) )', 'binary_parse_hypothesis': '( ( The athlete ) ( ( avoided ( the president ) ) . ) )', 'heuristic': 'lexical_overlap', 'subcase': 'ln_subject/object_swap', 'template': 'temp1'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from src.data.data import get_in_domain, get_out_domain\n",
    "\n",
    "in_domain_train, in_domain_test = get_in_domain()\n",
    "out_domain = get_out_domain()\n",
    "\n",
    "print(f\"In domain (MNLI):\\n{in_domain_train}\")\n",
    "pprint.pprint(in_domain_train[1])\n",
    "\n",
    "print(f\"\\nOut of domain (HANS):\\n{out_domain}\")\n",
    "pprint.pprint(out_domain[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and evaluation datasets\n",
    "\n",
    "The `get_random_subsets` method from `src/data/data.py` creates a dictionary of training and evaluation data organized by sample size. Each sample size will contain 10 randomly generated trials of that sample size. Evaluation sets contain 50 samples and are randomly generated a single time to ensure consistant comparison across fine-tuning methods.\n",
    "\n",
    "Before generating our datasets, we set random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets:\n",
      "{2: [...], 4: [...], 8: [...], 16: [...]}\n",
      "Eval datasets:\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from src.data.data import get_random_subsets\n",
    "\n",
    "# Seed random generators\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Generate training and evaluation datasets\n",
    "train_datasets, eval_dataset_in, eval_dataset_out = get_random_subsets(train_dataset=in_domain_train, \n",
    "                                                                       eval_dataset_in=in_domain_test, \n",
    "                                                                       eval_dataset_out=out_domain, \n",
    "                                                                       train_sample_sizes=[2, 4, 8, 16],\n",
    "                                                                       num_trials=10,\n",
    "                                                                       eval_sample_size=50)\n",
    "\n",
    "print(\"Train datasets:\")\n",
    "pprint.pprint(train_datasets, depth=1)\n",
    "print(\"\\nEval datasets:\")\n",
    "pprint.pprint(eval_dataset_in, depth=1)\n",
    "pprint.pprint(eval_dataset_out, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n",
    "\n",
    "Import models using methods from src/models/opt.py. Models are downloaded from huggingface and stored in /models/pretrained. Once downloaded, models are loaded locally. In the following experiments, model_loading is handled in the scope of each fine-tuning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model.model import get_model\n",
    "\n",
    "# # Get SequenceClassification models\n",
    "# model_opt125, tokenizer_opt125 = get_model(model_name='opt-125m', model_type='SequenceClassification', pretrained=True)\n",
    "# model_opt350, tokenizer_opt350 = get_model(model_name='opt-350m', model_type='SequenceClassification', pretrained=True)\n",
    "\n",
    "# # Get CasualLM models\n",
    "# model_opt125_causal, tokenizer_opt125_causal = get_model(model_name='opt-125m', model_type='CausalLM', pretrained=True)\n",
    "# model_opt350_causal, tokenizer_opt350_causal = get_model(model_name='opt-350m', model_type='CausalLM', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot baseline\n",
    "\n",
    "We evaluate both models on in and out of domain eval sets with no training or context using the `generate` method. These results serve as a baseline for comparison to other fine-tuning methods. Model parameters are not updated using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75cbcfdb28d4b39b8aedb981ba3a2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ea3e86875a44ba8705b0cd54f91d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8658ae5748448e98beabc53ac502b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\harri\\OneDrive\\Documents\\OMSCS\\CS 7643 DL\\Projects\\Group Project\\Efficient_LLM_Few-Example_Fine-Tuning\\experiments\\final_experiments.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfinetuners\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mzeroshot\u001b[39;00m \u001b[39mimport\u001b[39;00m batch_evaluate\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m metrics \u001b[39m=\u001b[39m batch_evaluate(model_names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mopt-125m\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mopt-350m\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                          eval_dataset_in\u001b[39m=\u001b[39;49meval_dataset_in, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                          eval_dataset_out\u001b[39m=\u001b[39;49meval_dataset_out, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                          verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                          disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMetrics:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pprint\u001b[39m.\u001b[39mpprint(metrics)\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\zeroshot.py:92\u001b[0m, in \u001b[0;36mbatch_evaluate\u001b[1;34m(model_names, eval_dataset_in, eval_dataset_out, verbose, disable_tqdm)\u001b[0m\n\u001b[0;32m     89\u001b[0m model, tokenizer \u001b[39m=\u001b[39m get_model(model_name, \u001b[39m'\u001b[39m\u001b[39mCausalLM\u001b[39m\u001b[39m'\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m eval_metrics \u001b[39m=\u001b[39m evaluate(model, tokenizer, eval_dataset_in, eval_dataset_out, verbose\u001b[39m=\u001b[39;49mverbose, disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm)\n\u001b[0;32m     94\u001b[0m \u001b[39m# Create results dict\u001b[39;00m\n\u001b[0;32m     95\u001b[0m sample_size \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(eval_dataset_in))\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\zeroshot.py:70\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, tokenizer, eval_dataset_in, eval_dataset_out, batch_size, verbose, disable_tqdm)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[0;32m     69\u001b[0m \u001b[39m# Evaluate - batch size = 8 due to GPU memory constraints\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m eval_metrics_in \u001b[39m=\u001b[39m evaluate_dataset(model, tokenizer, eval_dataset_in, batch_size\u001b[39m=\u001b[39;49mbatch_size)    \u001b[39m# In domain\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m     72\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn domain eval metrics:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00meval_metrics_in\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\zeroshot.py:38\u001b[0m, in \u001b[0;36mevaluate.<locals>.evaluate_dataset\u001b[1;34m(model, tokenizer, dataset, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tokenized_batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     36\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tokenized_batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 38\u001b[0m generated_tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m     39\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m     40\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m     41\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,   \u001b[39m# Max tokens to generate in output\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m     constraints\u001b[39m=\u001b[39;49m[yes_no_constraint],    \u001b[39m# Constrain output to 'Yes' or 'No'\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m   \u001b[39m# Use minimum number of beams to save compute\u001b[39;49;00m\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m generated_texts \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generated_tokens[:, input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)    \u001b[39m# Decode generated tokens\u001b[39;00m\n\u001b[0;32m     47\u001b[0m actual_labels_batch \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m batch]   \u001b[39m# Get actual labels from batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\generation\\utils.py:1829\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1823\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1824\u001b[0m     expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   1825\u001b[0m     is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1826\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1827\u001b[0m )\n\u001b[0;32m   1828\u001b[0m \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[1;32m-> 1829\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstrained_beam_search(\n\u001b[0;32m   1830\u001b[0m     input_ids,\n\u001b[0;32m   1831\u001b[0m     constrained_beam_scorer\u001b[39m=\u001b[39mconstrained_beam_scorer,\n\u001b[0;32m   1832\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1833\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1834\u001b[0m     pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1835\u001b[0m     eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1836\u001b[0m     output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1837\u001b[0m     return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1838\u001b[0m     synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1839\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1840\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\generation\\utils.py:4069\u001b[0m, in \u001b[0;36mGenerationMixin.constrained_beam_search\u001b[1;34m(self, input_ids, constrained_beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   4065\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   4067\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 4069\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   4070\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   4071\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   4072\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   4073\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   4074\u001b[0m )\n\u001b[0;32m   4076\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   4077\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:944\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    941\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    943\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 944\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m    945\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    946\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    947\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    948\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    949\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    950\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    951\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    952\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    953\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    954\u001b[0m )\n\u001b[0;32m    956\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    958\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:710\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    702\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    703\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m    704\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    708\u001b[0m     )\n\u001b[0;32m    709\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 710\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m    711\u001b[0m         hidden_states,\n\u001b[0;32m    712\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[0;32m    713\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    714\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    715\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    716\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    717\u001b[0m     )\n\u001b[0;32m    719\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    721\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:330\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    327\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    329\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    331\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    332\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    333\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    334\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    335\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    336\u001b[0m )\n\u001b[0;32m    337\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    338\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:225\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    221\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention mask should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39m1\u001b[39m,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39msrc_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m         )\n\u001b[0;32m    223\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[39m+\u001b[39m attention_mask\n\u001b[0;32m    224\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(\n\u001b[1;32m--> 225\u001b[0m         attn_weights, torch\u001b[39m.\u001b[39;49mtensor(torch\u001b[39m.\u001b[39;49mfinfo(attn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mmin, device\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    226\u001b[0m     )\n\u001b[0;32m    227\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m    229\u001b[0m \u001b[39m# upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.finetuners.zeroshot import batch_evaluate\n",
    "\n",
    "metrics = batch_evaluate(model_names=['opt-125m', 'opt-350m'], \n",
    "                         eval_dataset_in=eval_dataset_in, \n",
    "                         eval_dataset_out=eval_dataset_out, \n",
    "                         exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot fine-tuning\n",
    "\n",
    "We fine-tune both models on 10 trials of training data and evaluate on in and out of domain eval sets. This method updates all model parameters. Fine-tuned models can be saved locally to `models/finetuned/` by setting the `save_trials` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.fewshot import batch_fine_tune\n",
    "\n",
    "metrics, training_histories = batch_fine_tune(model_name=['opt-125m', 'opt-350m'], \n",
    "                                              train_datasets=train_datasets, \n",
    "                                              eval_dataset_in=eval_dataset_in, \n",
    "                                              eval_dataset_out=eval_dataset_out, \n",
    "                                              exp_label='final', \n",
    "                                              save_trials=False)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)\n",
    "print(\"Training histories:\")\n",
    "pprint.pprint(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-context learning (ICL)\n",
    "\n",
    "ICL is performed similarly to zero-shot evaluation, using the `generate` method. Context (labeled training examples) is pre-pended to each evaluation example. Model parameters are not updated using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.incontext import batch_evaluate\n",
    "\n",
    "metrics = batch_evaluate(model_names=['opt-125m', 'opt-350m'], \n",
    "                         train_datasets=train_datasets, \n",
    "                         eval_dataset_in=eval_dataset_in, \n",
    "                         eval_dataset_out=eval_dataset_out, \n",
    "                         exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-distillation fine-tuning\n",
    "TODO: add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot fine-tuning with LoRA\n",
    "TODO: add description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot in-domain vs. out-of-domain metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plot import plot_in_out_domain\n",
    "\n",
    "plot_in_out_domain(logfile='fewshot_metrics_final.csv', metric='accuracy')\n",
    "plot_in_out_domain(logfile='fewshot_metrics_final.csv', metric='peak_memory_gb')\n",
    "plot_in_out_domain(logfile='fewshot_metrics_final.csv', metric='runtime')\n",
    "\n",
    "#TODO: plot other methods and combine plots\n",
    "plot_in_out_domain(logfile='icl_metrics_final.csv', metric='accuracy')\n",
    "plot_in_out_domain(logfile='icl_metrics_final.csv', metric='peak_memory_gb')\n",
    "plot_in_out_domain(logfile='icl_metrics_final.csv', metric='runtime')\n",
    "\n",
    "plot_in_out_domain(logfile='zeroshot_metrics_final.csv', metric='accuracy')\n",
    "plot_in_out_domain(logfile='zeroshot_metrics_final.csv', metric='peak_memory_gb')\n",
    "plot_in_out_domain(logfile='zeroshot_metrics_final.csv', metric='runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plot import plot_learning_curves\n",
    "\n",
    "plot_learning_curves(logfile='fewshot_training_history_final.csv', subplot=False)\n",
    "plot_learning_curves(logfile='fewshot_training_history_final.csv', subplot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
