{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment notebook for \"Context is All You Need\"\n",
    "In this notebook we conduct experiments involving few-shot fine-tuning, in-context learning (ICL), and a novel implementation of context distillation.\n",
    "\n",
    "To install the development environment, run the following:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate fine-tuning\n",
    "```\n",
    "\n",
    "Run ```pip install -e .``` if module importing isn't working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the check below fails, verify your pytorch installation by following the steps at https://pytorch.org/get-started/locally/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    }
   ],
   "source": [
    "from src.utils import cuda_check\n",
    "\n",
    "cuda_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets\n",
    "\n",
    "Import datasets using methods from `src/data/data.py`. Datasets are downloaded from huggingface and stored in `/data`. Once downloaded, datasets are loaded locally.\n",
    "\n",
    "Our in domain dataset is [MNLI](https://huggingface.co/datasets/glue). Our out of domain dataset is [HANS](https://huggingface.co/datasets/hans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In domain (MNLI):\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 261802\n",
      "})\n",
      "{'hypothesis': 'A member of my team will execute your orders with immense '\n",
      "               'precision.',\n",
      " 'idx': 2,\n",
      " 'label': 0,\n",
      " 'premise': 'One of our number will carry out your instructions minutely.'}\n",
      "\n",
      "Out of domain (HANS):\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'binary_parse_hypothesis': '( ( The athlete ) ( ( avoided ( the president ) ) '\n",
      "                            '. ) )',\n",
      " 'binary_parse_premise': '( ( The president ) ( ( avoided ( the athlete ) ) . '\n",
      "                         ') )',\n",
      " 'heuristic': 'lexical_overlap',\n",
      " 'hypothesis': 'The athlete avoided the president .',\n",
      " 'label': 1,\n",
      " 'parse_hypothesis': '(ROOT (S (NP (DT The) (NN athlete)) (VP (VBD avoided) '\n",
      "                     '(NP (DT the) (NN president))) (. .)))',\n",
      " 'parse_premise': '(ROOT (S (NP (DT The) (NN president)) (VP (VBD avoided) (NP '\n",
      "                  '(DT the) (NN athlete))) (. .)))',\n",
      " 'premise': 'The president avoided the athlete .',\n",
      " 'subcase': 'ln_subject/object_swap',\n",
      " 'template': 'temp1'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from src.data.data import get_in_domain, get_out_domain\n",
    "\n",
    "in_domain_train, in_domain_test = get_in_domain()\n",
    "out_domain = get_out_domain()\n",
    "\n",
    "print(f\"In domain (MNLI):\\n{in_domain_train}\")\n",
    "pprint.pprint(in_domain_train[1])\n",
    "\n",
    "print(f\"\\nOut of domain (HANS):\\n{out_domain}\")\n",
    "pprint.pprint(out_domain[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and evaluation datasets\n",
    "\n",
    "The `get_random_subsets` method from `src/data/data.py` creates a dictionary of training and evaluation data organized by sample size. Each sample size will contain 10 randomly generated trials of that sample size. Evaluation sets contain 50 samples and are randomly generated a single time to ensure consistant comparison across fine-tuning methods.\n",
    "\n",
    "Before generating our datasets, we set random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets:\n",
      "{2: [...], 4: [...], 8: [...], 16: [...]}\n",
      "\n",
      "Eval datasets:\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from src.data.data import get_random_subsets\n",
    "\n",
    "# Seed random generators\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Generate training and evaluation datasets\n",
    "train_datasets, eval_dataset_in, eval_dataset_out = get_random_subsets(train_dataset=in_domain_train, \n",
    "                                                                       eval_dataset_in=in_domain_test, \n",
    "                                                                       eval_dataset_out=out_domain, \n",
    "                                                                       train_sample_sizes=[2, 4, 8, 16],\n",
    "                                                                       num_trials=10,\n",
    "                                                                       eval_sample_size=50)\n",
    "\n",
    "print(\"Train datasets:\")\n",
    "pprint.pprint(train_datasets, depth=1)\n",
    "print(\"\\nEval datasets:\")\n",
    "pprint.pprint(eval_dataset_in, depth=1)\n",
    "pprint.pprint(eval_dataset_out, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n",
    "\n",
    "Import models using methods from `src/models/opt.py`. Models are downloaded from huggingface and stored in `/models/pretrained`. Once downloaded, models are loaded locally. In the following experiments, model loading is handled in the scope of each fine-tuning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model.model import get_model\n",
    "\n",
    "# # Get SequenceClassification models\n",
    "# model_opt125, tokenizer_opt125 = get_model(model_name='opt-125m', model_type='SequenceClassification', pretrained=True)\n",
    "# model_opt350, tokenizer_opt350 = get_model(model_name='opt-350m', model_type='SequenceClassification', pretrained=True)\n",
    "\n",
    "# # Get CasualLM models\n",
    "# model_opt125_causal, tokenizer_opt125_causal = get_model(model_name='opt-125m', model_type='CausalLM', pretrained=True)\n",
    "# model_opt350_causal, tokenizer_opt350_causal = get_model(model_name='opt-350m', model_type='CausalLM', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot baseline\n",
    "\n",
    "We evaluate both models on in and out of domain eval sets with no training or context using the `generate` method. These results serve as a baseline for comparison to other fine-tuning methods. Model parameters are not updated using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzeroshot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_evaluate\n\u001b[0;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m batch_evaluate(model_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt-125m\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt-350m\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      4\u001b[0m                          eval_dataset_in\u001b[38;5;241m=\u001b[39meval_dataset_in, \n\u001b[0;32m      5\u001b[0m                          eval_dataset_out\u001b[38;5;241m=\u001b[39meval_dataset_out, \n\u001b[0;32m      6\u001b[0m                          exp_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\gatech\\fall2023\\project\\src\\src\\finetuners\\zeroshot.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Import Modules\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_minimal_pattern, tokenize_dataset, compute_metrics_causal, metrics_to_csv, get_yes_no_constraint, interpret_generated_texts, reset_memory_stats, get_peak_memory\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, tokenizer, eval_dataset_in, eval_dataset_out, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable_tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32md:\\gatech\\fall2023\\project\\src\\src\\finetuners\\utils.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_progress_bar\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\__init__.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m SCRIPTS_VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(__version__)\u001b[38;5;241m.\u001b[39mis_devrelease \u001b[38;5;28;01melse\u001b[39;00m __version__\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_suite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     AudioClassificationEvaluator,\n\u001b[0;32m     32\u001b[0m     AutomaticSpeechRecognitionEvaluator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     evaluator,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m push_to_hub\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DownloadConfig, DownloadMode, load_dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluation_module_factory\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\evaluator\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SUPPORTED_TASKS \u001b[38;5;28;01mas\u001b[39;00m SUPPORTED_PIPELINE_TASKS\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TASK_ALIASES\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_task \u001b[38;5;28;01mas\u001b[39;00m check_pipeline_task\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\pipelines\\__init__.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation, ConversationalPipeline\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepth_estimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthEstimationPipeline\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_question_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentQuestionAnsweringPipeline\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionPipeline\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfill_mask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FillMaskPipeline\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\pipelines\\document_question_answering.py:29\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ExplicitEnum,\n\u001b[0;32m     22\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     logging,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIPELINE_INIT_ARGS, ChunkPipeline\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquestion_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_starts_ends\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SquadExample, SquadFeatures, squad_convert_examples_to_features\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCard\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\data\\__init__.py:26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[0;32m     17\u001b[0m     DataCollatorForPermutationLanguageModeling,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     default_data_collator,\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glue_compute_metrics, xnli_compute_metrics\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     DataProcessor,\n\u001b[0;32m     29\u001b[0m     InputExample,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     xnli_tasks_num_labels,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\data\\metrics\\__init__.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sklearn_available, requires_backends\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, matthews_corrcoef\n\u001b[0;32m     23\u001b[0m DEPRECATION_WARNING \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrary. You can have a look at this example script for pointers: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\stats\\__init__.py:608\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\stats\\distributions.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_continuous_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:10452\u001b[0m\n\u001b[0;32m  10448\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_entropy\u001b[39m(\u001b[38;5;28mself\u001b[39m, beta):\n\u001b[0;32m  10449\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mbeta \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(beta) \u001b[38;5;241m+\u001b[39m sc\u001b[38;5;241m.\u001b[39mgammaln(\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mbeta)\n\u001b[1;32m> 10452\u001b[0m halfgennorm \u001b[38;5;241m=\u001b[39m \u001b[43mhalfgennorm_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhalfgennorm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10455\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mcrystalball_gen\u001b[39;00m(rv_continuous):\n\u001b[0;32m  10456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10457\u001b[0m \u001b[38;5;124;03m    Crystalball distribution\u001b[39;00m\n\u001b[0;32m  10458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10489\u001b[0m \u001b[38;5;124;03m    %(example)s\u001b[39;00m\n\u001b[0;32m  10490\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1845\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[1;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, seed)\u001b[0m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1844\u001b[0m     dct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(distcont)\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocdict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:809\u001b[0m, in \u001b[0;36mrv_generic._construct_doc\u001b[1;34m(self, docdict, shapes_vals)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(shapes)s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mdoccer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__doc__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempdict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to construct docstring for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    813\u001b[0m                     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mrepr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\scipy\\_lib\\doccer.py:65\u001b[0m, in \u001b[0;36mdocformat\u001b[1;34m(docstring, docdict)\u001b[0m\n\u001b[0;32m     63\u001b[0m     newlines \u001b[38;5;241m=\u001b[39m [lines[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m---> 65\u001b[0m         \u001b[43mnewlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     indented[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(newlines)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.finetuners.zeroshot import batch_evaluate\n",
    "\n",
    "metrics = batch_evaluate(model_names=['opt-125m', 'opt-350m'], \n",
    "                         eval_dataset_in=eval_dataset_in, \n",
    "                         eval_dataset_out=eval_dataset_out, \n",
    "                         exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot fine-tuning\n",
    "\n",
    "We fine-tune both models on 10 trials of training data and evaluate on in and out of domain eval sets. This method updates all model parameters. Fine-tuned models can be saved locally to `models/finetuned/` by setting the `save_trials` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d58b185f9542348cfa7d57db641c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 2-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfewshot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_fine_tune\n\u001b[1;32m----> 3\u001b[0m metrics, training_histories \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_fine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopt-350m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mtrain_datasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43meval_dataset_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43meval_dataset_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mexp_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43msave_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(metrics)\n",
      "File \u001b[1;32md:\\gatech\\fall2023\\project\\src\\src\\finetuners\\fewshot.py:102\u001b[0m, in \u001b[0;36mbatch_fine_tune\u001b[1;34m(model_names, train_datasets, eval_dataset_in, eval_dataset_out, exp_label, save_trials)\u001b[0m\n\u001b[0;32m     99\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial_num, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m--> 102\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m get_model(model_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSequenceClassification\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load original model from disk\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     metrics_trial, full_training_history \u001b[38;5;241m=\u001b[39m fine_tune(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m    104\u001b[0m                                                      tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[0;32m    105\u001b[0m                                                      train_dataset\u001b[38;5;241m=\u001b[39mdataset, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m                                                      val_in_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m    110\u001b[0m                                                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Fine-tune\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     metrics_trial \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name,\n\u001b[0;32m    113\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_size\u001b[39m\u001b[38;5;124m'\u001b[39m: sample_size,\n\u001b[0;32m    114\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics_trial}\n",
      "File \u001b[1;32md:\\gatech\\fall2023\\project\\src\\src\\finetuners\\fewshot.py:68\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(model, tokenizer, train_dataset, eval_dataset_in, eval_dataset_out, batch_size, val_in_training, verbose, disable_tqdm)\u001b[0m\n\u001b[0;32m     65\u001b[0m if not verbose:\n\u001b[0;32m     66\u001b[0m     trainer.remove_callback(PrinterCallback)\n\u001b[1;32m---> 68\u001b[0m # Train on in domain\n\u001b[0;32m     69\u001b[0m train_output = trainer.train()\n\u001b[0;32m     70\u001b[0m train_metrics = train_output.metrics\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:1999\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1999\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2003\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2331\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:3066\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3063\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3065\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3066\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3076\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:3359\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3355\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3356\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3357\u001b[0m         )\n\u001b[0;32m   3358\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3359\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3361\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\gatech\\fall2023\\project\\src\\src\\finetuners\\utils.py:154\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(predictions)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(predictions):\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute evaluation metrics.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     logits, labels \u001b[38;5;241m=\u001b[39m predictions\n\u001b[0;32m    156\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[0;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[0;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[0;32m    761\u001b[0m )\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\loading.py:639\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[1;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasurement\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubEvaluationModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 639\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\loading.py:479\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# get script and other files\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 479\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_loading_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# if there is no file found with current revision tag try to load main\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_SCRIPTS_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, SCRIPTS_VERSION) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\loading.py:469\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.download_loading_script\u001b[1;34m(self, revision)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_config\u001b[38;5;241m.\u001b[39mdownload_desc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mdownload_desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading builder script\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\utils\\file_utils.py:175\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(url_or_filename)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_url_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\utils\\file_utils.py:464\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001b[0m\n\u001b[0;32m    462\u001b[0m     connected \u001b[38;5;241m=\u001b[39m ftp_head(url)\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:  \u001b[38;5;66;03m# ok\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         etag \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_etag \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\utils\\file_utils.py:378\u001b[0m, in \u001b[0;36mhttp_head\u001b[1;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[0;32m    376\u001b[0m headers \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(headers) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    377\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_datasets_user_agent(user_agent\u001b[38;5;241m=\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 378\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\evaluate\\utils\\file_utils.py:307\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[0;32m    305\u001b[0m tries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(), url\u001b[38;5;241m=\u001b[39murl, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    308\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectTimeout, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1056\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1057\u001b[0m         (\n\u001b[0;32m   1058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1064\u001b[0m     )\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    436\u001b[0m     default_ssl_context\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    440\u001b[0m ):\n",
      "File \u001b[1;32md:\\Gatech\\Coding\\envs\\fine-tuning\\lib\\site-packages\\urllib3\\util\\ssl_.py:402\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.finetuners.fewshot import batch_fine_tune\n",
    "\n",
    "metrics, training_histories = batch_fine_tune(model_names=['opt-125m', 'opt-350m'], \n",
    "                                              train_datasets=train_datasets, \n",
    "                                              eval_dataset_in=eval_dataset_in, \n",
    "                                              eval_dataset_out=eval_dataset_out, \n",
    "                                              exp_label='final', \n",
    "                                              save_trials=False)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)\n",
    "print(\"Training histories:\")\n",
    "pprint.pprint(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot with LoRA (Low Rank Adaptation)\n",
    "\n",
    "We fine-tune both models identically to few-shot fine-tuning, but with each model converted to a `LoraModel` for Parameter-Efficient Fine-tuning (PEFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50bccfe5add4f80a320c8b0c5499b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 2-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443392 || all params: 331591680 || trainable%: 0.13%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e164eb74975844e99f7e71dc1e7824ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 4-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da742a4e2c3640228c275c3a236169f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 8-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9281a5a87d49fc856dd6d7b4d20cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 16-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "[{'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.4,\n",
      "  'eval_in_loss': 1.0200151205062866,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2656,\n",
      "  'eval_in_samples_per_second': 11.722,\n",
      "  'eval_in_steps_per_second': 1.641,\n",
      "  'eval_out_accuracy': 0.54,\n",
      "  'eval_out_loss': 0.7147576212882996,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.2663,\n",
      "  'eval_out_samples_per_second': 11.72,\n",
      "  'eval_out_steps_per_second': 1.641,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.5763686515390873,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 96.2347,\n",
      "  'train_samples_per_second': 0.831,\n",
      "  'train_steps_per_second': 0.416},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.38,\n",
      "  'eval_in_loss': 1.1879794597625732,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2346,\n",
      "  'eval_in_samples_per_second': 11.808,\n",
      "  'eval_in_steps_per_second': 1.653,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.8105555772781372,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.2967,\n",
      "  'eval_out_samples_per_second': 11.637,\n",
      "  'eval_out_steps_per_second': 1.629,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.2156417173333466,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 94.0443,\n",
      "  'train_samples_per_second': 0.851,\n",
      "  'train_steps_per_second': 0.425},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.52,\n",
      "  'eval_in_loss': 1.0712076425552368,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2534,\n",
      "  'eval_in_samples_per_second': 11.755,\n",
      "  'eval_in_steps_per_second': 1.646,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.7977505326271057,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.2356,\n",
      "  'eval_out_samples_per_second': 11.805,\n",
      "  'eval_out_steps_per_second': 1.653,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.761304946988821,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 94.5929,\n",
      "  'train_samples_per_second': 0.846,\n",
      "  'train_steps_per_second': 0.423},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.46,\n",
      "  'eval_in_loss': 1.105648159980774,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2723,\n",
      "  'eval_in_samples_per_second': 11.703,\n",
      "  'eval_in_steps_per_second': 1.638,\n",
      "  'eval_out_accuracy': 0.54,\n",
      "  'eval_out_loss': 0.7673619985580444,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.2417,\n",
      "  'eval_out_samples_per_second': 11.788,\n",
      "  'eval_out_steps_per_second': 1.65,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.2962923651561141,\n",
      "  'train_peak_memory_gb': 3.9469785690307617,\n",
      "  'train_runtime': 94.4192,\n",
      "  'train_samples_per_second': 0.847,\n",
      "  'train_steps_per_second': 0.424},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.38,\n",
      "  'eval_in_loss': 1.0267692804336548,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2416,\n",
      "  'eval_in_samples_per_second': 11.788,\n",
      "  'eval_in_steps_per_second': 1.65,\n",
      "  'eval_out_accuracy': 0.5,\n",
      "  'eval_out_loss': 0.7108142375946045,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.292,\n",
      "  'eval_out_samples_per_second': 11.649,\n",
      "  'eval_out_steps_per_second': 1.631,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.6678216308355331,\n",
      "  'train_peak_memory_gb': 3.9469785690307617,\n",
      "  'train_runtime': 94.6267,\n",
      "  'train_samples_per_second': 0.845,\n",
      "  'train_steps_per_second': 0.423},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.44,\n",
      "  'eval_in_loss': 1.024383544921875,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2511,\n",
      "  'eval_in_samples_per_second': 11.762,\n",
      "  'eval_in_steps_per_second': 1.647,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7222772240638733,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.262,\n",
      "  'eval_out_samples_per_second': 11.732,\n",
      "  'eval_out_steps_per_second': 1.642,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.7878906242549419,\n",
      "  'train_peak_memory_gb': 3.9469785690307617,\n",
      "  'train_runtime': 95.4313,\n",
      "  'train_samples_per_second': 0.838,\n",
      "  'train_steps_per_second': 0.419},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.48,\n",
      "  'eval_in_loss': 1.1033912897109985,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2393,\n",
      "  'eval_in_samples_per_second': 11.794,\n",
      "  'eval_in_steps_per_second': 1.651,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7798669338226318,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.7336,\n",
      "  'eval_out_samples_per_second': 10.563,\n",
      "  'eval_out_steps_per_second': 1.479,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.4205555975437164,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 94.7386,\n",
      "  'train_samples_per_second': 0.844,\n",
      "  'train_steps_per_second': 0.422},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.34,\n",
      "  'eval_in_loss': 0.9950324296951294,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.3274,\n",
      "  'eval_in_samples_per_second': 11.554,\n",
      "  'eval_in_steps_per_second': 1.618,\n",
      "  'eval_out_accuracy': 0.5,\n",
      "  'eval_out_loss': 0.6993972659111023,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.2903,\n",
      "  'eval_out_samples_per_second': 11.654,\n",
      "  'eval_out_steps_per_second': 1.632,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.6816607199609279,\n",
      "  'train_peak_memory_gb': 3.9469785690307617,\n",
      "  'train_runtime': 94.5368,\n",
      "  'train_samples_per_second': 0.846,\n",
      "  'train_steps_per_second': 0.423},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.52,\n",
      "  'eval_in_loss': 1.0836914777755737,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2501,\n",
      "  'eval_in_samples_per_second': 11.764,\n",
      "  'eval_in_steps_per_second': 1.647,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.8095176815986633,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.3211,\n",
      "  'eval_out_samples_per_second': 11.571,\n",
      "  'eval_out_steps_per_second': 1.62,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.37054455280303955,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 94.5837,\n",
      "  'train_samples_per_second': 0.846,\n",
      "  'train_steps_per_second': 0.423},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.38,\n",
      "  'eval_in_loss': 1.1637927293777466,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 4.2742,\n",
      "  'eval_in_samples_per_second': 11.698,\n",
      "  'eval_in_steps_per_second': 1.638,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.8181593418121338,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 4.3398,\n",
      "  'eval_out_samples_per_second': 11.521,\n",
      "  'eval_out_steps_per_second': 1.613,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'total_flos': 74650390364160.0,\n",
      "  'train_loss': 0.2901137759909034,\n",
      "  'train_peak_memory_gb': 3.9469785690307617,\n",
      "  'train_runtime': 94.4379,\n",
      "  'train_samples_per_second': 0.847,\n",
      "  'train_steps_per_second': 0.424},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.5,\n",
      "  'eval_in_loss': 1.019397497177124,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 12.8705,\n",
      "  'eval_in_samples_per_second': 3.885,\n",
      "  'eval_in_steps_per_second': 0.544,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7381919622421265,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.0075,\n",
      "  'eval_out_samples_per_second': 3.844,\n",
      "  'eval_out_steps_per_second': 0.538,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 1.06295298486948,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 181.1924,\n",
      "  'train_samples_per_second': 0.883,\n",
      "  'train_steps_per_second': 0.221},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.46,\n",
      "  'eval_in_loss': 1.0978542566299438,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 14.5632,\n",
      "  'eval_in_samples_per_second': 3.433,\n",
      "  'eval_in_steps_per_second': 0.481,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.7790064215660095,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 14.5629,\n",
      "  'eval_out_samples_per_second': 3.433,\n",
      "  'eval_out_steps_per_second': 0.481,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.358922915160656,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 211.4514,\n",
      "  'train_samples_per_second': 0.757,\n",
      "  'train_steps_per_second': 0.189},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.36,\n",
      "  'eval_in_loss': 1.005492091178894,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 13.7665,\n",
      "  'eval_in_samples_per_second': 3.632,\n",
      "  'eval_in_steps_per_second': 0.508,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.7064031362533569,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.759,\n",
      "  'eval_out_samples_per_second': 3.634,\n",
      "  'eval_out_steps_per_second': 0.509,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.6193244688212871,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 203.4422,\n",
      "  'train_samples_per_second': 0.786,\n",
      "  'train_steps_per_second': 0.197},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.48,\n",
      "  'eval_in_loss': 1.0457133054733276,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 13.6248,\n",
      "  'eval_in_samples_per_second': 3.67,\n",
      "  'eval_in_steps_per_second': 0.514,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7180789113044739,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.6029,\n",
      "  'eval_out_samples_per_second': 3.676,\n",
      "  'eval_out_steps_per_second': 0.515,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.41961178332567217,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 196.7181,\n",
      "  'train_samples_per_second': 0.813,\n",
      "  'train_steps_per_second': 0.203},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.36,\n",
      "  'eval_in_loss': 1.0281023979187012,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 13.5874,\n",
      "  'eval_in_samples_per_second': 3.68,\n",
      "  'eval_in_steps_per_second': 0.515,\n",
      "  'eval_out_accuracy': 0.6,\n",
      "  'eval_out_loss': 0.7273518443107605,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.9124,\n",
      "  'eval_out_samples_per_second': 3.594,\n",
      "  'eval_out_steps_per_second': 0.503,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.5188165992498398,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 194.6394,\n",
      "  'train_samples_per_second': 0.822,\n",
      "  'train_steps_per_second': 0.206},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.38,\n",
      "  'eval_in_loss': 1.1286262273788452,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 13.4997,\n",
      "  'eval_in_samples_per_second': 3.704,\n",
      "  'eval_in_steps_per_second': 0.519,\n",
      "  'eval_out_accuracy': 0.58,\n",
      "  'eval_out_loss': 0.8094373345375061,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.5111,\n",
      "  'eval_out_samples_per_second': 3.701,\n",
      "  'eval_out_steps_per_second': 0.518,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.5932341076433658,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 189.0167,\n",
      "  'train_samples_per_second': 0.846,\n",
      "  'train_steps_per_second': 0.212},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.32,\n",
      "  'eval_in_loss': 1.008223056793213,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 13.717,\n",
      "  'eval_in_samples_per_second': 3.645,\n",
      "  'eval_in_steps_per_second': 0.51,\n",
      "  'eval_out_accuracy': 0.54,\n",
      "  'eval_out_loss': 0.7033131122589111,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.7395,\n",
      "  'eval_out_samples_per_second': 3.639,\n",
      "  'eval_out_steps_per_second': 0.509,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 1.3020227342844009,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 187.968,\n",
      "  'train_samples_per_second': 0.851,\n",
      "  'train_steps_per_second': 0.213},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.34,\n",
      "  'eval_in_loss': 0.9921084642410278,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 15.0401,\n",
      "  'eval_in_samples_per_second': 3.324,\n",
      "  'eval_in_steps_per_second': 0.465,\n",
      "  'eval_out_accuracy': 0.5,\n",
      "  'eval_out_loss': 0.6938083171844482,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 14.9814,\n",
      "  'eval_out_samples_per_second': 3.337,\n",
      "  'eval_out_steps_per_second': 0.467,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.8273517176508903,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 218.5081,\n",
      "  'train_samples_per_second': 0.732,\n",
      "  'train_steps_per_second': 0.183},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.38,\n",
      "  'eval_in_loss': 1.1400256156921387,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 13.4871,\n",
      "  'eval_in_samples_per_second': 3.707,\n",
      "  'eval_in_steps_per_second': 0.519,\n",
      "  'eval_out_accuracy': 0.6,\n",
      "  'eval_out_loss': 0.783449649810791,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 13.5063,\n",
      "  'eval_out_samples_per_second': 3.702,\n",
      "  'eval_out_steps_per_second': 0.518,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.6654212914407254,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 190.4649,\n",
      "  'train_samples_per_second': 0.84,\n",
      "  'train_steps_per_second': 0.21},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.38,\n",
      "  'eval_in_loss': 1.1548206806182861,\n",
      "  'eval_in_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_in_runtime': 14.5733,\n",
      "  'eval_in_samples_per_second': 3.431,\n",
      "  'eval_in_steps_per_second': 0.48,\n",
      "  'eval_out_accuracy': 0.6,\n",
      "  'eval_out_loss': 0.7862339615821838,\n",
      "  'eval_out_peak_memory_gb': 2.3248257637023926,\n",
      "  'eval_out_runtime': 14.5156,\n",
      "  'eval_out_samples_per_second': 3.445,\n",
      "  'eval_out_steps_per_second': 0.482,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'total_flos': 149300780728320.0,\n",
      "  'train_loss': 0.1728431588038802,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 198.3198,\n",
      "  'train_samples_per_second': 0.807,\n",
      "  'train_steps_per_second': 0.202},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.44,\n",
      "  'eval_in_loss': 0.9869996905326843,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 6.1494,\n",
      "  'eval_in_samples_per_second': 8.131,\n",
      "  'eval_in_steps_per_second': 2.114,\n",
      "  'eval_out_accuracy': 0.54,\n",
      "  'eval_out_loss': 0.7111024260520935,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 6.2105,\n",
      "  'eval_out_samples_per_second': 8.051,\n",
      "  'eval_out_steps_per_second': 2.093,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.726583632081747,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 183.8361,\n",
      "  'train_samples_per_second': 1.741,\n",
      "  'train_steps_per_second': 0.435},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.36,\n",
      "  'eval_in_loss': 0.9766526818275452,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.1218,\n",
      "  'eval_in_samples_per_second': 9.762,\n",
      "  'eval_in_steps_per_second': 2.538,\n",
      "  'eval_out_accuracy': 0.5,\n",
      "  'eval_out_loss': 0.6994683742523193,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.2021,\n",
      "  'eval_out_samples_per_second': 9.611,\n",
      "  'eval_out_steps_per_second': 2.499,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.7210132364183665,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 182.1671,\n",
      "  'train_samples_per_second': 1.757,\n",
      "  'train_steps_per_second': 0.439},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.46,\n",
      "  'eval_in_loss': 1.0281208753585815,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 6.7393,\n",
      "  'eval_in_samples_per_second': 7.419,\n",
      "  'eval_in_steps_per_second': 1.929,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.713161289691925,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 6.6219,\n",
      "  'eval_out_samples_per_second': 7.551,\n",
      "  'eval_out_steps_per_second': 1.963,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.4933850301429629,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 196.4757,\n",
      "  'train_samples_per_second': 1.629,\n",
      "  'train_steps_per_second': 0.407},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.42,\n",
      "  'eval_in_loss': 1.241722583770752,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.3529,\n",
      "  'eval_in_samples_per_second': 9.341,\n",
      "  'eval_in_steps_per_second': 2.429,\n",
      "  'eval_out_accuracy': 0.48,\n",
      "  'eval_out_loss': 0.9453263878822327,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.2958,\n",
      "  'eval_out_samples_per_second': 9.441,\n",
      "  'eval_out_steps_per_second': 2.455,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.5339707661420107,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 158.89,\n",
      "  'train_samples_per_second': 2.014,\n",
      "  'train_steps_per_second': 0.503},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.34,\n",
      "  'eval_in_loss': 1.0142260789871216,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.93,\n",
      "  'eval_in_samples_per_second': 8.432,\n",
      "  'eval_in_steps_per_second': 2.192,\n",
      "  'eval_out_accuracy': 0.62,\n",
      "  'eval_out_loss': 0.7446430921554565,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.8219,\n",
      "  'eval_out_samples_per_second': 8.588,\n",
      "  'eval_out_steps_per_second': 2.233,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.7234230455011129,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 171.3139,\n",
      "  'train_samples_per_second': 1.868,\n",
      "  'train_steps_per_second': 0.467},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.52,\n",
      "  'eval_in_loss': 1.0330554246902466,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.0739,\n",
      "  'eval_in_samples_per_second': 9.854,\n",
      "  'eval_in_steps_per_second': 2.562,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.7849755883216858,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.2081,\n",
      "  'eval_out_samples_per_second': 9.601,\n",
      "  'eval_out_steps_per_second': 2.496,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.5143752258270979,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 155.9576,\n",
      "  'train_samples_per_second': 2.052,\n",
      "  'train_steps_per_second': 0.513},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.4,\n",
      "  'eval_in_loss': 0.9891657829284668,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.5073,\n",
      "  'eval_in_samples_per_second': 9.079,\n",
      "  'eval_in_steps_per_second': 2.36,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7262564897537231,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.4402,\n",
      "  'eval_out_samples_per_second': 9.191,\n",
      "  'eval_out_steps_per_second': 2.39,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.7012737464159727,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 183.6348,\n",
      "  'train_samples_per_second': 1.743,\n",
      "  'train_steps_per_second': 0.436},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.48,\n",
      "  'eval_in_loss': 1.1733620166778564,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.0815,\n",
      "  'eval_in_samples_per_second': 9.84,\n",
      "  'eval_in_steps_per_second': 2.558,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.7927089929580688,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.1884,\n",
      "  'eval_out_samples_per_second': 9.637,\n",
      "  'eval_out_steps_per_second': 2.506,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.4528445038944483,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 165.4173,\n",
      "  'train_samples_per_second': 1.935,\n",
      "  'train_steps_per_second': 0.484},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.34,\n",
      "  'eval_in_loss': 0.9981803894042969,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.1149,\n",
      "  'eval_in_samples_per_second': 9.775,\n",
      "  'eval_in_steps_per_second': 2.542,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.710095226764679,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.1252,\n",
      "  'eval_out_samples_per_second': 9.756,\n",
      "  'eval_out_steps_per_second': 2.536,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.7075376685708761,\n",
      "  'train_peak_memory_gb': 6.639483451843262,\n",
      "  'train_runtime': 158.9452,\n",
      "  'train_samples_per_second': 2.013,\n",
      "  'train_steps_per_second': 0.503},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.46,\n",
      "  'eval_in_loss': 1.1560779809951782,\n",
      "  'eval_in_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_in_runtime': 5.4333,\n",
      "  'eval_in_samples_per_second': 9.203,\n",
      "  'eval_in_steps_per_second': 2.393,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7630157470703125,\n",
      "  'eval_out_peak_memory_gb': 1.7896389961242676,\n",
      "  'eval_out_runtime': 5.4331,\n",
      "  'eval_out_samples_per_second': 9.203,\n",
      "  'eval_out_steps_per_second': 2.393,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'total_flos': 298601561456640.0,\n",
      "  'train_loss': 0.30088053867220876,\n",
      "  'train_peak_memory_gb': 6.6394758224487305,\n",
      "  'train_runtime': 177.4945,\n",
      "  'train_samples_per_second': 1.803,\n",
      "  'train_steps_per_second': 0.451},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.34,\n",
      "  'eval_in_loss': 0.9728612303733826,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.4335,\n",
      "  'eval_in_samples_per_second': 11.278,\n",
      "  'eval_in_steps_per_second': 5.639,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.7060052752494812,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.5057,\n",
      "  'eval_out_samples_per_second': 11.097,\n",
      "  'eval_out_steps_per_second': 5.549,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.8095592551631853,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 171.8619,\n",
      "  'train_samples_per_second': 3.724,\n",
      "  'train_steps_per_second': 1.862},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.54,\n",
      "  'eval_in_loss': 1.0118420124053955,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.3288,\n",
      "  'eval_in_samples_per_second': 11.551,\n",
      "  'eval_in_steps_per_second': 5.775,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7130058407783508,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.3445,\n",
      "  'eval_out_samples_per_second': 11.509,\n",
      "  'eval_out_steps_per_second': 5.754,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.5107013254892081,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 171.445,\n",
      "  'train_samples_per_second': 3.733,\n",
      "  'train_steps_per_second': 1.866},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.46,\n",
      "  'eval_in_loss': 1.0250109434127808,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.3798,\n",
      "  'eval_in_samples_per_second': 11.416,\n",
      "  'eval_in_steps_per_second': 5.708,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7398499846458435,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.4203,\n",
      "  'eval_out_samples_per_second': 11.312,\n",
      "  'eval_out_steps_per_second': 5.656,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.5499713982455432,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 172.4553,\n",
      "  'train_samples_per_second': 3.711,\n",
      "  'train_steps_per_second': 1.856},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.4,\n",
      "  'eval_in_loss': 1.266458511352539,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.3545,\n",
      "  'eval_in_samples_per_second': 11.482,\n",
      "  'eval_in_steps_per_second': 5.741,\n",
      "  'eval_out_accuracy': 0.6,\n",
      "  'eval_out_loss': 0.7862338423728943,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.3589,\n",
      "  'eval_out_samples_per_second': 11.471,\n",
      "  'eval_out_steps_per_second': 5.735,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.5290714550530538,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 172.3795,\n",
      "  'train_samples_per_second': 3.713,\n",
      "  'train_steps_per_second': 1.856},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.46,\n",
      "  'eval_in_loss': 0.9833840131759644,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.3591,\n",
      "  'eval_in_samples_per_second': 11.47,\n",
      "  'eval_in_steps_per_second': 5.735,\n",
      "  'eval_out_accuracy': 0.5,\n",
      "  'eval_out_loss': 0.7989174127578735,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.4051,\n",
      "  'eval_out_samples_per_second': 11.35,\n",
      "  'eval_out_steps_per_second': 5.675,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.7427294386085123,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 173.3527,\n",
      "  'train_samples_per_second': 3.692,\n",
      "  'train_steps_per_second': 1.846},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.44,\n",
      "  'eval_in_loss': 1.0197787284851074,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.3928,\n",
      "  'eval_in_samples_per_second': 11.382,\n",
      "  'eval_in_steps_per_second': 5.691,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7419705986976624,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.2942,\n",
      "  'eval_out_samples_per_second': 11.644,\n",
      "  'eval_out_steps_per_second': 5.822,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.5030025236075744,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 170.5295,\n",
      "  'train_samples_per_second': 3.753,\n",
      "  'train_steps_per_second': 1.877},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.36,\n",
      "  'eval_in_loss': 0.9977893829345703,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.3681,\n",
      "  'eval_in_samples_per_second': 11.447,\n",
      "  'eval_in_steps_per_second': 5.723,\n",
      "  'eval_out_accuracy': 0.46,\n",
      "  'eval_out_loss': 0.728563129901886,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.3449,\n",
      "  'eval_out_samples_per_second': 11.508,\n",
      "  'eval_out_steps_per_second': 5.754,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.5637440774124116,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 172.0025,\n",
      "  'train_samples_per_second': 3.721,\n",
      "  'train_steps_per_second': 1.86},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.52,\n",
      "  'eval_in_loss': 1.0451923608779907,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.398,\n",
      "  'eval_in_samples_per_second': 11.369,\n",
      "  'eval_in_steps_per_second': 5.684,\n",
      "  'eval_out_accuracy': 0.52,\n",
      "  'eval_out_loss': 0.808504581451416,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.3444,\n",
      "  'eval_out_samples_per_second': 11.509,\n",
      "  'eval_out_steps_per_second': 5.754,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.7181582871126011,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 172.5387,\n",
      "  'train_samples_per_second': 3.709,\n",
      "  'train_steps_per_second': 1.855},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.4,\n",
      "  'eval_in_loss': 0.8553858995437622,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.358,\n",
      "  'eval_in_samples_per_second': 11.473,\n",
      "  'eval_in_steps_per_second': 5.737,\n",
      "  'eval_out_accuracy': 0.56,\n",
      "  'eval_out_loss': 0.7195879220962524,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.3426,\n",
      "  'eval_out_samples_per_second': 11.514,\n",
      "  'eval_out_steps_per_second': 5.757,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.9048312320373952,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 171.344,\n",
      "  'train_samples_per_second': 3.735,\n",
      "  'train_steps_per_second': 1.868},\n",
      " {'epoch': 40.0,\n",
      "  'eval_in_accuracy': 0.54,\n",
      "  'eval_in_loss': 0.9881778955459595,\n",
      "  'eval_in_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_in_runtime': 4.4521,\n",
      "  'eval_in_samples_per_second': 11.231,\n",
      "  'eval_in_steps_per_second': 5.615,\n",
      "  'eval_out_accuracy': 0.58,\n",
      "  'eval_out_loss': 0.7286532521247864,\n",
      "  'eval_out_peak_memory_gb': 1.522045612335205,\n",
      "  'eval_out_runtime': 4.3731,\n",
      "  'eval_out_samples_per_second': 11.434,\n",
      "  'eval_out_steps_per_second': 5.717,\n",
      "  'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'total_flos': 597203122913280.0,\n",
      "  'train_loss': 0.8250088006956503,\n",
      "  'train_peak_memory_gb': 3.9469709396362305,\n",
      "  'train_runtime': 171.6695,\n",
      "  'train_samples_per_second': 3.728,\n",
      "  'train_steps_per_second': 1.864}]\n",
      "Training histories:\n",
      "[{'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.6312,\n",
      "                 0.7324,\n",
      "                 0.7742,\n",
      "                 0.5532,\n",
      "                 0.4992,\n",
      "                 0.4947,\n",
      "                 0.5642,\n",
      "                 0.5422,\n",
      "                 0.8834,\n",
      "                 0.5077,\n",
      "                 0.6308,\n",
      "                 0.6453,\n",
      "                 0.7662,\n",
      "                 0.5244,\n",
      "                 0.4855,\n",
      "                 0.5794,\n",
      "                 0.6832,\n",
      "                 0.5756,\n",
      "                 0.4283,\n",
      "                 0.4829,\n",
      "                 0.6633,\n",
      "                 0.5634,\n",
      "                 0.7014,\n",
      "                 0.6775,\n",
      "                 0.4949,\n",
      "                 0.5029,\n",
      "                 0.5822,\n",
      "                 0.3485,\n",
      "                 0.4139,\n",
      "                 0.5447,\n",
      "                 0.5568,\n",
      "                 0.6821,\n",
      "                 0.5802,\n",
      "                 0.4762,\n",
      "                 0.5834,\n",
      "                 0.5035,\n",
      "                 0.5457,\n",
      "                 0.5796,\n",
      "                 0.4576,\n",
      "                 0.6128],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.57745760679245,\n",
      "               0.5753026008605957,\n",
      "               0.5721268057823181,\n",
      "               0.5688502788543701,\n",
      "               0.5658209919929504,\n",
      "               0.5632616281509399,\n",
      "               0.5612854957580566,\n",
      "               0.5592943429946899,\n",
      "               0.5573409199714661,\n",
      "               0.5555916428565979,\n",
      "               0.5542250871658325,\n",
      "               0.5528786778450012,\n",
      "               0.5516281127929688,\n",
      "               0.5504664182662964,\n",
      "               0.5493770837783813,\n",
      "               0.5483916997909546,\n",
      "               0.54752516746521,\n",
      "               0.5467569231987,\n",
      "               0.5460313558578491,\n",
      "               0.545356035232544,\n",
      "               0.544727623462677,\n",
      "               0.5441800355911255,\n",
      "               0.5437018871307373,\n",
      "               0.5433425903320312,\n",
      "               0.5430310368537903,\n",
      "               0.5427398681640625,\n",
      "               0.5424655079841614,\n",
      "               0.5422146916389465,\n",
      "               0.5419927835464478,\n",
      "               0.5417889356613159,\n",
      "               0.5416162610054016,\n",
      "               0.5414559245109558,\n",
      "               0.5413187742233276,\n",
      "               0.5411929488182068,\n",
      "               0.5410882234573364,\n",
      "               0.5410073399543762,\n",
      "               0.5409406423568726,\n",
      "               0.5408928990364075,\n",
      "               0.5408695340156555]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.4135,\n",
      "                 0.482,\n",
      "                 0.429,\n",
      "                 0.4344,\n",
      "                 0.4843,\n",
      "                 0.6284,\n",
      "                 0.4642,\n",
      "                 0.3339,\n",
      "                 0.2347,\n",
      "                 0.3709,\n",
      "                 0.1773,\n",
      "                 0.2497,\n",
      "                 0.1394,\n",
      "                 0.1712,\n",
      "                 0.2694,\n",
      "                 0.1806,\n",
      "                 0.1768,\n",
      "                 0.1637,\n",
      "                 0.1732,\n",
      "                 0.1454,\n",
      "                 0.1511,\n",
      "                 0.1463,\n",
      "                 0.131,\n",
      "                 0.1749,\n",
      "                 0.1515,\n",
      "                 0.0751,\n",
      "                 0.1411,\n",
      "                 0.1275,\n",
      "                 0.1412,\n",
      "                 0.0912,\n",
      "                 0.1041,\n",
      "                 0.0865,\n",
      "                 0.2591,\n",
      "                 0.0651,\n",
      "                 0.1411,\n",
      "                 0.0593,\n",
      "                 0.0918,\n",
      "                 0.1204,\n",
      "                 0.1197,\n",
      "                 0.1258],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5800433158874512,\n",
      "               0.5829126834869385,\n",
      "               0.5875214338302612,\n",
      "               0.5940467715263367,\n",
      "               0.600712239742279,\n",
      "               0.6076474785804749,\n",
      "               0.6147996783256531,\n",
      "               0.6221701502799988,\n",
      "               0.6296199560165405,\n",
      "               0.6372594833374023,\n",
      "               0.6449936032295227,\n",
      "               0.652799129486084,\n",
      "               0.6606515049934387,\n",
      "               0.668389618396759,\n",
      "               0.6760823130607605,\n",
      "               0.6836885213851929,\n",
      "               0.6912075281143188,\n",
      "               0.6986724138259888,\n",
      "               0.7059646248817444,\n",
      "               0.7130813598632812,\n",
      "               0.7199665904045105,\n",
      "               0.726604163646698,\n",
      "               0.7330420613288879,\n",
      "               0.7391562461853027,\n",
      "               0.7449963688850403,\n",
      "               0.7505005598068237,\n",
      "               0.7556679844856262,\n",
      "               0.7605023384094238,\n",
      "               0.764994740486145,\n",
      "               0.7691071629524231,\n",
      "               0.7728503942489624,\n",
      "               0.7761889100074768,\n",
      "               0.7791367769241333,\n",
      "               0.7816740274429321,\n",
      "               0.7838196754455566,\n",
      "               0.7855566740036011,\n",
      "               0.7868642807006836,\n",
      "               0.7877294421195984,\n",
      "               0.7881677746772766]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [1.2283,\n",
      "                 1.369,\n",
      "                 1.0327,\n",
      "                 1.058,\n",
      "                 1.1464,\n",
      "                 1.3396,\n",
      "                 1.0699,\n",
      "                 0.9721,\n",
      "                 1.1226,\n",
      "                 0.9041,\n",
      "                 1.022,\n",
      "                 0.9141,\n",
      "                 0.8867,\n",
      "                 0.7913,\n",
      "                 0.6398,\n",
      "                 0.9533,\n",
      "                 0.5755,\n",
      "                 0.7384,\n",
      "                 0.7177,\n",
      "                 0.6744,\n",
      "                 0.6754,\n",
      "                 0.6673,\n",
      "                 0.6589,\n",
      "                 0.7339,\n",
      "                 0.6888,\n",
      "                 0.6371,\n",
      "                 0.6359,\n",
      "                 0.5092,\n",
      "                 0.5839,\n",
      "                 0.4511,\n",
      "                 0.4259,\n",
      "                 0.3569,\n",
      "                 0.4315,\n",
      "                 0.6399,\n",
      "                 0.4088,\n",
      "                 0.5726,\n",
      "                 0.416,\n",
      "                 0.632,\n",
      "                 0.5981,\n",
      "                 0.5735],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5771386623382568,\n",
      "               0.5742751359939575,\n",
      "               0.5701751112937927,\n",
      "               0.5652195811271667,\n",
      "               0.5610108375549316,\n",
      "               0.5573500394821167,\n",
      "               0.554232656955719,\n",
      "               0.5516671538352966,\n",
      "               0.549593448638916,\n",
      "               0.5479317307472229,\n",
      "               0.5466881394386292,\n",
      "               0.54582679271698,\n",
      "               0.5452815294265747,\n",
      "               0.5450116395950317,\n",
      "               0.5450016260147095,\n",
      "               0.5452372431755066,\n",
      "               0.5456898212432861,\n",
      "               0.5463075637817383,\n",
      "               0.5470708608627319,\n",
      "               0.5479602813720703,\n",
      "               0.548949658870697,\n",
      "               0.5499939322471619,\n",
      "               0.5511118769645691,\n",
      "               0.5522711277008057,\n",
      "               0.5534204840660095,\n",
      "               0.5545507669448853,\n",
      "               0.5556665658950806,\n",
      "               0.5567505955696106,\n",
      "               0.5578042268753052,\n",
      "               0.5587968826293945,\n",
      "               0.5597243309020996,\n",
      "               0.5605717301368713,\n",
      "               0.5613225698471069,\n",
      "               0.5619790554046631,\n",
      "               0.5625424385070801,\n",
      "               0.5629976987838745,\n",
      "               0.563339114189148,\n",
      "               0.5635687112808228,\n",
      "               0.5636848211288452]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.6213,\n",
      "                 0.7618,\n",
      "                 0.3657,\n",
      "                 0.4952,\n",
      "                 0.5912,\n",
      "                 0.7154,\n",
      "                 0.5757,\n",
      "                 0.4783,\n",
      "                 0.2821,\n",
      "                 0.4491,\n",
      "                 0.2829,\n",
      "                 0.4355,\n",
      "                 0.3569,\n",
      "                 0.3086,\n",
      "                 0.2123,\n",
      "                 0.3556,\n",
      "                 0.2307,\n",
      "                 0.3531,\n",
      "                 0.1937,\n",
      "                 0.1642,\n",
      "                 0.1464,\n",
      "                 0.2993,\n",
      "                 0.193,\n",
      "                 0.2964,\n",
      "                 0.2952,\n",
      "                 0.362,\n",
      "                 0.1489,\n",
      "                 0.1077,\n",
      "                 0.1659,\n",
      "                 0.2074,\n",
      "                 0.1443,\n",
      "                 0.1277,\n",
      "                 0.1148,\n",
      "                 0.1037,\n",
      "                 0.0773,\n",
      "                 0.1214,\n",
      "                 0.2151,\n",
      "                 0.1523,\n",
      "                 0.2362,\n",
      "                 0.1074],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5772839784622192,\n",
      "               0.5746544003486633,\n",
      "               0.5709278583526611,\n",
      "               0.5663113594055176,\n",
      "               0.5623015761375427,\n",
      "               0.5587906837463379,\n",
      "               0.5557098388671875,\n",
      "               0.5530454516410828,\n",
      "               0.5508062839508057,\n",
      "               0.5488966703414917,\n",
      "               0.5472909212112427,\n",
      "               0.5459972620010376,\n",
      "               0.5449846386909485,\n",
      "               0.5441852807998657,\n",
      "               0.5436305403709412,\n",
      "               0.5432851314544678,\n",
      "               0.5431333780288696,\n",
      "               0.5431534051895142,\n",
      "               0.5433169603347778,\n",
      "               0.5435582995414734,\n",
      "               0.5438876748085022,\n",
      "               0.5442672371864319,\n",
      "               0.5446951985359192,\n",
      "               0.5451942682266235,\n",
      "               0.5457212924957275,\n",
      "               0.5462678074836731,\n",
      "               0.5468186140060425,\n",
      "               0.5473682880401611,\n",
      "               0.5479134321212769,\n",
      "               0.548454999923706,\n",
      "               0.548977255821228,\n",
      "               0.5494590997695923,\n",
      "               0.5498985052108765,\n",
      "               0.5502764582633972,\n",
      "               0.5506018400192261,\n",
      "               0.5508698225021362,\n",
      "               0.5510712265968323,\n",
      "               0.5512067675590515,\n",
      "               0.5512754321098328]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.8452,\n",
      "                 0.8579,\n",
      "                 0.9053,\n",
      "                 0.9095,\n",
      "                 0.8316,\n",
      "                 0.741,\n",
      "                 0.5246,\n",
      "                 0.8124,\n",
      "                 0.7938,\n",
      "                 0.6524,\n",
      "                 0.5112,\n",
      "                 0.7641,\n",
      "                 0.6185,\n",
      "                 0.7429,\n",
      "                 0.9017,\n",
      "                 0.6339,\n",
      "                 0.7951,\n",
      "                 0.4441,\n",
      "                 0.5471,\n",
      "                 0.6513,\n",
      "                 0.52,\n",
      "                 0.5888,\n",
      "                 0.828,\n",
      "                 0.7157,\n",
      "                 0.4868,\n",
      "                 0.6737,\n",
      "                 0.6117,\n",
      "                 0.8712,\n",
      "                 0.6982,\n",
      "                 0.6056,\n",
      "                 0.4736,\n",
      "                 0.5554,\n",
      "                 0.6653,\n",
      "                 0.5489,\n",
      "                 0.6074,\n",
      "                 0.507,\n",
      "                 0.5893,\n",
      "                 0.5796,\n",
      "                 0.6356,\n",
      "                 0.4672],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5783931612968445,\n",
      "               0.5775853395462036,\n",
      "               0.5759877562522888,\n",
      "               0.5739609003067017,\n",
      "               0.5720902681350708,\n",
      "               0.5703567862510681,\n",
      "               0.5692214369773865,\n",
      "               0.5679164528846741,\n",
      "               0.5668379664421082,\n",
      "               0.5658328533172607,\n",
      "               0.5650237798690796,\n",
      "               0.5643936395645142,\n",
      "               0.5639214515686035,\n",
      "               0.5636312365531921,\n",
      "               0.5633388757705688,\n",
      "               0.5630456209182739,\n",
      "               0.5629679560661316,\n",
      "               0.5629730820655823,\n",
      "               0.5630545616149902,\n",
      "               0.5630088448524475,\n",
      "               0.5630453824996948,\n",
      "               0.5630077123641968,\n",
      "               0.5630012154579163,\n",
      "               0.5629836916923523,\n",
      "               0.5629872679710388,\n",
      "               0.5630146265029907,\n",
      "               0.5630075931549072,\n",
      "               0.5629823803901672,\n",
      "               0.5629635453224182,\n",
      "               0.5629673600196838,\n",
      "               0.5629764795303345,\n",
      "               0.5629583597183228,\n",
      "               0.562947154045105,\n",
      "               0.5629202723503113,\n",
      "               0.5628922581672668,\n",
      "               0.5628783702850342,\n",
      "               0.5628758668899536,\n",
      "               0.5628695487976074,\n",
      "               0.5628679394721985]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [1.0271,\n",
      "                 1.0508,\n",
      "                 0.6861,\n",
      "                 1.1071,\n",
      "                 0.8515,\n",
      "                 0.9481,\n",
      "                 1.1609,\n",
      "                 1.0168,\n",
      "                 0.7708,\n",
      "                 0.8818,\n",
      "                 0.8065,\n",
      "                 1.0172,\n",
      "                 0.8324,\n",
      "                 1.0083,\n",
      "                 0.6834,\n",
      "                 0.9177,\n",
      "                 0.7926,\n",
      "                 0.6168,\n",
      "                 0.7904,\n",
      "                 0.4998,\n",
      "                 0.7728,\n",
      "                 0.7818,\n",
      "                 0.674,\n",
      "                 0.7934,\n",
      "                 0.701,\n",
      "                 0.9495,\n",
      "                 0.6223,\n",
      "                 0.6883,\n",
      "                 0.6891,\n",
      "                 0.5655,\n",
      "                 0.8068,\n",
      "                 0.6178,\n",
      "                 0.7032,\n",
      "                 0.6771,\n",
      "                 0.7184,\n",
      "                 0.6104,\n",
      "                 0.971,\n",
      "                 0.4958,\n",
      "                 0.7144,\n",
      "                 0.497],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5774034261703491,\n",
      "               0.5751227140426636,\n",
      "               0.5720127820968628,\n",
      "               0.5679027438163757,\n",
      "               0.5642406940460205,\n",
      "               0.5609256625175476,\n",
      "               0.5579118728637695,\n",
      "               0.5551793575286865,\n",
      "               0.5527599453926086,\n",
      "               0.5505743026733398,\n",
      "               0.5485742688179016,\n",
      "               0.546807587146759,\n",
      "               0.5452186465263367,\n",
      "               0.5437493920326233,\n",
      "               0.5424416065216064,\n",
      "               0.5413009524345398,\n",
      "               0.540241539478302,\n",
      "               0.5392888784408569,\n",
      "               0.5384644269943237,\n",
      "               0.537699818611145,\n",
      "               0.5370107889175415,\n",
      "               0.53639817237854,\n",
      "               0.5358468294143677,\n",
      "               0.5353357791900635,\n",
      "               0.5348953008651733,\n",
      "               0.5345160961151123,\n",
      "               0.5341753363609314,\n",
      "               0.5338764190673828,\n",
      "               0.5336061120033264,\n",
      "               0.5333613157272339,\n",
      "               0.5331544280052185,\n",
      "               0.5329753160476685,\n",
      "               0.532813310623169,\n",
      "               0.5326640009880066,\n",
      "               0.5325468182563782,\n",
      "               0.5324578285217285,\n",
      "               0.5323935747146606,\n",
      "               0.5323517918586731,\n",
      "               0.5323296785354614]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.6909,\n",
      "                 0.633,\n",
      "                 0.7314,\n",
      "                 0.7062,\n",
      "                 0.7149,\n",
      "                 0.9232,\n",
      "                 0.6162,\n",
      "                 0.7241,\n",
      "                 0.5093,\n",
      "                 0.6514,\n",
      "                 0.5256,\n",
      "                 0.7412,\n",
      "                 0.3923,\n",
      "                 0.5959,\n",
      "                 0.2743,\n",
      "                 0.3884,\n",
      "                 0.3381,\n",
      "                 0.708,\n",
      "                 0.3512,\n",
      "                 0.5149,\n",
      "                 0.2798,\n",
      "                 0.289,\n",
      "                 0.3162,\n",
      "                 0.2912,\n",
      "                 0.4472,\n",
      "                 0.4735,\n",
      "                 0.1602,\n",
      "                 0.1875,\n",
      "                 0.2396,\n",
      "                 0.2735,\n",
      "                 0.2018,\n",
      "                 0.2822,\n",
      "                 0.2659,\n",
      "                 0.1733,\n",
      "                 0.2319,\n",
      "                 0.1664,\n",
      "                 0.1441,\n",
      "                 0.149,\n",
      "                 0.1979,\n",
      "                 0.3215],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.577223002910614,\n",
      "               0.5745033025741577,\n",
      "               0.5706710815429688,\n",
      "               0.5661360025405884,\n",
      "               0.5622797608375549,\n",
      "               0.558931827545166,\n",
      "               0.5560763478279114,\n",
      "               0.5536550283432007,\n",
      "               0.5516756176948547,\n",
      "               0.5500530004501343,\n",
      "               0.5487426519393921,\n",
      "               0.5477367639541626,\n",
      "               0.547029972076416,\n",
      "               0.5465866923332214,\n",
      "               0.5464087724685669,\n",
      "               0.5464731454849243,\n",
      "               0.546707034111023,\n",
      "               0.5470935106277466,\n",
      "               0.5476102828979492,\n",
      "               0.5482468008995056,\n",
      "               0.5489684343338013,\n",
      "               0.5497610569000244,\n",
      "               0.5506206154823303,\n",
      "               0.5515117645263672,\n",
      "               0.5524206757545471,\n",
      "               0.5533221364021301,\n",
      "               0.554215669631958,\n",
      "               0.5550819635391235,\n",
      "               0.5559210777282715,\n",
      "               0.5567253232002258,\n",
      "               0.5574746131896973,\n",
      "               0.558164656162262,\n",
      "               0.5587800145149231,\n",
      "               0.5593053102493286,\n",
      "               0.5597644448280334,\n",
      "               0.5601328611373901,\n",
      "               0.5604076981544495,\n",
      "               0.560594916343689,\n",
      "               0.5606913566589355]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [1.0166,\n",
      "                 0.639,\n",
      "                 0.7197,\n",
      "                 0.8898,\n",
      "                 0.6759,\n",
      "                 0.7726,\n",
      "                 0.707,\n",
      "                 0.8278,\n",
      "                 0.7516,\n",
      "                 0.6857,\n",
      "                 0.9082,\n",
      "                 0.8764,\n",
      "                 0.6481,\n",
      "                 0.4878,\n",
      "                 0.7207,\n",
      "                 0.6992,\n",
      "                 0.5584,\n",
      "                 0.5925,\n",
      "                 0.8259,\n",
      "                 0.6185,\n",
      "                 0.6762,\n",
      "                 0.6784,\n",
      "                 0.6794,\n",
      "                 0.7607,\n",
      "                 0.7889,\n",
      "                 0.7086,\n",
      "                 0.6314,\n",
      "                 0.6456,\n",
      "                 0.7212,\n",
      "                 0.5655,\n",
      "                 0.4505,\n",
      "                 0.5683,\n",
      "                 0.6419,\n",
      "                 0.5309,\n",
      "                 0.5827,\n",
      "                 0.585,\n",
      "                 0.5112,\n",
      "                 0.6155,\n",
      "                 0.6602,\n",
      "                 0.6429],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5784352421760559,\n",
      "               0.5775484442710876,\n",
      "               0.5759924054145813,\n",
      "               0.5740283727645874,\n",
      "               0.5724844932556152,\n",
      "               0.5711098909378052,\n",
      "               0.5698261857032776,\n",
      "               0.568718433380127,\n",
      "               0.5678271055221558,\n",
      "               0.5667433738708496,\n",
      "               0.565539538860321,\n",
      "               0.5644543766975403,\n",
      "               0.5636143684387207,\n",
      "               0.5628136396408081,\n",
      "               0.5620425343513489,\n",
      "               0.5613253116607666,\n",
      "               0.5606541037559509,\n",
      "               0.5601129531860352,\n",
      "               0.559664249420166,\n",
      "               0.5591117143630981,\n",
      "               0.5586450695991516,\n",
      "               0.5582116842269897,\n",
      "               0.5578650236129761,\n",
      "               0.557528555393219,\n",
      "               0.5572671890258789,\n",
      "               0.556979775428772,\n",
      "               0.5567653775215149,\n",
      "               0.5566056370735168,\n",
      "               0.556500256061554,\n",
      "               0.5564818978309631,\n",
      "               0.5564190745353699,\n",
      "               0.5562999844551086,\n",
      "               0.5561889410018921,\n",
      "               0.5560709238052368,\n",
      "               0.5559744238853455,\n",
      "               0.5559052228927612,\n",
      "               0.5558550357818604,\n",
      "               0.5558179616928101,\n",
      "               0.5557945370674133]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.5794,\n",
      "                 0.7153,\n",
      "                 0.6459,\n",
      "                 0.5737,\n",
      "                 0.5543,\n",
      "                 0.6481,\n",
      "                 0.5427,\n",
      "                 0.5067,\n",
      "                 0.5374,\n",
      "                 0.4005,\n",
      "                 0.5081,\n",
      "                 0.4229,\n",
      "                 0.5823,\n",
      "                 0.4215,\n",
      "                 0.3442,\n",
      "                 0.3915,\n",
      "                 0.4871,\n",
      "                 0.2819,\n",
      "                 0.2768,\n",
      "                 0.3258,\n",
      "                 0.2385,\n",
      "                 0.3033,\n",
      "                 0.3924,\n",
      "                 0.2737,\n",
      "                 0.2279,\n",
      "                 0.279,\n",
      "                 0.3897,\n",
      "                 0.3064,\n",
      "                 0.2243,\n",
      "                 0.2485,\n",
      "                 0.1499,\n",
      "                 0.2688,\n",
      "                 0.3155,\n",
      "                 0.1299,\n",
      "                 0.2451,\n",
      "                 0.2026,\n",
      "                 0.3252,\n",
      "                 0.183,\n",
      "                 0.1977,\n",
      "                 0.1743],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5769845247268677,\n",
      "               0.5738494396209717,\n",
      "               0.5695074796676636,\n",
      "               0.5642703175544739,\n",
      "               0.5598158836364746,\n",
      "               0.5560208559036255,\n",
      "               0.5529075264930725,\n",
      "               0.5503559112548828,\n",
      "               0.5483397841453552,\n",
      "               0.5468184947967529,\n",
      "               0.5457097291946411,\n",
      "               0.5450308918952942,\n",
      "               0.5446909070014954,\n",
      "               0.5446426868438721,\n",
      "               0.5448622703552246,\n",
      "               0.5453435182571411,\n",
      "               0.5460290908813477,\n",
      "               0.5468777418136597,\n",
      "               0.5478724241256714,\n",
      "               0.5489851236343384,\n",
      "               0.550197958946228,\n",
      "               0.5514882802963257,\n",
      "               0.5528284311294556,\n",
      "               0.5542203187942505,\n",
      "               0.5556260943412781,\n",
      "               0.5570145845413208,\n",
      "               0.5583702325820923,\n",
      "               0.5596837401390076,\n",
      "               0.5609353184700012,\n",
      "               0.5621210932731628,\n",
      "               0.5632275938987732,\n",
      "               0.5642385482788086,\n",
      "               0.565144956111908,\n",
      "               0.565937340259552,\n",
      "               0.5666113495826721,\n",
      "               0.567160964012146,\n",
      "               0.5675755739212036,\n",
      "               0.5678541660308838,\n",
      "               0.5679937601089478]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 2,\n",
      "  'train_loss': [0.5069,\n",
      "                 0.5824,\n",
      "                 0.4401,\n",
      "                 0.5384,\n",
      "                 0.5339,\n",
      "                 0.5655,\n",
      "                 0.5335,\n",
      "                 0.6146,\n",
      "                 0.2668,\n",
      "                 0.3427,\n",
      "                 0.3465,\n",
      "                 0.2299,\n",
      "                 0.2054,\n",
      "                 0.3221,\n",
      "                 0.2628,\n",
      "                 0.2809,\n",
      "                 0.3508,\n",
      "                 0.301,\n",
      "                 0.2371,\n",
      "                 0.258,\n",
      "                 0.3519,\n",
      "                 0.2121,\n",
      "                 0.2552,\n",
      "                 0.1997,\n",
      "                 0.1234,\n",
      "                 0.1779,\n",
      "                 0.1478,\n",
      "                 0.245,\n",
      "                 0.1704,\n",
      "                 0.2287,\n",
      "                 0.1439,\n",
      "                 0.1104,\n",
      "                 0.1944,\n",
      "                 0.2103,\n",
      "                 0.183,\n",
      "                 0.1451,\n",
      "                 0.1555,\n",
      "                 0.175,\n",
      "                 0.2553,\n",
      "                 0.2001],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5800145864486694,\n",
      "               0.5828603506088257,\n",
      "               0.5874332189559937,\n",
      "               0.5939781069755554,\n",
      "               0.6008307337760925,\n",
      "               0.6078938245773315,\n",
      "               0.6150683760643005,\n",
      "               0.6224385499954224,\n",
      "               0.6298704147338867,\n",
      "               0.6374455094337463,\n",
      "               0.6452048420906067,\n",
      "               0.6530288457870483,\n",
      "               0.6609014272689819,\n",
      "               0.6686768531799316,\n",
      "               0.6763297915458679,\n",
      "               0.6839464902877808,\n",
      "               0.6915143728256226,\n",
      "               0.6989480257034302,\n",
      "               0.7063004374504089,\n",
      "               0.7134675979614258,\n",
      "               0.7203512191772461,\n",
      "               0.7269498705863953,\n",
      "               0.7334827780723572,\n",
      "               0.7398175001144409,\n",
      "               0.7458314299583435,\n",
      "               0.7515623569488525,\n",
      "               0.7569199800491333,\n",
      "               0.7619905471801758,\n",
      "               0.7667424082756042,\n",
      "               0.7710898518562317,\n",
      "               0.7751300930976868,\n",
      "               0.7787516713142395,\n",
      "               0.7819157838821411,\n",
      "               0.7846238017082214,\n",
      "               0.7869241237640381,\n",
      "               0.7887906432151794,\n",
      "               0.7902156114578247,\n",
      "               0.7911534905433655,\n",
      "               0.7916223406791687]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [1.2224,\n",
      "                 1.4478,\n",
      "                 1.4676,\n",
      "                 1.4441,\n",
      "                 1.163,\n",
      "                 0.8848,\n",
      "                 1.0591,\n",
      "                 0.8518,\n",
      "                 1.234,\n",
      "                 1.2591,\n",
      "                 0.9746,\n",
      "                 1.2376,\n",
      "                 0.9604,\n",
      "                 1.2548,\n",
      "                 1.1396,\n",
      "                 1.0842,\n",
      "                 0.9128,\n",
      "                 1.0897,\n",
      "                 0.9034,\n",
      "                 1.1852,\n",
      "                 0.9241,\n",
      "                 0.9791,\n",
      "                 1.1475,\n",
      "                 0.8935,\n",
      "                 1.2817,\n",
      "                 0.9612,\n",
      "                 0.9734,\n",
      "                 0.9973,\n",
      "                 0.9884,\n",
      "                 0.874,\n",
      "                 0.9618,\n",
      "                 1.0899,\n",
      "                 1.0143,\n",
      "                 0.9922,\n",
      "                 1.0066,\n",
      "                 1.0578,\n",
      "                 0.951,\n",
      "                 0.864,\n",
      "                 0.8403,\n",
      "                 0.9439],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.577569305896759,\n",
      "               0.5753837823867798,\n",
      "               0.5721549391746521,\n",
      "               0.5681445002555847,\n",
      "               0.5646713376045227,\n",
      "               0.5613385438919067,\n",
      "               0.5583629608154297,\n",
      "               0.5558174252510071,\n",
      "               0.5536221861839294,\n",
      "               0.5517794489860535,\n",
      "               0.5502282977104187,\n",
      "               0.5489041805267334,\n",
      "               0.5478266477584839,\n",
      "               0.5469524264335632,\n",
      "               0.546225905418396,\n",
      "               0.5456622242927551,\n",
      "               0.5452659726142883,\n",
      "               0.5449874997138977,\n",
      "               0.5447589159011841,\n",
      "               0.5445784330368042,\n",
      "               0.5444217324256897,\n",
      "               0.5443211197853088,\n",
      "               0.544257640838623,\n",
      "               0.5442264676094055,\n",
      "               0.5442485809326172,\n",
      "               0.5443121194839478,\n",
      "               0.5444250106811523,\n",
      "               0.5445584058761597,\n",
      "               0.5446906685829163,\n",
      "               0.5448068976402283,\n",
      "               0.544924259185791,\n",
      "               0.5450361967086792,\n",
      "               0.5451451539993286,\n",
      "               0.5452461242675781,\n",
      "               0.5453321933746338,\n",
      "               0.5454031825065613,\n",
      "               0.5454573035240173,\n",
      "               0.5454921126365662,\n",
      "               0.5455102920532227]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.6358,\n",
      "                 0.5231,\n",
      "                 0.6106,\n",
      "                 0.5103,\n",
      "                 0.4085,\n",
      "                 0.4487,\n",
      "                 0.4898,\n",
      "                 0.4126,\n",
      "                 0.502,\n",
      "                 0.3842,\n",
      "                 0.3796,\n",
      "                 0.4266,\n",
      "                 0.3673,\n",
      "                 0.326,\n",
      "                 0.3745,\n",
      "                 0.4579,\n",
      "                 0.3864,\n",
      "                 0.4041,\n",
      "                 0.3233,\n",
      "                 0.2465,\n",
      "                 0.3685,\n",
      "                 0.3925,\n",
      "                 0.3347,\n",
      "                 0.2409,\n",
      "                 0.3039,\n",
      "                 0.2472,\n",
      "                 0.3461,\n",
      "                 0.2456,\n",
      "                 0.2606,\n",
      "                 0.2503,\n",
      "                 0.3259,\n",
      "                 0.278,\n",
      "                 0.2806,\n",
      "                 0.2477,\n",
      "                 0.2611,\n",
      "                 0.2689,\n",
      "                 0.282,\n",
      "                 0.278,\n",
      "                 0.2853,\n",
      "                 0.2413],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5773841142654419,\n",
      "               0.5751552581787109,\n",
      "               0.5720146298408508,\n",
      "               0.5682531595230103,\n",
      "               0.5651419758796692,\n",
      "               0.5625118017196655,\n",
      "               0.5603581666946411,\n",
      "               0.5585804581642151,\n",
      "               0.5571657419204712,\n",
      "               0.556074857711792,\n",
      "               0.5552967190742493,\n",
      "               0.5547957420349121,\n",
      "               0.5545511245727539,\n",
      "               0.5545915365219116,\n",
      "               0.5548169016838074,\n",
      "               0.5551940202713013,\n",
      "               0.5557585954666138,\n",
      "               0.5564706325531006,\n",
      "               0.5572879910469055,\n",
      "               0.5582072734832764,\n",
      "               0.5591858625411987,\n",
      "               0.5602176785469055,\n",
      "               0.561281681060791,\n",
      "               0.5623401999473572,\n",
      "               0.5633953213691711,\n",
      "               0.56438809633255,\n",
      "               0.5653644800186157,\n",
      "               0.5663511753082275,\n",
      "               0.5673050880432129,\n",
      "               0.5681355595588684,\n",
      "               0.5689100027084351,\n",
      "               0.5696090459823608,\n",
      "               0.5702254772186279,\n",
      "               0.5707739591598511,\n",
      "               0.5712409615516663,\n",
      "               0.5716151595115662,\n",
      "               0.5718982219696045,\n",
      "               0.5720887184143066,\n",
      "               0.5721805691719055]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.8042,\n",
      "                 0.7852,\n",
      "                 0.6971,\n",
      "                 0.622,\n",
      "                 0.7218,\n",
      "                 0.8807,\n",
      "                 0.7829,\n",
      "                 0.501,\n",
      "                 0.5321,\n",
      "                 0.7489,\n",
      "                 0.7888,\n",
      "                 0.6617,\n",
      "                 0.7718,\n",
      "                 0.6152,\n",
      "                 0.6267,\n",
      "                 0.589,\n",
      "                 0.4766,\n",
      "                 0.6187,\n",
      "                 0.565,\n",
      "                 0.5977,\n",
      "                 0.6295,\n",
      "                 0.5882,\n",
      "                 0.4021,\n",
      "                 0.8136,\n",
      "                 0.4499,\n",
      "                 0.5123,\n",
      "                 0.4464,\n",
      "                 0.6594,\n",
      "                 0.4225,\n",
      "                 0.5333,\n",
      "                 0.814,\n",
      "                 0.7089,\n",
      "                 0.587,\n",
      "                 0.5718,\n",
      "                 0.7126,\n",
      "                 0.4443,\n",
      "                 0.5035,\n",
      "                 0.5757,\n",
      "                 0.5244,\n",
      "                 0.4868],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5777390599250793,\n",
      "               0.5761007070541382,\n",
      "               0.5734028220176697,\n",
      "               0.5697788000106812,\n",
      "               0.5665801763534546,\n",
      "               0.5636308789253235,\n",
      "               0.5610253214836121,\n",
      "               0.5587211847305298,\n",
      "               0.557073712348938,\n",
      "               0.5555174946784973,\n",
      "               0.5542246699333191,\n",
      "               0.5531597137451172,\n",
      "               0.5520978569984436,\n",
      "               0.5512951612472534,\n",
      "               0.550521731376648,\n",
      "               0.5497021675109863,\n",
      "               0.5489131808280945,\n",
      "               0.5481532216072083,\n",
      "               0.547478199005127,\n",
      "               0.546908438205719,\n",
      "               0.5463635921478271,\n",
      "               0.5458794832229614,\n",
      "               0.5454503297805786,\n",
      "               0.5450504422187805,\n",
      "               0.5446443557739258,\n",
      "               0.5442879796028137,\n",
      "               0.5439993739128113,\n",
      "               0.5437315702438354,\n",
      "               0.5435494184494019,\n",
      "               0.543379008769989,\n",
      "               0.5432256460189819,\n",
      "               0.5431105494499207,\n",
      "               0.5429884195327759,\n",
      "               0.542900562286377,\n",
      "               0.5428134202957153,\n",
      "               0.5427383780479431,\n",
      "               0.542678713798523,\n",
      "               0.5426376461982727,\n",
      "               0.5426168441772461]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.5307,\n",
      "                 0.5272,\n",
      "                 0.4937,\n",
      "                 0.6189,\n",
      "                 0.511,\n",
      "                 0.5598,\n",
      "                 0.5506,\n",
      "                 0.3264,\n",
      "                 0.4537,\n",
      "                 0.5884,\n",
      "                 0.5066,\n",
      "                 0.5453,\n",
      "                 0.3651,\n",
      "                 0.5449,\n",
      "                 0.4701,\n",
      "                 0.335,\n",
      "                 0.4129,\n",
      "                 0.4839,\n",
      "                 0.5156,\n",
      "                 0.3882,\n",
      "                 0.4146,\n",
      "                 0.444,\n",
      "                 0.3802,\n",
      "                 0.3334,\n",
      "                 0.3386,\n",
      "                 0.517,\n",
      "                 0.3007,\n",
      "                 0.3901,\n",
      "                 0.3341,\n",
      "                 0.3879,\n",
      "                 0.3348,\n",
      "                 0.3888,\n",
      "                 0.3641,\n",
      "                 0.3035,\n",
      "                 0.2741,\n",
      "                 0.3474,\n",
      "                 0.3264,\n",
      "                 0.277,\n",
      "                 0.3272,\n",
      "                 0.2725],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5776064991950989,\n",
      "               0.5755743384361267,\n",
      "               0.5728459358215332,\n",
      "               0.5693231821060181,\n",
      "               0.5662515163421631,\n",
      "               0.5635031461715698,\n",
      "               0.5609501600265503,\n",
      "               0.5586679577827454,\n",
      "               0.5566481351852417,\n",
      "               0.5548393130302429,\n",
      "               0.5533037185668945,\n",
      "               0.5519506335258484,\n",
      "               0.550713837146759,\n",
      "               0.5496796369552612,\n",
      "               0.5487216711044312,\n",
      "               0.5479110479354858,\n",
      "               0.547368049621582,\n",
      "               0.5469130277633667,\n",
      "               0.5465114712715149,\n",
      "               0.5461475849151611,\n",
      "               0.5457894802093506,\n",
      "               0.5454508066177368,\n",
      "               0.5451564192771912,\n",
      "               0.5448851585388184,\n",
      "               0.5446492433547974,\n",
      "               0.5444450378417969,\n",
      "               0.5443032383918762,\n",
      "               0.5441804528236389,\n",
      "               0.5440827012062073,\n",
      "               0.544010579586029,\n",
      "               0.5439523458480835,\n",
      "               0.5439082384109497,\n",
      "               0.5438650846481323,\n",
      "               0.5438312292098999,\n",
      "               0.5437965989112854,\n",
      "               0.5437706112861633,\n",
      "               0.5437448024749756,\n",
      "               0.5437343120574951,\n",
      "               0.5437290668487549]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.7273,\n",
      "                 0.7223,\n",
      "                 0.5416,\n",
      "                 0.6047,\n",
      "                 0.7058,\n",
      "                 0.5586,\n",
      "                 0.5188,\n",
      "                 0.4959,\n",
      "                 0.5859,\n",
      "                 0.5707,\n",
      "                 0.4451,\n",
      "                 0.6015,\n",
      "                 0.3749,\n",
      "                 0.5241,\n",
      "                 0.5257,\n",
      "                 0.5627,\n",
      "                 0.4114,\n",
      "                 0.4967,\n",
      "                 0.8078,\n",
      "                 0.3905,\n",
      "                 0.4434,\n",
      "                 0.5927,\n",
      "                 0.4998,\n",
      "                 0.4521,\n",
      "                 0.4604,\n",
      "                 0.4906,\n",
      "                 0.5257,\n",
      "                 0.48,\n",
      "                 0.6442,\n",
      "                 0.4065,\n",
      "                 0.4587,\n",
      "                 0.6778,\n",
      "                 0.3925,\n",
      "                 0.6082,\n",
      "                 0.475,\n",
      "                 0.4071,\n",
      "                 0.3952,\n",
      "                 0.3448,\n",
      "                 0.3277,\n",
      "                 0.4984],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5793064832687378,\n",
      "               0.5809338688850403,\n",
      "               0.5832210779190063,\n",
      "               0.5868502259254456,\n",
      "               0.5905934572219849,\n",
      "               0.5943803787231445,\n",
      "               0.5975360870361328,\n",
      "               0.6007155179977417,\n",
      "               0.603620171546936,\n",
      "               0.6066166162490845,\n",
      "               0.6095398664474487,\n",
      "               0.6125696897506714,\n",
      "               0.6154567003250122,\n",
      "               0.6180051565170288,\n",
      "               0.6202861070632935,\n",
      "               0.6228164434432983,\n",
      "               0.6247637867927551,\n",
      "               0.6268545389175415,\n",
      "               0.6288043856620789,\n",
      "               0.6304219365119934,\n",
      "               0.6318118572235107,\n",
      "               0.6326740384101868,\n",
      "               0.6337793469429016,\n",
      "               0.6345458626747131,\n",
      "               0.6351058483123779,\n",
      "               0.6355859041213989,\n",
      "               0.6360217332839966,\n",
      "               0.6365731954574585,\n",
      "               0.6369771957397461,\n",
      "               0.6372882127761841,\n",
      "               0.6375703811645508,\n",
      "               0.6377648115158081,\n",
      "               0.6379124522209167,\n",
      "               0.6380160450935364,\n",
      "               0.6380585432052612,\n",
      "               0.6381844282150269,\n",
      "               0.6383164525032043,\n",
      "               0.6384167671203613,\n",
      "               0.638466477394104]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.7196,\n",
      "                 0.9666,\n",
      "                 1.1684,\n",
      "                 0.7168,\n",
      "                 0.8783,\n",
      "                 0.8312,\n",
      "                 0.6329,\n",
      "                 1.0356,\n",
      "                 0.6081,\n",
      "                 0.8157,\n",
      "                 0.808,\n",
      "                 0.583,\n",
      "                 0.7138,\n",
      "                 0.5825,\n",
      "                 0.5579,\n",
      "                 0.6638,\n",
      "                 0.6751,\n",
      "                 0.5478,\n",
      "                 0.5021,\n",
      "                 0.5705,\n",
      "                 0.6157,\n",
      "                 0.5994,\n",
      "                 0.4268,\n",
      "                 0.5827,\n",
      "                 0.3456,\n",
      "                 0.4964,\n",
      "                 0.5026,\n",
      "                 0.4091,\n",
      "                 0.3905,\n",
      "                 0.5024,\n",
      "                 0.4849,\n",
      "                 0.4474,\n",
      "                 0.3282,\n",
      "                 0.418,\n",
      "                 0.3721,\n",
      "                 0.5287,\n",
      "                 0.4723,\n",
      "                 0.4435,\n",
      "                 0.3378,\n",
      "                 0.4475],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.580106258392334,\n",
      "               0.5832017660140991,\n",
      "               0.5879524946212769,\n",
      "               0.5947253704071045,\n",
      "               0.6018846035003662,\n",
      "               0.6093513369560242,\n",
      "               0.6170170903205872,\n",
      "               0.6246712803840637,\n",
      "               0.6323521137237549,\n",
      "               0.6400567293167114,\n",
      "               0.6477546691894531,\n",
      "               0.6553942561149597,\n",
      "               0.663148820400238,\n",
      "               0.6708930730819702,\n",
      "               0.6785968542098999,\n",
      "               0.6862348318099976,\n",
      "               0.6936647891998291,\n",
      "               0.7007880210876465,\n",
      "               0.7077010869979858,\n",
      "               0.714446485042572,\n",
      "               0.7209821939468384,\n",
      "               0.7273971438407898,\n",
      "               0.7336485981941223,\n",
      "               0.7395746111869812,\n",
      "               0.7452343106269836,\n",
      "               0.750449538230896,\n",
      "               0.7552200555801392,\n",
      "               0.7597914934158325,\n",
      "               0.764045238494873,\n",
      "               0.7680115699768066,\n",
      "               0.7715532183647156,\n",
      "               0.7746520042419434,\n",
      "               0.7773693799972534,\n",
      "               0.7796916961669922,\n",
      "               0.7816388010978699,\n",
      "               0.7831811904907227,\n",
      "               0.7843425869941711,\n",
      "               0.7851086854934692,\n",
      "               0.7854859232902527]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [1.2401,\n",
      "                 1.5679,\n",
      "                 1.5077,\n",
      "                 1.3562,\n",
      "                 1.4389,\n",
      "                 1.6838,\n",
      "                 1.4472,\n",
      "                 1.4852,\n",
      "                 1.4378,\n",
      "                 1.4222,\n",
      "                 1.2149,\n",
      "                 1.296,\n",
      "                 1.5158,\n",
      "                 1.4351,\n",
      "                 1.2444,\n",
      "                 1.5176,\n",
      "                 1.2326,\n",
      "                 1.275,\n",
      "                 1.4313,\n",
      "                 1.3538,\n",
      "                 1.2095,\n",
      "                 1.2063,\n",
      "                 1.208,\n",
      "                 1.4059,\n",
      "                 1.3113,\n",
      "                 1.2364,\n",
      "                 0.9212,\n",
      "                 1.2283,\n",
      "                 1.1532,\n",
      "                 1.4084,\n",
      "                 1.277,\n",
      "                 0.9686,\n",
      "                 1.0284,\n",
      "                 1.1361,\n",
      "                 1.2242,\n",
      "                 1.1541,\n",
      "                 1.3796,\n",
      "                 1.1438,\n",
      "                 1.0973,\n",
      "                 1.2796],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5787591338157654,\n",
      "               0.5791911482810974,\n",
      "               0.5798124670982361,\n",
      "               0.5806523561477661,\n",
      "               0.5816951990127563,\n",
      "               0.5828741788864136,\n",
      "               0.5839176774024963,\n",
      "               0.5849871039390564,\n",
      "               0.5858224630355835,\n",
      "               0.5867537260055542,\n",
      "               0.5876805186271667,\n",
      "               0.5883957147598267,\n",
      "               0.5891368985176086,\n",
      "               0.5898531079292297,\n",
      "               0.5907655954360962,\n",
      "               0.5917267799377441,\n",
      "               0.5926912426948547,\n",
      "               0.593543291091919,\n",
      "               0.5943965911865234,\n",
      "               0.5953012704849243,\n",
      "               0.5961297750473022,\n",
      "               0.5970145463943481,\n",
      "               0.5977205634117126,\n",
      "               0.5983311533927917,\n",
      "               0.5990194082260132,\n",
      "               0.5995965003967285,\n",
      "               0.6000654697418213,\n",
      "               0.6004995107650757,\n",
      "               0.6008971929550171,\n",
      "               0.6012852787971497,\n",
      "               0.6016523838043213,\n",
      "               0.6020196676254272,\n",
      "               0.602353572845459,\n",
      "               0.6026103496551514,\n",
      "               0.6027930378913879,\n",
      "               0.602933406829834,\n",
      "               0.6030368208885193,\n",
      "               0.60312420129776,\n",
      "               0.6031617522239685]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.7952,\n",
      "                 0.8267,\n",
      "                 0.8931,\n",
      "                 1.0042,\n",
      "                 0.7574,\n",
      "                 0.8874,\n",
      "                 0.8111,\n",
      "                 0.8057,\n",
      "                 0.771,\n",
      "                 0.7619,\n",
      "                 0.684,\n",
      "                 0.8331,\n",
      "                 0.7139,\n",
      "                 1.0306,\n",
      "                 1.0145,\n",
      "                 1.0359,\n",
      "                 0.7151,\n",
      "                 0.8289,\n",
      "                 0.9576,\n",
      "                 0.8511,\n",
      "                 0.8321,\n",
      "                 1.0525,\n",
      "                 0.8772,\n",
      "                 0.853,\n",
      "                 0.7931,\n",
      "                 1.0338,\n",
      "                 0.5657,\n",
      "                 0.6978,\n",
      "                 0.9262,\n",
      "                 0.8445,\n",
      "                 0.8564,\n",
      "                 0.7859,\n",
      "                 0.7023,\n",
      "                 0.9373,\n",
      "                 0.8044,\n",
      "                 0.6313,\n",
      "                 0.7114,\n",
      "                 0.7828,\n",
      "                 0.7958,\n",
      "                 0.6321],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.578738808631897,\n",
      "               0.578954815864563,\n",
      "               0.5786460041999817,\n",
      "               0.577717125415802,\n",
      "               0.5763852596282959,\n",
      "               0.575073778629303,\n",
      "               0.5742015242576599,\n",
      "               0.5734549760818481,\n",
      "               0.5724749565124512,\n",
      "               0.5713783502578735,\n",
      "               0.5704776048660278,\n",
      "               0.569978654384613,\n",
      "               0.5692331194877625,\n",
      "               0.5684889554977417,\n",
      "               0.5676758885383606,\n",
      "               0.5672733783721924,\n",
      "               0.5670343637466431,\n",
      "               0.5668057203292847,\n",
      "               0.5666317939758301,\n",
      "               0.5665901899337769,\n",
      "               0.5664038062095642,\n",
      "               0.5662752389907837,\n",
      "               0.5662830471992493,\n",
      "               0.5663180947303772,\n",
      "               0.566271185874939,\n",
      "               0.5662677884101868,\n",
      "               0.566367506980896,\n",
      "               0.5663408637046814,\n",
      "               0.566347599029541,\n",
      "               0.566320538520813,\n",
      "               0.5663117170333862,\n",
      "               0.5663343667984009,\n",
      "               0.5662947297096252,\n",
      "               0.5662624835968018,\n",
      "               0.5662651062011719,\n",
      "               0.5662685632705688,\n",
      "               0.5662791132926941,\n",
      "               0.5662822723388672,\n",
      "               0.5662869811058044]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.9061,\n",
      "                 0.9335,\n",
      "                 0.9298,\n",
      "                 0.8917,\n",
      "                 0.6709,\n",
      "                 0.674,\n",
      "                 1.021,\n",
      "                 0.7673,\n",
      "                 0.7881,\n",
      "                 0.8472,\n",
      "                 0.8009,\n",
      "                 0.8158,\n",
      "                 0.6839,\n",
      "                 0.6885,\n",
      "                 0.7116,\n",
      "                 0.6896,\n",
      "                 0.646,\n",
      "                 0.7593,\n",
      "                 0.5717,\n",
      "                 0.6713,\n",
      "                 0.5783,\n",
      "                 0.5536,\n",
      "                 0.6208,\n",
      "                 0.6093,\n",
      "                 0.4282,\n",
      "                 0.455,\n",
      "                 0.692,\n",
      "                 0.5683,\n",
      "                 0.5794,\n",
      "                 0.6997,\n",
      "                 0.3994,\n",
      "                 0.5396,\n",
      "                 0.5881,\n",
      "                 0.5195,\n",
      "                 0.541,\n",
      "                 0.5721,\n",
      "                 0.5679,\n",
      "                 0.586,\n",
      "                 0.5298,\n",
      "                 0.5206],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5797679424285889,\n",
      "               0.5822712779045105,\n",
      "               0.5861418843269348,\n",
      "               0.5917004346847534,\n",
      "               0.5974804162979126,\n",
      "               0.603601336479187,\n",
      "               0.609832763671875,\n",
      "               0.6161621809005737,\n",
      "               0.6225857138633728,\n",
      "               0.6290246844291687,\n",
      "               0.6354631781578064,\n",
      "               0.6418336629867554,\n",
      "               0.6481572985649109,\n",
      "               0.6543235778808594,\n",
      "               0.6602845788002014,\n",
      "               0.6660873889923096,\n",
      "               0.6717370748519897,\n",
      "               0.6772491335868835,\n",
      "               0.6825137138366699,\n",
      "               0.687589168548584,\n",
      "               0.6925347447395325,\n",
      "               0.6972496509552002,\n",
      "               0.7017489075660706,\n",
      "               0.7057775855064392,\n",
      "               0.7095783948898315,\n",
      "               0.7131621837615967,\n",
      "               0.7164639234542847,\n",
      "               0.7196216583251953,\n",
      "               0.7225377559661865,\n",
      "               0.7249773144721985,\n",
      "               0.7271065711975098,\n",
      "               0.7289696931838989,\n",
      "               0.7306138873100281,\n",
      "               0.7319285273551941,\n",
      "               0.73296058177948,\n",
      "               0.7337441444396973,\n",
      "               0.7343194484710693,\n",
      "               0.7347014546394348,\n",
      "               0.7348926663398743]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 4,\n",
      "  'train_loss': [0.3008,\n",
      "                 0.2242,\n",
      "                 0.2885,\n",
      "                 0.2506,\n",
      "                 0.2045,\n",
      "                 0.306,\n",
      "                 0.2487,\n",
      "                 0.2467,\n",
      "                 0.2245,\n",
      "                 0.3027,\n",
      "                 0.1593,\n",
      "                 0.2386,\n",
      "                 0.1447,\n",
      "                 0.1846,\n",
      "                 0.1443,\n",
      "                 0.114,\n",
      "                 0.1551,\n",
      "                 0.228,\n",
      "                 0.1827,\n",
      "                 0.1866,\n",
      "                 0.1648,\n",
      "                 0.1597,\n",
      "                 0.1456,\n",
      "                 0.1356,\n",
      "                 0.1028,\n",
      "                 0.1479,\n",
      "                 0.0827,\n",
      "                 0.1663,\n",
      "                 0.1547,\n",
      "                 0.1025,\n",
      "                 0.1222,\n",
      "                 0.0965,\n",
      "                 0.1187,\n",
      "                 0.1706,\n",
      "                 0.1114,\n",
      "                 0.0863,\n",
      "                 0.1225,\n",
      "                 0.1267,\n",
      "                 0.1056,\n",
      "                 0.1555],\n",
      "  'val_loss': [0.5786162614822388,\n",
      "               0.5799700021743774,\n",
      "               0.5827633142471313,\n",
      "               0.5872474908828735,\n",
      "               0.5935617685317993,\n",
      "               0.6001425981521606,\n",
      "               0.6068403124809265,\n",
      "               0.6136730909347534,\n",
      "               0.6205418705940247,\n",
      "               0.6275255084037781,\n",
      "               0.6343858242034912,\n",
      "               0.6413482427597046,\n",
      "               0.6477474570274353,\n",
      "               0.6541920304298401,\n",
      "               0.6606631278991699,\n",
      "               0.6668144464492798,\n",
      "               0.6727160811424255,\n",
      "               0.6786801218986511,\n",
      "               0.6843123435974121,\n",
      "               0.6898894906044006,\n",
      "               0.6951904892921448,\n",
      "               0.700084388256073,\n",
      "               0.7047890424728394,\n",
      "               0.7092595100402832,\n",
      "               0.7136228084564209,\n",
      "               0.7177428007125854,\n",
      "               0.7211702466011047,\n",
      "               0.7244209051132202,\n",
      "               0.7274619340896606,\n",
      "               0.7299951314926147,\n",
      "               0.7323480248451233,\n",
      "               0.7341288328170776,\n",
      "               0.7357891201972961,\n",
      "               0.737131655216217,\n",
      "               0.7381026148796082,\n",
      "               0.7387844920158386,\n",
      "               0.739316463470459,\n",
      "               0.7397368550300598,\n",
      "               0.7400078773498535,\n",
      "               0.7401362657546997]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [1.0801,\n",
      "                 0.8471,\n",
      "                 0.9416,\n",
      "                 0.7848,\n",
      "                 0.7436,\n",
      "                 1.009,\n",
      "                 0.7163,\n",
      "                 1.0185,\n",
      "                 0.5255,\n",
      "                 1.1028,\n",
      "                 0.7969,\n",
      "                 0.5798,\n",
      "                 0.9464,\n",
      "                 0.7022,\n",
      "                 0.8509,\n",
      "                 1.0642,\n",
      "                 0.7872,\n",
      "                 0.8609,\n",
      "                 0.9885,\n",
      "                 0.551,\n",
      "                 0.8014,\n",
      "                 0.8308,\n",
      "                 0.5916,\n",
      "                 0.9933,\n",
      "                 0.7971,\n",
      "                 0.6417,\n",
      "                 0.6796,\n",
      "                 0.7522,\n",
      "                 0.7722,\n",
      "                 0.6408,\n",
      "                 0.5108,\n",
      "                 0.8932,\n",
      "                 0.6292,\n",
      "                 0.7334,\n",
      "                 0.9145,\n",
      "                 0.5449,\n",
      "                 0.686,\n",
      "                 0.5679,\n",
      "                 0.9474,\n",
      "                 0.5956,\n",
      "                 0.559,\n",
      "                 0.9689,\n",
      "                 0.701,\n",
      "                 0.5424,\n",
      "                 0.5017,\n",
      "                 0.7934,\n",
      "                 0.9558,\n",
      "                 0.6161,\n",
      "                 0.7765,\n",
      "                 0.4509,\n",
      "                 0.7201,\n",
      "                 0.6087,\n",
      "                 0.6058,\n",
      "                 1.029,\n",
      "                 0.398,\n",
      "                 0.6416,\n",
      "                 0.6572,\n",
      "                 0.5809,\n",
      "                 0.6678,\n",
      "                 0.6365,\n",
      "                 0.6654,\n",
      "                 0.6407,\n",
      "                 0.6099,\n",
      "                 0.6139,\n",
      "                 0.759,\n",
      "                 0.6973,\n",
      "                 0.5485,\n",
      "                 0.6878,\n",
      "                 0.768,\n",
      "                 0.5047,\n",
      "                 0.8195,\n",
      "                 0.6061,\n",
      "                 0.8725,\n",
      "                 0.6884,\n",
      "                 0.6538,\n",
      "                 0.7252,\n",
      "                 0.6778,\n",
      "                 0.5212,\n",
      "                 0.548,\n",
      "                 0.6865],\n",
      "  'val_loss': [0.5783883333206177,\n",
      "               0.5770462155342102,\n",
      "               0.5750454068183899,\n",
      "               0.5728395581245422,\n",
      "               0.5693337917327881,\n",
      "               0.5657175779342651,\n",
      "               0.5633261203765869,\n",
      "               0.5605819225311279,\n",
      "               0.5588098168373108,\n",
      "               0.5565719604492188,\n",
      "               0.5545123815536499,\n",
      "               0.5527269840240479,\n",
      "               0.5510339736938477,\n",
      "               0.5498376488685608,\n",
      "               0.5485124588012695,\n",
      "               0.5472029447555542,\n",
      "               0.5459386706352234,\n",
      "               0.5448631048202515,\n",
      "               0.5440273284912109,\n",
      "               0.5431865453720093,\n",
      "               0.5424472689628601,\n",
      "               0.5418812036514282,\n",
      "               0.5414100885391235,\n",
      "               0.5409694910049438,\n",
      "               0.5405411720275879,\n",
      "               0.5402163863182068,\n",
      "               0.5399340391159058,\n",
      "               0.5395981073379517,\n",
      "               0.5392872095108032,\n",
      "               0.5390143394470215,\n",
      "               0.5388098955154419,\n",
      "               0.5386325120925903,\n",
      "               0.5384336113929749,\n",
      "               0.5382612943649292,\n",
      "               0.5381449460983276,\n",
      "               0.5380547642707825,\n",
      "               0.5379809141159058,\n",
      "               0.5379256010055542,\n",
      "               0.5379015207290649,\n",
      "               0.5378883481025696]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [1.7904,\n",
      "                 0.2797,\n",
      "                 0.8208,\n",
      "                 0.5996,\n",
      "                 0.8005,\n",
      "                 0.8473,\n",
      "                 0.6198,\n",
      "                 1.1396,\n",
      "                 0.2078,\n",
      "                 1.3826,\n",
      "                 1.0166,\n",
      "                 0.5932,\n",
      "                 0.4509,\n",
      "                 1.1815,\n",
      "                 1.1855,\n",
      "                 0.6517,\n",
      "                 0.6608,\n",
      "                 0.6326,\n",
      "                 0.7997,\n",
      "                 0.6941,\n",
      "                 0.8237,\n",
      "                 0.4221,\n",
      "                 0.9805,\n",
      "                 0.669,\n",
      "                 1.3427,\n",
      "                 0.5378,\n",
      "                 0.4551,\n",
      "                 1.1443,\n",
      "                 1.1309,\n",
      "                 0.5611,\n",
      "                 1.0434,\n",
      "                 0.4452,\n",
      "                 0.7661,\n",
      "                 0.9758,\n",
      "                 0.7382,\n",
      "                 0.8569,\n",
      "                 0.4931,\n",
      "                 0.8625,\n",
      "                 0.6968,\n",
      "                 0.5512,\n",
      "                 0.6498,\n",
      "                 0.5682,\n",
      "                 0.506,\n",
      "                 0.8684,\n",
      "                 0.2343,\n",
      "                 1.0568,\n",
      "                 1.1632,\n",
      "                 0.557,\n",
      "                 0.9087,\n",
      "                 0.3971,\n",
      "                 0.5711,\n",
      "                 0.9193,\n",
      "                 0.3424,\n",
      "                 0.8748,\n",
      "                 0.2188,\n",
      "                 1.1828,\n",
      "                 0.9605,\n",
      "                 0.2374,\n",
      "                 0.4496,\n",
      "                 0.6583,\n",
      "                 0.8908,\n",
      "                 0.3645,\n",
      "                 0.9646,\n",
      "                 0.4229,\n",
      "                 0.5501,\n",
      "                 0.6303,\n",
      "                 0.5447,\n",
      "                 0.5524,\n",
      "                 0.5545,\n",
      "                 0.6468,\n",
      "                 1.0532,\n",
      "                 0.2395,\n",
      "                 0.9785,\n",
      "                 0.3041,\n",
      "                 1.0314,\n",
      "                 0.1543,\n",
      "                 0.9586,\n",
      "                 0.4971,\n",
      "                 0.7354,\n",
      "                 0.4319],\n",
      "  'val_loss': [0.5787137746810913,\n",
      "               0.5787172317504883,\n",
      "               0.5778549313545227,\n",
      "               0.5776116251945496,\n",
      "               0.5774935483932495,\n",
      "               0.5764116048812866,\n",
      "               0.5768274664878845,\n",
      "               0.5763238072395325,\n",
      "               0.575250506401062,\n",
      "               0.5747629404067993,\n",
      "               0.574202835559845,\n",
      "               0.5736957788467407,\n",
      "               0.5726876258850098,\n",
      "               0.5724717378616333,\n",
      "               0.5717223882675171,\n",
      "               0.5716118812561035,\n",
      "               0.5714484453201294,\n",
      "               0.5712157487869263,\n",
      "               0.57085782289505,\n",
      "               0.5707944631576538,\n",
      "               0.5703862905502319,\n",
      "               0.5697869658470154,\n",
      "               0.56963050365448,\n",
      "               0.5694482922554016,\n",
      "               0.5693435668945312,\n",
      "               0.5692683458328247,\n",
      "               0.5693880915641785,\n",
      "               0.5693005323410034,\n",
      "               0.5690656900405884,\n",
      "               0.5689005851745605,\n",
      "               0.568759560585022,\n",
      "               0.5686525106430054,\n",
      "               0.5685478448867798,\n",
      "               0.5685067176818848,\n",
      "               0.5683801770210266,\n",
      "               0.5682851076126099,\n",
      "               0.5682223439216614,\n",
      "               0.5681888461112976,\n",
      "               0.5681723356246948,\n",
      "               0.5681737661361694]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [0.2111,\n",
      "                 1.0487,\n",
      "                 0.5666,\n",
      "                 0.555,\n",
      "                 0.5587,\n",
      "                 0.5119,\n",
      "                 0.7055,\n",
      "                 0.5815,\n",
      "                 0.9531,\n",
      "                 0.3047,\n",
      "                 0.8877,\n",
      "                 0.279,\n",
      "                 0.6067,\n",
      "                 0.4928,\n",
      "                 0.5542,\n",
      "                 0.515,\n",
      "                 0.5177,\n",
      "                 0.6855,\n",
      "                 0.5596,\n",
      "                 0.7486,\n",
      "                 0.5314,\n",
      "                 0.6221,\n",
      "                 0.4983,\n",
      "                 0.5321,\n",
      "                 0.5625,\n",
      "                 0.3717,\n",
      "                 0.4288,\n",
      "                 0.3902,\n",
      "                 0.2606,\n",
      "                 0.674,\n",
      "                 0.4707,\n",
      "                 0.355,\n",
      "                 0.7312,\n",
      "                 0.2878,\n",
      "                 0.2025,\n",
      "                 0.7718,\n",
      "                 0.5695,\n",
      "                 0.3719,\n",
      "                 0.5539,\n",
      "                 0.4308,\n",
      "                 0.7133,\n",
      "                 0.2402,\n",
      "                 0.3123,\n",
      "                 0.6801,\n",
      "                 0.651,\n",
      "                 0.275,\n",
      "                 0.3876,\n",
      "                 0.4128,\n",
      "                 0.4123,\n",
      "                 0.4267,\n",
      "                 0.1444,\n",
      "                 1.0573,\n",
      "                 0.5094,\n",
      "                 0.3324,\n",
      "                 0.5713,\n",
      "                 0.2208,\n",
      "                 0.2466,\n",
      "                 0.7331,\n",
      "                 0.497,\n",
      "                 0.2702,\n",
      "                 0.6156,\n",
      "                 0.4065,\n",
      "                 0.5392,\n",
      "                 0.4608,\n",
      "                 0.8002,\n",
      "                 0.3159,\n",
      "                 0.6276,\n",
      "                 0.1339,\n",
      "                 0.6804,\n",
      "                 0.1578,\n",
      "                 0.4562,\n",
      "                 0.3829,\n",
      "                 0.226,\n",
      "                 0.5884,\n",
      "                 0.1652,\n",
      "                 0.826,\n",
      "                 0.2258,\n",
      "                 0.5523,\n",
      "                 0.2864,\n",
      "                 0.4692],\n",
      "  'val_loss': [0.5785096287727356,\n",
      "               0.5774437785148621,\n",
      "               0.5749713778495789,\n",
      "               0.572462260723114,\n",
      "               0.5687850117683411,\n",
      "               0.5660665035247803,\n",
      "               0.5632068514823914,\n",
      "               0.5606988072395325,\n",
      "               0.558564305305481,\n",
      "               0.5566977262496948,\n",
      "               0.5552513599395752,\n",
      "               0.5544031262397766,\n",
      "               0.5535899996757507,\n",
      "               0.5532512664794922,\n",
      "               0.5527620315551758,\n",
      "               0.552424430847168,\n",
      "               0.5521558523178101,\n",
      "               0.5519608855247498,\n",
      "               0.5517479181289673,\n",
      "               0.5514118075370789,\n",
      "               0.5511879920959473,\n",
      "               0.5508326292037964,\n",
      "               0.5507178902626038,\n",
      "               0.5506576895713806,\n",
      "               0.550698459148407,\n",
      "               0.5507410764694214,\n",
      "               0.5507915019989014,\n",
      "               0.550659716129303,\n",
      "               0.5507076978683472,\n",
      "               0.5507979393005371,\n",
      "               0.5509368777275085,\n",
      "               0.5510531663894653,\n",
      "               0.5510574579238892,\n",
      "               0.5510658025741577,\n",
      "               0.5510357022285461,\n",
      "               0.5510414838790894,\n",
      "               0.5510430335998535,\n",
      "               0.5510540008544922,\n",
      "               0.5510408282279968,\n",
      "               0.5510409474372864]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [1.1216,\n",
      "                 0.6479,\n",
      "                 1.042,\n",
      "                 0.7361,\n",
      "                 0.8576,\n",
      "                 0.6509,\n",
      "                 0.3798,\n",
      "                 1.1896,\n",
      "                 0.7696,\n",
      "                 0.6152,\n",
      "                 0.6268,\n",
      "                 0.6341,\n",
      "                 0.6245,\n",
      "                 0.6114,\n",
      "                 0.9778,\n",
      "                 0.6609,\n",
      "                 0.4362,\n",
      "                 0.8884,\n",
      "                 0.6734,\n",
      "                 0.6142,\n",
      "                 0.5839,\n",
      "                 0.6608,\n",
      "                 0.6397,\n",
      "                 0.4246,\n",
      "                 0.8443,\n",
      "                 0.4267,\n",
      "                 0.1822,\n",
      "                 0.7745,\n",
      "                 0.9558,\n",
      "                 0.2415,\n",
      "                 0.4961,\n",
      "                 0.6803,\n",
      "                 0.5619,\n",
      "                 0.6244,\n",
      "                 0.4638,\n",
      "                 0.7205,\n",
      "                 0.3264,\n",
      "                 0.683,\n",
      "                 0.3296,\n",
      "                 0.5756,\n",
      "                 0.4569,\n",
      "                 0.2778,\n",
      "                 0.3609,\n",
      "                 0.4309,\n",
      "                 0.2377,\n",
      "                 0.4959,\n",
      "                 0.7376,\n",
      "                 0.3469,\n",
      "                 0.7245,\n",
      "                 0.1297,\n",
      "                 0.4001,\n",
      "                 0.7646,\n",
      "                 0.224,\n",
      "                 0.5414,\n",
      "                 0.1551,\n",
      "                 0.5516,\n",
      "                 0.6169,\n",
      "                 0.1893,\n",
      "                 0.3419,\n",
      "                 0.4453,\n",
      "                 0.5999,\n",
      "                 0.1612,\n",
      "                 0.5872,\n",
      "                 0.1478,\n",
      "                 0.2278,\n",
      "                 0.6858,\n",
      "                 0.4124,\n",
      "                 0.34,\n",
      "                 0.2491,\n",
      "                 0.6432,\n",
      "                 0.632,\n",
      "                 0.1885,\n",
      "                 0.7312,\n",
      "                 0.1977,\n",
      "                 0.5497,\n",
      "                 0.3554,\n",
      "                 0.5722,\n",
      "                 0.1532,\n",
      "                 0.2458,\n",
      "                 0.6545],\n",
      "  'val_loss': [0.5793054699897766,\n",
      "               0.5826418995857239,\n",
      "               0.5892692804336548,\n",
      "               0.6002715229988098,\n",
      "               0.6152063608169556,\n",
      "               0.6308093070983887,\n",
      "               0.6477961540222168,\n",
      "               0.6645044088363647,\n",
      "               0.6824002265930176,\n",
      "               0.7000877261161804,\n",
      "               0.7161005735397339,\n",
      "               0.7314386367797852,\n",
      "               0.7469552755355835,\n",
      "               0.7637687921524048,\n",
      "               0.7800359725952148,\n",
      "               0.7943663597106934,\n",
      "               0.8086406588554382,\n",
      "               0.8214396238327026,\n",
      "               0.8332405090332031,\n",
      "               0.844325840473175,\n",
      "               0.8540140986442566,\n",
      "               0.8648513555526733,\n",
      "               0.876078724861145,\n",
      "               0.8864768147468567,\n",
      "               0.8971912264823914,\n",
      "               0.9079378247261047,\n",
      "               0.9176069498062134,\n",
      "               0.926439642906189,\n",
      "               0.9342538118362427,\n",
      "               0.9417799115180969,\n",
      "               0.9476833343505859,\n",
      "               0.9535294771194458,\n",
      "               0.9589496850967407,\n",
      "               0.9627043604850769,\n",
      "               0.9663143157958984,\n",
      "               0.9691034555435181,\n",
      "               0.9714586138725281,\n",
      "               0.973068356513977,\n",
      "               0.974215030670166,\n",
      "               0.9747591018676758]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [0.9467,\n",
      "                 0.6664,\n",
      "                 0.8249,\n",
      "                 0.945,\n",
      "                 0.6035,\n",
      "                 1.2697,\n",
      "                 0.6753,\n",
      "                 0.9659,\n",
      "                 0.7804,\n",
      "                 1.2855,\n",
      "                 0.8739,\n",
      "                 0.6289,\n",
      "                 0.7671,\n",
      "                 0.8752,\n",
      "                 0.8095,\n",
      "                 0.8805,\n",
      "                 1.0979,\n",
      "                 0.8452,\n",
      "                 0.8026,\n",
      "                 0.8599,\n",
      "                 0.559,\n",
      "                 0.7445,\n",
      "                 0.8251,\n",
      "                 0.7017,\n",
      "                 1.0583,\n",
      "                 0.5564,\n",
      "                 0.6921,\n",
      "                 0.8435,\n",
      "                 0.7704,\n",
      "                 0.7912,\n",
      "                 0.7728,\n",
      "                 0.7177,\n",
      "                 0.8352,\n",
      "                 0.6366,\n",
      "                 0.7207,\n",
      "                 0.7135,\n",
      "                 0.5671,\n",
      "                 0.7392,\n",
      "                 0.7308,\n",
      "                 0.7154,\n",
      "                 0.806,\n",
      "                 0.7143,\n",
      "                 0.5816,\n",
      "                 0.5401,\n",
      "                 0.5942,\n",
      "                 0.6904,\n",
      "                 0.6896,\n",
      "                 0.7514,\n",
      "                 0.6595,\n",
      "                 0.4768,\n",
      "                 0.4884,\n",
      "                 0.9013,\n",
      "                 0.926,\n",
      "                 0.6032,\n",
      "                 0.484,\n",
      "                 0.6152,\n",
      "                 0.6877,\n",
      "                 0.5034,\n",
      "                 0.7167,\n",
      "                 0.6119,\n",
      "                 0.832,\n",
      "                 0.6664,\n",
      "                 0.6371,\n",
      "                 0.664,\n",
      "                 0.8205,\n",
      "                 0.4968,\n",
      "                 0.6567,\n",
      "                 0.5154,\n",
      "                 0.6918,\n",
      "                 0.6482,\n",
      "                 0.5154,\n",
      "                 0.5343,\n",
      "                 0.7355,\n",
      "                 0.5829,\n",
      "                 0.7655,\n",
      "                 0.4793,\n",
      "                 0.5465,\n",
      "                 0.7221,\n",
      "                 0.73,\n",
      "                 0.4966],\n",
      "  'val_loss': [0.5791244506835938,\n",
      "               0.5805330872535706,\n",
      "               0.5826693177223206,\n",
      "               0.5862412452697754,\n",
      "               0.5894670486450195,\n",
      "               0.5933355093002319,\n",
      "               0.5950673222541809,\n",
      "               0.5976178050041199,\n",
      "               0.6009097099304199,\n",
      "               0.6028248071670532,\n",
      "               0.6048263907432556,\n",
      "               0.608027458190918,\n",
      "               0.6117216944694519,\n",
      "               0.6151341199874878,\n",
      "               0.6193622350692749,\n",
      "               0.6234857439994812,\n",
      "               0.6281384229660034,\n",
      "               0.6328392028808594,\n",
      "               0.6375553011894226,\n",
      "               0.6415260434150696,\n",
      "               0.6456994414329529,\n",
      "               0.649679958820343,\n",
      "               0.6523626446723938,\n",
      "               0.6553232073783875,\n",
      "               0.6571677327156067,\n",
      "               0.6591503024101257,\n",
      "               0.6606415510177612,\n",
      "               0.6619156002998352,\n",
      "               0.663190484046936,\n",
      "               0.6642781496047974,\n",
      "               0.6653875708580017,\n",
      "               0.6663068532943726,\n",
      "               0.66680508852005,\n",
      "               0.667236328125,\n",
      "               0.66766756772995,\n",
      "               0.6679442524909973,\n",
      "               0.6681863069534302,\n",
      "               0.6683721542358398,\n",
      "               0.6685755252838135,\n",
      "               0.6686495542526245]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [0.3117,\n",
      "                 1.0857,\n",
      "                 0.5532,\n",
      "                 0.6514,\n",
      "                 0.6515,\n",
      "                 0.8039,\n",
      "                 0.847,\n",
      "                 0.5811,\n",
      "                 0.7381,\n",
      "                 0.8013,\n",
      "                 0.6507,\n",
      "                 0.6917,\n",
      "                 0.838,\n",
      "                 0.3864,\n",
      "                 0.6603,\n",
      "                 0.7163,\n",
      "                 0.8344,\n",
      "                 0.5664,\n",
      "                 0.3735,\n",
      "                 0.7241,\n",
      "                 0.4086,\n",
      "                 0.8307,\n",
      "                 0.2994,\n",
      "                 0.5775,\n",
      "                 0.6998,\n",
      "                 0.4293,\n",
      "                 0.7365,\n",
      "                 0.3502,\n",
      "                 0.368,\n",
      "                 0.7439,\n",
      "                 0.3224,\n",
      "                 0.6168,\n",
      "                 0.5942,\n",
      "                 0.5186,\n",
      "                 0.6572,\n",
      "                 0.3902,\n",
      "                 0.6167,\n",
      "                 0.3118,\n",
      "                 0.5132,\n",
      "                 0.4506,\n",
      "                 0.3777,\n",
      "                 0.6025,\n",
      "                 0.5493,\n",
      "                 0.3708,\n",
      "                 0.4086,\n",
      "                 0.4529,\n",
      "                 0.5192,\n",
      "                 0.389,\n",
      "                 0.4875,\n",
      "                 0.399,\n",
      "                 0.5808,\n",
      "                 0.4143,\n",
      "                 0.4944,\n",
      "                 0.4011,\n",
      "                 0.3842,\n",
      "                 0.3344,\n",
      "                 0.4116,\n",
      "                 0.5274,\n",
      "                 0.397,\n",
      "                 0.4918,\n",
      "                 0.4091,\n",
      "                 0.4945,\n",
      "                 0.3513,\n",
      "                 0.5564,\n",
      "                 0.3587,\n",
      "                 0.4325,\n",
      "                 0.3411,\n",
      "                 0.4674,\n",
      "                 0.4063,\n",
      "                 0.3301,\n",
      "                 0.3964,\n",
      "                 0.3963,\n",
      "                 0.3849,\n",
      "                 0.4179,\n",
      "                 0.4017,\n",
      "                 0.5566,\n",
      "                 0.4881,\n",
      "                 0.3154,\n",
      "                 0.4259,\n",
      "                 0.3234],\n",
      "  'val_loss': [0.5784996151924133,\n",
      "               0.5769017338752747,\n",
      "               0.5733379125595093,\n",
      "               0.5678485035896301,\n",
      "               0.5620083808898926,\n",
      "               0.557237982749939,\n",
      "               0.5538392663002014,\n",
      "               0.5511223077774048,\n",
      "               0.5488883256912231,\n",
      "               0.5475606322288513,\n",
      "               0.5466731786727905,\n",
      "               0.5462222695350647,\n",
      "               0.5461148023605347,\n",
      "               0.546237587928772,\n",
      "               0.5465388894081116,\n",
      "               0.5467397570610046,\n",
      "               0.5470194816589355,\n",
      "               0.5474444627761841,\n",
      "               0.5479879975318909,\n",
      "               0.5486757159233093,\n",
      "               0.5493601560592651,\n",
      "               0.5499369502067566,\n",
      "               0.55042964220047,\n",
      "               0.5510320663452148,\n",
      "               0.551445484161377,\n",
      "               0.5517142415046692,\n",
      "               0.5519464015960693,\n",
      "               0.552127480506897,\n",
      "               0.552039384841919,\n",
      "               0.5519803762435913,\n",
      "               0.5519084930419922,\n",
      "               0.5519031286239624,\n",
      "               0.5518544316291809,\n",
      "               0.5518239736557007,\n",
      "               0.5517838597297668,\n",
      "               0.5517242550849915,\n",
      "               0.5516539812088013,\n",
      "               0.5515996217727661,\n",
      "               0.5515998601913452,\n",
      "               0.55158531665802]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [1.0543,\n",
      "                 0.4941,\n",
      "                 1.0485,\n",
      "                 0.7587,\n",
      "                 0.8629,\n",
      "                 0.8942,\n",
      "                 1.2001,\n",
      "                 0.6835,\n",
      "                 0.755,\n",
      "                 0.982,\n",
      "                 0.441,\n",
      "                 1.1392,\n",
      "                 0.9215,\n",
      "                 0.9304,\n",
      "                 1.0439,\n",
      "                 0.7985,\n",
      "                 1.2056,\n",
      "                 0.5569,\n",
      "                 1.0002,\n",
      "                 0.579,\n",
      "                 0.7057,\n",
      "                 0.886,\n",
      "                 0.4041,\n",
      "                 0.9112,\n",
      "                 0.6689,\n",
      "                 0.7653,\n",
      "                 0.7691,\n",
      "                 0.6883,\n",
      "                 0.8498,\n",
      "                 0.8838,\n",
      "                 0.8221,\n",
      "                 0.6606,\n",
      "                 0.544,\n",
      "                 0.9697,\n",
      "                 0.7424,\n",
      "                 0.6124,\n",
      "                 0.5781,\n",
      "                 0.7986,\n",
      "                 0.7627,\n",
      "                 0.5468,\n",
      "                 0.5577,\n",
      "                 0.6431,\n",
      "                 0.8062,\n",
      "                 0.549,\n",
      "                 0.6905,\n",
      "                 0.6893,\n",
      "                 0.6699,\n",
      "                 0.7278,\n",
      "                 0.3753,\n",
      "                 0.8327,\n",
      "                 0.779,\n",
      "                 0.5141,\n",
      "                 0.6074,\n",
      "                 0.5639,\n",
      "                 0.5381,\n",
      "                 0.7436,\n",
      "                 0.6782,\n",
      "                 0.5727,\n",
      "                 0.825,\n",
      "                 0.3633,\n",
      "                 0.3935,\n",
      "                 0.6669,\n",
      "                 0.4652,\n",
      "                 0.6863,\n",
      "                 0.6232,\n",
      "                 0.5775,\n",
      "                 0.5101,\n",
      "                 0.6575,\n",
      "                 0.5867,\n",
      "                 0.6397,\n",
      "                 0.5144,\n",
      "                 0.5112,\n",
      "                 0.559,\n",
      "                 0.5892,\n",
      "                 0.5722,\n",
      "                 0.6262,\n",
      "                 0.6394,\n",
      "                 0.6374,\n",
      "                 0.4928,\n",
      "                 0.5077],\n",
      "  'val_loss': [0.5782836675643921,\n",
      "               0.576705813407898,\n",
      "               0.5743061900138855,\n",
      "               0.5705444812774658,\n",
      "               0.567268967628479,\n",
      "               0.5648439526557922,\n",
      "               0.5619513392448425,\n",
      "               0.5592223405838013,\n",
      "               0.5571550130844116,\n",
      "               0.5550898909568787,\n",
      "               0.5529980063438416,\n",
      "               0.5513125061988831,\n",
      "               0.5495955348014832,\n",
      "               0.5480769872665405,\n",
      "               0.5468581318855286,\n",
      "               0.5455594062805176,\n",
      "               0.5446735620498657,\n",
      "               0.5438297986984253,\n",
      "               0.5432451963424683,\n",
      "               0.5427279472351074,\n",
      "               0.5422325730323792,\n",
      "               0.541899561882019,\n",
      "               0.5417254567146301,\n",
      "               0.5415740609169006,\n",
      "               0.5414237976074219,\n",
      "               0.5411828756332397,\n",
      "               0.5409782528877258,\n",
      "               0.5407362580299377,\n",
      "               0.5404835343360901,\n",
      "               0.5402385592460632,\n",
      "               0.5400265455245972,\n",
      "               0.5398011207580566,\n",
      "               0.5396102666854858,\n",
      "               0.5394682288169861,\n",
      "               0.5393749475479126,\n",
      "               0.5393258929252625,\n",
      "               0.5393036603927612,\n",
      "               0.5392935872077942,\n",
      "               0.539271354675293,\n",
      "               0.5392600893974304]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [0.9161,\n",
      "                 0.5768,\n",
      "                 1.3496,\n",
      "                 0.1111,\n",
      "                 0.9321,\n",
      "                 0.4178,\n",
      "                 0.7113,\n",
      "                 0.92,\n",
      "                 0.5914,\n",
      "                 0.8194,\n",
      "                 0.6592,\n",
      "                 0.5961,\n",
      "                 0.7935,\n",
      "                 0.4305,\n",
      "                 1.0001,\n",
      "                 0.2065,\n",
      "                 0.2435,\n",
      "                 0.8401,\n",
      "                 0.6154,\n",
      "                 0.3123,\n",
      "                 0.8705,\n",
      "                 0.2578,\n",
      "                 0.5098,\n",
      "                 0.4755,\n",
      "                 0.5987,\n",
      "                 0.317,\n",
      "                 0.3285,\n",
      "                 0.5206,\n",
      "                 0.4083,\n",
      "                 0.4772,\n",
      "                 0.4267,\n",
      "                 0.481,\n",
      "                 0.5637,\n",
      "                 0.6158,\n",
      "                 0.3953,\n",
      "                 0.4203,\n",
      "                 0.2746,\n",
      "                 0.6449,\n",
      "                 0.4764,\n",
      "                 0.4034,\n",
      "                 0.5155,\n",
      "                 0.4321,\n",
      "                 0.3752,\n",
      "                 0.4721,\n",
      "                 0.3505,\n",
      "                 0.2715,\n",
      "                 0.3974,\n",
      "                 0.3367,\n",
      "                 0.4891,\n",
      "                 0.2716,\n",
      "                 0.3245,\n",
      "                 0.5374,\n",
      "                 0.3806,\n",
      "                 0.3944,\n",
      "                 0.2766,\n",
      "                 0.5088,\n",
      "                 0.343,\n",
      "                 0.203,\n",
      "                 0.3308,\n",
      "                 0.3767,\n",
      "                 0.387,\n",
      "                 0.3671,\n",
      "                 0.2278,\n",
      "                 0.3536,\n",
      "                 0.2859,\n",
      "                 0.2457,\n",
      "                 0.3618,\n",
      "                 0.225,\n",
      "                 0.2664,\n",
      "                 0.2608,\n",
      "                 0.4571,\n",
      "                 0.1797,\n",
      "                 0.3849,\n",
      "                 0.4032,\n",
      "                 0.2952,\n",
      "                 0.2583,\n",
      "                 0.3478,\n",
      "                 0.3062,\n",
      "                 0.258,\n",
      "                 0.2601],\n",
      "  'val_loss': [0.5779376029968262,\n",
      "               0.5756717324256897,\n",
      "               0.5723026394844055,\n",
      "               0.5672307014465332,\n",
      "               0.5613620281219482,\n",
      "               0.5560632944107056,\n",
      "               0.5518220663070679,\n",
      "               0.549026370048523,\n",
      "               0.5470854043960571,\n",
      "               0.5456595420837402,\n",
      "               0.5449053049087524,\n",
      "               0.5443347096443176,\n",
      "               0.5444072484970093,\n",
      "               0.5446910262107849,\n",
      "               0.5449047088623047,\n",
      "               0.5453112125396729,\n",
      "               0.5458964109420776,\n",
      "               0.5466569066047668,\n",
      "               0.5473805069923401,\n",
      "               0.548524022102356,\n",
      "               0.5498546361923218,\n",
      "               0.5511453747749329,\n",
      "               0.5526058077812195,\n",
      "               0.5541540384292603,\n",
      "               0.5554930567741394,\n",
      "               0.5565279126167297,\n",
      "               0.5574636459350586,\n",
      "               0.5585756301879883,\n",
      "               0.5595743060112,\n",
      "               0.5605905652046204,\n",
      "               0.5613376498222351,\n",
      "               0.5619470477104187,\n",
      "               0.5625702142715454,\n",
      "               0.563052773475647,\n",
      "               0.5633876323699951,\n",
      "               0.5637367963790894,\n",
      "               0.5639753341674805,\n",
      "               0.5642217397689819,\n",
      "               0.5643370747566223,\n",
      "               0.5643805265426636]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [0.8689,\n",
      "                 0.7273,\n",
      "                 0.6969,\n",
      "                 1.1246,\n",
      "                 0.9118,\n",
      "                 0.8605,\n",
      "                 0.643,\n",
      "                 0.7272,\n",
      "                 0.6173,\n",
      "                 1.0646,\n",
      "                 0.509,\n",
      "                 0.9274,\n",
      "                 1.272,\n",
      "                 0.4471,\n",
      "                 0.7901,\n",
      "                 0.9147,\n",
      "                 0.6045,\n",
      "                 0.9644,\n",
      "                 0.9292,\n",
      "                 0.6733,\n",
      "                 0.5553,\n",
      "                 0.9012,\n",
      "                 0.8151,\n",
      "                 0.7172,\n",
      "                 0.8297,\n",
      "                 0.7397,\n",
      "                 0.8224,\n",
      "                 0.4003,\n",
      "                 0.7274,\n",
      "                 0.9305,\n",
      "                 0.748,\n",
      "                 0.882,\n",
      "                 0.5408,\n",
      "                 0.7233,\n",
      "                 0.8972,\n",
      "                 0.3866,\n",
      "                 0.7809,\n",
      "                 0.7957,\n",
      "                 0.9343,\n",
      "                 0.4707,\n",
      "                 0.2872,\n",
      "                 0.9141,\n",
      "                 0.6101,\n",
      "                 0.6676,\n",
      "                 0.4871,\n",
      "                 1.0001,\n",
      "                 0.5933,\n",
      "                 0.7343,\n",
      "                 0.5965,\n",
      "                 0.5578,\n",
      "                 0.6511,\n",
      "                 0.8766,\n",
      "                 0.9341,\n",
      "                 0.4616,\n",
      "                 0.4164,\n",
      "                 0.6319,\n",
      "                 1.059,\n",
      "                 0.4996,\n",
      "                 0.3947,\n",
      "                 0.8532,\n",
      "                 0.5605,\n",
      "                 0.714,\n",
      "                 0.5564,\n",
      "                 0.6426,\n",
      "                 0.4606,\n",
      "                 0.7244,\n",
      "                 0.7154,\n",
      "                 0.572,\n",
      "                 0.4581,\n",
      "                 0.7498,\n",
      "                 0.6717,\n",
      "                 0.583,\n",
      "                 0.8369,\n",
      "                 0.5375,\n",
      "                 0.7412,\n",
      "                 0.7018,\n",
      "                 0.5339,\n",
      "                 0.5993,\n",
      "                 0.7299,\n",
      "                 0.4157],\n",
      "  'val_loss': [0.5787720680236816,\n",
      "               0.5793924331665039,\n",
      "               0.5807485580444336,\n",
      "               0.5805489420890808,\n",
      "               0.5816988945007324,\n",
      "               0.5824995040893555,\n",
      "               0.5828043818473816,\n",
      "               0.5835694074630737,\n",
      "               0.585654616355896,\n",
      "               0.5877941846847534,\n",
      "               0.5898948907852173,\n",
      "               0.5924933552742004,\n",
      "               0.5952752828598022,\n",
      "               0.5975232124328613,\n",
      "               0.6003926992416382,\n",
      "               0.6020575761795044,\n",
      "               0.604408860206604,\n",
      "               0.6055701971054077,\n",
      "               0.6074093580245972,\n",
      "               0.6082856059074402,\n",
      "               0.6090485453605652,\n",
      "               0.6096583604812622,\n",
      "               0.6102770566940308,\n",
      "               0.6103123426437378,\n",
      "               0.6107471585273743,\n",
      "               0.6110824942588806,\n",
      "               0.6108903884887695,\n",
      "               0.6114917397499084,\n",
      "               0.6119049191474915,\n",
      "               0.6124753355979919,\n",
      "               0.6128823161125183,\n",
      "               0.6132192611694336,\n",
      "               0.6135061383247375,\n",
      "               0.6134389042854309,\n",
      "               0.6136244535446167,\n",
      "               0.613634467124939,\n",
      "               0.6136257648468018,\n",
      "               0.6136183142662048,\n",
      "               0.6136186718940735,\n",
      "               0.6136420369148254]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 8,\n",
      "  'train_loss': [0.4051,\n",
      "                 0.8056,\n",
      "                 0.5854,\n",
      "                 0.6314,\n",
      "                 0.6659,\n",
      "                 0.2996,\n",
      "                 0.2868,\n",
      "                 0.8864,\n",
      "                 0.6019,\n",
      "                 0.2175,\n",
      "                 0.4646,\n",
      "                 0.3758,\n",
      "                 0.4172,\n",
      "                 0.3826,\n",
      "                 0.3822,\n",
      "                 0.3791,\n",
      "                 0.3015,\n",
      "                 0.4519,\n",
      "                 0.3068,\n",
      "                 0.3108,\n",
      "                 0.3408,\n",
      "                 0.2803,\n",
      "                 0.4497,\n",
      "                 0.2646,\n",
      "                 0.2469,\n",
      "                 0.2111,\n",
      "                 0.2691,\n",
      "                 0.3791,\n",
      "                 0.3158,\n",
      "                 0.3414,\n",
      "                 0.2061,\n",
      "                 0.3027,\n",
      "                 0.3483,\n",
      "                 0.2007,\n",
      "                 0.1897,\n",
      "                 0.3666,\n",
      "                 0.2103,\n",
      "                 0.3386,\n",
      "                 0.2127,\n",
      "                 0.408,\n",
      "                 0.2708,\n",
      "                 0.2221,\n",
      "                 0.2076,\n",
      "                 0.2926,\n",
      "                 0.2125,\n",
      "                 0.1813,\n",
      "                 0.2026,\n",
      "                 0.1994,\n",
      "                 0.2722,\n",
      "                 0.1325,\n",
      "                 0.1868,\n",
      "                 0.2923,\n",
      "                 0.2077,\n",
      "                 0.2319,\n",
      "                 0.2393,\n",
      "                 0.2849,\n",
      "                 0.2186,\n",
      "                 0.2742,\n",
      "                 0.2945,\n",
      "                 0.1695,\n",
      "                 0.2987,\n",
      "                 0.166,\n",
      "                 0.3429,\n",
      "                 0.1759,\n",
      "                 0.2134,\n",
      "                 0.2066,\n",
      "                 0.3275,\n",
      "                 0.1127,\n",
      "                 0.1826,\n",
      "                 0.2748,\n",
      "                 0.1896,\n",
      "                 0.2453,\n",
      "                 0.2296,\n",
      "                 0.148,\n",
      "                 0.1575,\n",
      "                 0.2561,\n",
      "                 0.257,\n",
      "                 0.2659,\n",
      "                 0.1646,\n",
      "                 0.2216],\n",
      "  'val_loss': [0.5780311822891235,\n",
      "               0.5752614736557007,\n",
      "               0.5705880522727966,\n",
      "               0.5644168853759766,\n",
      "               0.5583264231681824,\n",
      "               0.5540834069252014,\n",
      "               0.550629734992981,\n",
      "               0.5481280088424683,\n",
      "               0.5466073155403137,\n",
      "               0.5456522703170776,\n",
      "               0.5451794862747192,\n",
      "               0.5451600551605225,\n",
      "               0.5455259084701538,\n",
      "               0.545969545841217,\n",
      "               0.5466097593307495,\n",
      "               0.5472065210342407,\n",
      "               0.5478866696357727,\n",
      "               0.5486540198326111,\n",
      "               0.549595296382904,\n",
      "               0.5505213737487793,\n",
      "               0.551247775554657,\n",
      "               0.5521230101585388,\n",
      "               0.5527281761169434,\n",
      "               0.5531514883041382,\n",
      "               0.5535850524902344,\n",
      "               0.553988516330719,\n",
      "               0.5541784763336182,\n",
      "               0.554519534111023,\n",
      "               0.5548943877220154,\n",
      "               0.5553098917007446,\n",
      "               0.555453360080719,\n",
      "               0.5555708408355713,\n",
      "               0.5556402206420898,\n",
      "               0.5555750131607056,\n",
      "               0.5556483268737793,\n",
      "               0.5557114481925964,\n",
      "               0.5557661056518555,\n",
      "               0.5558422207832336,\n",
      "               0.5558959245681763,\n",
      "               0.5559195280075073]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [1.6644,\n",
      "                 0.6753,\n",
      "                 1.1453,\n",
      "                 0.53,\n",
      "                 1.3425,\n",
      "                 0.5516,\n",
      "                 0.3486,\n",
      "                 1.5213,\n",
      "                 0.6568,\n",
      "                 0.818,\n",
      "                 0.4727,\n",
      "                 0.7533,\n",
      "                 2.1807,\n",
      "                 0.6722,\n",
      "                 1.3624,\n",
      "                 0.6889,\n",
      "                 0.8963,\n",
      "                 0.7825,\n",
      "                 0.5514,\n",
      "                 1.6817,\n",
      "                 0.7975,\n",
      "                 1.8742,\n",
      "                 0.588,\n",
      "                 0.6017,\n",
      "                 0.9169,\n",
      "                 0.6246,\n",
      "                 1.4478,\n",
      "                 1.0469,\n",
      "                 0.3084,\n",
      "                 0.759,\n",
      "                 1.0087,\n",
      "                 1.1599,\n",
      "                 0.4781,\n",
      "                 1.0318,\n",
      "                 0.6522,\n",
      "                 1.971,\n",
      "                 1.6524,\n",
      "                 0.5981,\n",
      "                 1.4554,\n",
      "                 0.4086,\n",
      "                 1.3861,\n",
      "                 0.9276,\n",
      "                 1.4016,\n",
      "                 0.5012,\n",
      "                 0.8839,\n",
      "                 0.5847,\n",
      "                 0.5938,\n",
      "                 2.4629,\n",
      "                 1.1239,\n",
      "                 1.1176,\n",
      "                 1.3098,\n",
      "                 0.5166,\n",
      "                 0.8261,\n",
      "                 0.4805,\n",
      "                 1.701,\n",
      "                 0.6377,\n",
      "                 0.9984,\n",
      "                 0.7547,\n",
      "                 1.4009,\n",
      "                 0.7085,\n",
      "                 0.8495,\n",
      "                 0.2589,\n",
      "                 1.4436,\n",
      "                 0.9412,\n",
      "                 0.7118,\n",
      "                 0.5365,\n",
      "                 0.615,\n",
      "                 1.8415,\n",
      "                 0.8293,\n",
      "                 0.7981,\n",
      "                 0.9046,\n",
      "                 0.7244,\n",
      "                 0.2618,\n",
      "                 0.4758,\n",
      "                 0.8126,\n",
      "                 1.1473,\n",
      "                 0.7178,\n",
      "                 0.9618,\n",
      "                 1.859,\n",
      "                 0.7803,\n",
      "                 1.1883,\n",
      "                 0.2181,\n",
      "                 1.2315,\n",
      "                 1.6982,\n",
      "                 0.6572,\n",
      "                 0.9859,\n",
      "                 0.5798,\n",
      "                 1.0126,\n",
      "                 0.6034,\n",
      "                 0.9389,\n",
      "                 1.0719,\n",
      "                 0.7503,\n",
      "                 0.4118,\n",
      "                 1.116,\n",
      "                 0.8317,\n",
      "                 1.4923,\n",
      "                 0.5564,\n",
      "                 0.5193,\n",
      "                 0.8552,\n",
      "                 0.983,\n",
      "                 0.9062,\n",
      "                 1.0507,\n",
      "                 1.3929,\n",
      "                 0.6452,\n",
      "                 1.0137,\n",
      "                 0.5839,\n",
      "                 1.7654,\n",
      "                 0.5627,\n",
      "                 0.3693,\n",
      "                 1.9394,\n",
      "                 0.6183,\n",
      "                 0.9161,\n",
      "                 0.5815,\n",
      "                 0.4906,\n",
      "                 0.5781,\n",
      "                 1.1648,\n",
      "                 1.6441,\n",
      "                 0.6778,\n",
      "                 1.2049,\n",
      "                 0.4403,\n",
      "                 0.8477,\n",
      "                 0.9033,\n",
      "                 1.7995,\n",
      "                 0.5223,\n",
      "                 0.9502,\n",
      "                 1.0549,\n",
      "                 0.4218,\n",
      "                 0.7645,\n",
      "                 0.9655,\n",
      "                 0.8312,\n",
      "                 0.6757,\n",
      "                 0.5766,\n",
      "                 1.415,\n",
      "                 1.2218,\n",
      "                 0.7253,\n",
      "                 0.3715,\n",
      "                 0.2074,\n",
      "                 0.756,\n",
      "                 0.7299,\n",
      "                 0.5795,\n",
      "                 0.704,\n",
      "                 1.4542,\n",
      "                 0.7952,\n",
      "                 1.3315,\n",
      "                 0.9581,\n",
      "                 0.4866,\n",
      "                 0.6219,\n",
      "                 1.1068,\n",
      "                 0.8363,\n",
      "                 0.8509,\n",
      "                 0.5444,\n",
      "                 1.3423,\n",
      "                 0.7608,\n",
      "                 0.6808,\n",
      "                 0.7859,\n",
      "                 0.8558,\n",
      "                 1.0204,\n",
      "                 0.5482,\n",
      "                 0.9471,\n",
      "                 0.5281,\n",
      "                 0.5843,\n",
      "                 1.2789,\n",
      "                 0.755,\n",
      "                 0.559,\n",
      "                 0.3769,\n",
      "                 0.8795,\n",
      "                 0.6746,\n",
      "                 0.7057,\n",
      "                 0.5301,\n",
      "                 0.8357,\n",
      "                 0.3707,\n",
      "                 0.9616,\n",
      "                 0.6426,\n",
      "                 0.5555,\n",
      "                 0.6021,\n",
      "                 1.1846,\n",
      "                 0.4826,\n",
      "                 1.2246,\n",
      "                 0.5247,\n",
      "                 0.6933,\n",
      "                 0.6049,\n",
      "                 1.1054,\n",
      "                 0.9951,\n",
      "                 0.6282,\n",
      "                 0.5778,\n",
      "                 0.7059,\n",
      "                 0.5463,\n",
      "                 1.0026,\n",
      "                 0.4661,\n",
      "                 0.6641,\n",
      "                 0.5259,\n",
      "                 1.6675,\n",
      "                 1.0385,\n",
      "                 0.4494,\n",
      "                 0.714,\n",
      "                 0.597,\n",
      "                 0.916,\n",
      "                 0.2271,\n",
      "                 0.7144,\n",
      "                 1.1792,\n",
      "                 1.2061,\n",
      "                 0.4798,\n",
      "                 1.2642,\n",
      "                 0.396,\n",
      "                 0.5408,\n",
      "                 0.2812,\n",
      "                 0.5098,\n",
      "                 0.8639,\n",
      "                 1.0878,\n",
      "                 0.3618,\n",
      "                 0.4403,\n",
      "                 0.3781,\n",
      "                 1.0133,\n",
      "                 0.6728,\n",
      "                 1.1841,\n",
      "                 0.8075,\n",
      "                 0.7895,\n",
      "                 0.5804,\n",
      "                 0.7463,\n",
      "                 1.0004,\n",
      "                 0.2854,\n",
      "                 1.1185,\n",
      "                 0.8081,\n",
      "                 0.3433,\n",
      "                 0.5996,\n",
      "                 0.6304,\n",
      "                 0.8204,\n",
      "                 0.3742,\n",
      "                 0.665,\n",
      "                 0.4601,\n",
      "                 1.5041,\n",
      "                 0.4007,\n",
      "                 0.304,\n",
      "                 0.5399,\n",
      "                 1.0135,\n",
      "                 0.9148,\n",
      "                 1.0194,\n",
      "                 0.3381,\n",
      "                 0.6559,\n",
      "                 0.9655,\n",
      "                 0.7143,\n",
      "                 0.8572,\n",
      "                 0.4019,\n",
      "                 0.902,\n",
      "                 1.0032,\n",
      "                 0.7733,\n",
      "                 0.2938,\n",
      "                 0.3914,\n",
      "                 0.399,\n",
      "                 0.7218,\n",
      "                 1.0074,\n",
      "                 0.2699,\n",
      "                 0.4666,\n",
      "                 0.3761,\n",
      "                 1.2303,\n",
      "                 0.5816,\n",
      "                 0.4896,\n",
      "                 0.3266,\n",
      "                 0.3867,\n",
      "                 1.8659,\n",
      "                 0.4427,\n",
      "                 1.5963,\n",
      "                 0.3654,\n",
      "                 0.6809,\n",
      "                 1.4317,\n",
      "                 0.1119,\n",
      "                 0.3251,\n",
      "                 1.1202,\n",
      "                 0.739,\n",
      "                 0.4035,\n",
      "                 0.3977,\n",
      "                 1.0553,\n",
      "                 0.689,\n",
      "                 0.4713,\n",
      "                 0.5227,\n",
      "                 1.0111,\n",
      "                 0.7519,\n",
      "                 0.1501,\n",
      "                 0.5513,\n",
      "                 0.7786,\n",
      "                 0.5399,\n",
      "                 0.4927,\n",
      "                 1.6003,\n",
      "                 0.6386,\n",
      "                 0.55,\n",
      "                 0.3831,\n",
      "                 0.2936,\n",
      "                 0.7139,\n",
      "                 0.9807,\n",
      "                 0.9598,\n",
      "                 0.7921,\n",
      "                 0.8217,\n",
      "                 0.3322,\n",
      "                 0.3977,\n",
      "                 0.5753,\n",
      "                 0.504,\n",
      "                 0.3562,\n",
      "                 1.2235,\n",
      "                 0.3935,\n",
      "                 0.5632,\n",
      "                 0.7754,\n",
      "                 0.8314,\n",
      "                 0.7148,\n",
      "                 0.7333,\n",
      "                 0.7178,\n",
      "                 0.4455,\n",
      "                 0.6355,\n",
      "                 0.4102,\n",
      "                 1.3595,\n",
      "                 0.3185,\n",
      "                 0.4123,\n",
      "                 1.7067,\n",
      "                 0.8333,\n",
      "                 1.0983,\n",
      "                 0.9288,\n",
      "                 0.11,\n",
      "                 1.0044,\n",
      "                 0.3472,\n",
      "                 0.6548,\n",
      "                 0.5506],\n",
      "  'val_loss': [0.5773907899856567,\n",
      "               0.5759792923927307,\n",
      "               0.5765266418457031,\n",
      "               0.5732094049453735,\n",
      "               0.5700737833976746,\n",
      "               0.5707347989082336,\n",
      "               0.5717836618423462,\n",
      "               0.5762222409248352,\n",
      "               0.5800786018371582,\n",
      "               0.5837193727493286,\n",
      "               0.588976263999939,\n",
      "               0.588912844657898,\n",
      "               0.5892685055732727,\n",
      "               0.5896893739700317,\n",
      "               0.5929669141769409,\n",
      "               0.5960311889648438,\n",
      "               0.5986014604568481,\n",
      "               0.597817063331604,\n",
      "               0.5977103114128113,\n",
      "               0.5985149145126343,\n",
      "               0.5977484583854675,\n",
      "               0.5975317358970642,\n",
      "               0.5966793298721313,\n",
      "               0.5965462923049927,\n",
      "               0.598028838634491,\n",
      "               0.5996463894844055,\n",
      "               0.6014692783355713,\n",
      "               0.6028232574462891,\n",
      "               0.6050899624824524,\n",
      "               0.6053369641304016,\n",
      "               0.6062740087509155,\n",
      "               0.6066389083862305,\n",
      "               0.6064481735229492,\n",
      "               0.6064944267272949,\n",
      "               0.6063154339790344,\n",
      "               0.6065616607666016,\n",
      "               0.6068034172058105,\n",
      "               0.6071243286132812,\n",
      "               0.6069523096084595,\n",
      "               0.6069357991218567]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.6367,\n",
      "                 0.5372,\n",
      "                 1.3319,\n",
      "                 0.6529,\n",
      "                 0.9443,\n",
      "                 0.2484,\n",
      "                 0.471,\n",
      "                 0.6705,\n",
      "                 0.5236,\n",
      "                 0.6669,\n",
      "                 0.7455,\n",
      "                 0.3216,\n",
      "                 0.467,\n",
      "                 1.2027,\n",
      "                 0.6088,\n",
      "                 1.2359,\n",
      "                 0.3731,\n",
      "                 0.884,\n",
      "                 0.5907,\n",
      "                 0.9775,\n",
      "                 0.885,\n",
      "                 0.4417,\n",
      "                 0.3978,\n",
      "                 0.6378,\n",
      "                 0.5149,\n",
      "                 1.1457,\n",
      "                 0.4095,\n",
      "                 0.7519,\n",
      "                 0.4522,\n",
      "                 1.171,\n",
      "                 0.8352,\n",
      "                 0.4294,\n",
      "                 0.5074,\n",
      "                 0.8605,\n",
      "                 0.6864,\n",
      "                 0.5679,\n",
      "                 0.5507,\n",
      "                 0.6932,\n",
      "                 0.7411,\n",
      "                 0.6377,\n",
      "                 0.3777,\n",
      "                 0.2855,\n",
      "                 0.7913,\n",
      "                 0.7664,\n",
      "                 0.6623,\n",
      "                 0.5861,\n",
      "                 0.6051,\n",
      "                 0.6503,\n",
      "                 0.3927,\n",
      "                 0.368,\n",
      "                 0.8219,\n",
      "                 0.8378,\n",
      "                 0.5155,\n",
      "                 0.8115,\n",
      "                 0.3585,\n",
      "                 0.647,\n",
      "                 0.3673,\n",
      "                 0.3436,\n",
      "                 0.6733,\n",
      "                 0.9673,\n",
      "                 0.3965,\n",
      "                 0.6167,\n",
      "                 0.3769,\n",
      "                 0.6519,\n",
      "                 0.84,\n",
      "                 0.4563,\n",
      "                 1.1172,\n",
      "                 0.3194,\n",
      "                 0.5965,\n",
      "                 0.4006,\n",
      "                 0.354,\n",
      "                 0.3355,\n",
      "                 0.5778,\n",
      "                 0.5531,\n",
      "                 0.7984,\n",
      "                 1.0175,\n",
      "                 0.4712,\n",
      "                 0.4749,\n",
      "                 0.5073,\n",
      "                 0.4522,\n",
      "                 0.6193,\n",
      "                 0.3443,\n",
      "                 0.277,\n",
      "                 0.2699,\n",
      "                 0.9567,\n",
      "                 0.8474,\n",
      "                 0.6108,\n",
      "                 0.4169,\n",
      "                 0.9986,\n",
      "                 0.4195,\n",
      "                 0.3103,\n",
      "                 0.5137,\n",
      "                 0.4465,\n",
      "                 1.1068,\n",
      "                 0.8232,\n",
      "                 0.4722,\n",
      "                 1.1397,\n",
      "                 0.6427,\n",
      "                 0.1938,\n",
      "                 0.4733,\n",
      "                 0.6248,\n",
      "                 0.178,\n",
      "                 0.3091,\n",
      "                 0.7699,\n",
      "                 0.3518,\n",
      "                 0.3402,\n",
      "                 0.6591,\n",
      "                 1.0722,\n",
      "                 0.5984,\n",
      "                 0.4123,\n",
      "                 0.4078,\n",
      "                 0.3781,\n",
      "                 0.6515,\n",
      "                 0.7631,\n",
      "                 0.7257,\n",
      "                 0.3276,\n",
      "                 0.3896,\n",
      "                 0.2817,\n",
      "                 0.234,\n",
      "                 0.6188,\n",
      "                 1.1537,\n",
      "                 0.5922,\n",
      "                 0.3002,\n",
      "                 0.615,\n",
      "                 0.5297,\n",
      "                 0.3391,\n",
      "                 0.4045,\n",
      "                 0.6217,\n",
      "                 0.568,\n",
      "                 0.4662,\n",
      "                 0.3209,\n",
      "                 0.6356,\n",
      "                 0.6933,\n",
      "                 0.5289,\n",
      "                 0.2411,\n",
      "                 0.2468,\n",
      "                 0.3183,\n",
      "                 0.6647,\n",
      "                 0.239,\n",
      "                 0.7553,\n",
      "                 0.7661,\n",
      "                 0.4595,\n",
      "                 0.4507,\n",
      "                 0.2387,\n",
      "                 0.795,\n",
      "                 0.5909,\n",
      "                 0.5159,\n",
      "                 0.3341,\n",
      "                 0.1392,\n",
      "                 0.229,\n",
      "                 0.531,\n",
      "                 0.4177,\n",
      "                 0.2427,\n",
      "                 0.6312,\n",
      "                 0.2344,\n",
      "                 0.6911,\n",
      "                 0.256,\n",
      "                 0.6555,\n",
      "                 0.9711,\n",
      "                 0.2115,\n",
      "                 0.522,\n",
      "                 0.4034,\n",
      "                 0.9459,\n",
      "                 0.227,\n",
      "                 0.5361,\n",
      "                 0.2087,\n",
      "                 0.4035,\n",
      "                 0.4363,\n",
      "                 0.541,\n",
      "                 0.2067,\n",
      "                 0.2885,\n",
      "                 0.3209,\n",
      "                 0.5106,\n",
      "                 0.5896,\n",
      "                 0.4093,\n",
      "                 0.7675,\n",
      "                 0.1602,\n",
      "                 0.3594,\n",
      "                 0.6496,\n",
      "                 0.7438,\n",
      "                 0.974,\n",
      "                 0.3562,\n",
      "                 0.2278,\n",
      "                 0.1907,\n",
      "                 0.3032,\n",
      "                 0.7159,\n",
      "                 0.6577,\n",
      "                 0.6444,\n",
      "                 0.3625,\n",
      "                 0.4007,\n",
      "                 0.2303,\n",
      "                 0.2426,\n",
      "                 0.2541,\n",
      "                 0.137,\n",
      "                 0.7174,\n",
      "                 0.284,\n",
      "                 0.3952,\n",
      "                 0.8325,\n",
      "                 0.3883,\n",
      "                 0.4357,\n",
      "                 0.7243,\n",
      "                 0.464,\n",
      "                 0.4041,\n",
      "                 0.5835,\n",
      "                 0.4805,\n",
      "                 0.2608,\n",
      "                 0.2134,\n",
      "                 0.4387,\n",
      "                 0.3053,\n",
      "                 1.0292,\n",
      "                 0.5513,\n",
      "                 0.3507,\n",
      "                 0.2222,\n",
      "                 0.2341,\n",
      "                 0.5384,\n",
      "                 0.3381,\n",
      "                 0.6634,\n",
      "                 0.7787,\n",
      "                 0.8676,\n",
      "                 0.5783,\n",
      "                 0.234,\n",
      "                 0.189,\n",
      "                 0.3908,\n",
      "                 0.3103,\n",
      "                 0.6248,\n",
      "                 0.2933,\n",
      "                 1.0552,\n",
      "                 0.2503,\n",
      "                 0.4025,\n",
      "                 0.5018,\n",
      "                 0.3909,\n",
      "                 0.4388,\n",
      "                 0.2443,\n",
      "                 1.0213,\n",
      "                 0.3544,\n",
      "                 0.5714,\n",
      "                 0.1554,\n",
      "                 0.3837,\n",
      "                 0.2223,\n",
      "                 0.3227,\n",
      "                 0.7572,\n",
      "                 0.1574,\n",
      "                 0.6123,\n",
      "                 0.2047,\n",
      "                 0.6716,\n",
      "                 0.2054,\n",
      "                 0.6231,\n",
      "                 0.4755,\n",
      "                 0.2157,\n",
      "                 0.9077,\n",
      "                 0.201,\n",
      "                 0.2525,\n",
      "                 0.7295,\n",
      "                 0.2284,\n",
      "                 0.1537,\n",
      "                 1.0,\n",
      "                 0.3512,\n",
      "                 0.3257,\n",
      "                 0.9591,\n",
      "                 0.2399,\n",
      "                 0.6117,\n",
      "                 0.2608,\n",
      "                 0.5527,\n",
      "                 0.3039,\n",
      "                 0.2961,\n",
      "                 0.2215,\n",
      "                 0.385,\n",
      "                 0.2643,\n",
      "                 0.4606,\n",
      "                 0.3614,\n",
      "                 0.9016,\n",
      "                 0.1588,\n",
      "                 0.4676,\n",
      "                 0.2814,\n",
      "                 0.3091,\n",
      "                 0.2554,\n",
      "                 0.7016,\n",
      "                 0.1628,\n",
      "                 0.521,\n",
      "                 0.9231,\n",
      "                 0.6061,\n",
      "                 0.5935,\n",
      "                 0.2179,\n",
      "                 0.2413,\n",
      "                 1.1179,\n",
      "                 0.2498,\n",
      "                 0.2662,\n",
      "                 0.4937,\n",
      "                 0.3775,\n",
      "                 0.5913,\n",
      "                 0.2125,\n",
      "                 0.2862,\n",
      "                 0.4611,\n",
      "                 0.1437,\n",
      "                 0.6456,\n",
      "                 0.3269,\n",
      "                 0.6589,\n",
      "                 0.2317,\n",
      "                 0.5783,\n",
      "                 0.3042,\n",
      "                 0.606,\n",
      "                 0.1795,\n",
      "                 0.2956,\n",
      "                 0.5532,\n",
      "                 0.8076,\n",
      "                 0.483,\n",
      "                 0.4212,\n",
      "                 0.168,\n",
      "                 0.208,\n",
      "                 0.5655,\n",
      "                 0.5188,\n",
      "                 0.3512,\n",
      "                 0.6355,\n",
      "                 0.4754,\n",
      "                 0.3416,\n",
      "                 0.2145,\n",
      "                 0.4046,\n",
      "                 0.5297,\n",
      "                 0.1885,\n",
      "                 0.7148],\n",
      "  'val_loss': [0.5773840546607971,\n",
      "               0.5741069912910461,\n",
      "               0.568513035774231,\n",
      "               0.5597367286682129,\n",
      "               0.5531467199325562,\n",
      "               0.5463667511940002,\n",
      "               0.542961597442627,\n",
      "               0.5405920147895813,\n",
      "               0.539291262626648,\n",
      "               0.5391716957092285,\n",
      "               0.5366836786270142,\n",
      "               0.5340573191642761,\n",
      "               0.5329028367996216,\n",
      "               0.5322391390800476,\n",
      "               0.5312119722366333,\n",
      "               0.5304247736930847,\n",
      "               0.5299919247627258,\n",
      "               0.529384434223175,\n",
      "               0.5280134081840515,\n",
      "               0.5266825556755066,\n",
      "               0.5261019468307495,\n",
      "               0.5257253050804138,\n",
      "               0.5253941416740417,\n",
      "               0.5249655842781067,\n",
      "               0.5247083902359009,\n",
      "               0.5246488451957703,\n",
      "               0.5243455767631531,\n",
      "               0.5238109230995178,\n",
      "               0.5229429006576538,\n",
      "               0.5220149755477905,\n",
      "               0.5209976434707642,\n",
      "               0.5204043388366699,\n",
      "               0.5203536152839661,\n",
      "               0.5201342701911926,\n",
      "               0.5199995636940002,\n",
      "               0.5197911858558655,\n",
      "               0.5196855664253235,\n",
      "               0.5196347236633301,\n",
      "               0.519577145576477,\n",
      "               0.5195798873901367]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.4563,\n",
      "                 0.4913,\n",
      "                 1.0311,\n",
      "                 0.3001,\n",
      "                 1.0058,\n",
      "                 0.8598,\n",
      "                 0.6049,\n",
      "                 1.6,\n",
      "                 1.0708,\n",
      "                 0.7845,\n",
      "                 0.3076,\n",
      "                 1.0653,\n",
      "                 0.9853,\n",
      "                 0.3575,\n",
      "                 0.6706,\n",
      "                 0.6773,\n",
      "                 0.8296,\n",
      "                 0.4416,\n",
      "                 0.4384,\n",
      "                 1.8802,\n",
      "                 0.3195,\n",
      "                 1.5664,\n",
      "                 0.5839,\n",
      "                 0.3162,\n",
      "                 0.8366,\n",
      "                 0.6324,\n",
      "                 1.3078,\n",
      "                 0.6055,\n",
      "                 0.6981,\n",
      "                 0.2987,\n",
      "                 0.5575,\n",
      "                 0.8236,\n",
      "                 0.2446,\n",
      "                 0.5878,\n",
      "                 0.4133,\n",
      "                 1.212,\n",
      "                 0.705,\n",
      "                 1.0489,\n",
      "                 0.6027,\n",
      "                 0.5537,\n",
      "                 0.5023,\n",
      "                 0.5871,\n",
      "                 0.5981,\n",
      "                 0.2949,\n",
      "                 0.6811,\n",
      "                 0.2642,\n",
      "                 0.2979,\n",
      "                 1.1275,\n",
      "                 0.7256,\n",
      "                 0.7602,\n",
      "                 0.5116,\n",
      "                 0.2532,\n",
      "                 1.29,\n",
      "                 0.4522,\n",
      "                 1.2139,\n",
      "                 0.6393,\n",
      "                 0.802,\n",
      "                 0.4932,\n",
      "                 0.6861,\n",
      "                 0.4692,\n",
      "                 0.7463,\n",
      "                 0.4772,\n",
      "                 0.8301,\n",
      "                 1.1308,\n",
      "                 1.2649,\n",
      "                 0.2718,\n",
      "                 0.3033,\n",
      "                 0.8202,\n",
      "                 0.2649,\n",
      "                 0.4377,\n",
      "                 0.4997,\n",
      "                 0.818,\n",
      "                 0.7266,\n",
      "                 0.6219,\n",
      "                 0.7929,\n",
      "                 0.2657,\n",
      "                 0.4146,\n",
      "                 0.7385,\n",
      "                 1.4198,\n",
      "                 0.97,\n",
      "                 0.4146,\n",
      "                 0.4463,\n",
      "                 0.454,\n",
      "                 0.5463,\n",
      "                 0.6724,\n",
      "                 0.3384,\n",
      "                 0.6644,\n",
      "                 0.7871,\n",
      "                 0.4288,\n",
      "                 0.8438,\n",
      "                 0.4405,\n",
      "                 0.492,\n",
      "                 0.4628,\n",
      "                 0.5653,\n",
      "                 1.0974,\n",
      "                 0.6582,\n",
      "                 0.2569,\n",
      "                 0.4003,\n",
      "                 0.6425,\n",
      "                 1.0985,\n",
      "                 0.6035,\n",
      "                 0.9227,\n",
      "                 0.6311,\n",
      "                 0.5748,\n",
      "                 0.5235,\n",
      "                 0.4387,\n",
      "                 0.7488,\n",
      "                 0.3641,\n",
      "                 0.3624,\n",
      "                 0.7557,\n",
      "                 0.4767,\n",
      "                 0.3313,\n",
      "                 0.4001,\n",
      "                 0.4139,\n",
      "                 0.6324,\n",
      "                 0.6484,\n",
      "                 0.6384,\n",
      "                 0.4904,\n",
      "                 0.7278,\n",
      "                 0.3636,\n",
      "                 0.4311,\n",
      "                 0.4904,\n",
      "                 0.4617,\n",
      "                 0.4445,\n",
      "                 0.9167,\n",
      "                 1.1762,\n",
      "                 0.2851,\n",
      "                 0.2632,\n",
      "                 0.6082,\n",
      "                 0.5538,\n",
      "                 0.4825,\n",
      "                 0.3913,\n",
      "                 0.3592,\n",
      "                 0.9236,\n",
      "                 0.9737,\n",
      "                 0.3379,\n",
      "                 0.3778,\n",
      "                 0.1933,\n",
      "                 0.243,\n",
      "                 0.6596,\n",
      "                 0.6005,\n",
      "                 0.7197,\n",
      "                 0.9949,\n",
      "                 0.4846,\n",
      "                 0.341,\n",
      "                 0.251,\n",
      "                 0.2642,\n",
      "                 0.8734,\n",
      "                 0.52,\n",
      "                 0.5466,\n",
      "                 0.3524,\n",
      "                 0.3513,\n",
      "                 0.6318,\n",
      "                 0.4593,\n",
      "                 0.422,\n",
      "                 0.4151,\n",
      "                 0.7004,\n",
      "                 0.6417,\n",
      "                 0.4415,\n",
      "                 0.1471,\n",
      "                 0.4494,\n",
      "                 0.8527,\n",
      "                 0.2801,\n",
      "                 0.7,\n",
      "                 0.3292,\n",
      "                 0.3189,\n",
      "                 0.2429,\n",
      "                 0.753,\n",
      "                 0.114,\n",
      "                 0.8861,\n",
      "                 0.4848,\n",
      "                 0.5947,\n",
      "                 0.3258,\n",
      "                 0.4023,\n",
      "                 0.3599,\n",
      "                 0.9673,\n",
      "                 0.1262,\n",
      "                 1.0485,\n",
      "                 0.2527,\n",
      "                 0.5157,\n",
      "                 0.6608,\n",
      "                 0.9347,\n",
      "                 0.5874,\n",
      "                 0.4266,\n",
      "                 0.4695,\n",
      "                 0.229,\n",
      "                 0.6317,\n",
      "                 0.6463,\n",
      "                 0.4102,\n",
      "                 1.1236,\n",
      "                 0.1531,\n",
      "                 0.7546,\n",
      "                 0.7211,\n",
      "                 0.7299,\n",
      "                 0.1779,\n",
      "                 0.3712,\n",
      "                 0.7189,\n",
      "                 0.3553,\n",
      "                 0.4678,\n",
      "                 0.7977,\n",
      "                 0.3793,\n",
      "                 0.2476,\n",
      "                 0.553,\n",
      "                 0.3762,\n",
      "                 0.4343,\n",
      "                 0.15,\n",
      "                 0.728,\n",
      "                 0.7612,\n",
      "                 0.8292,\n",
      "                 0.1935,\n",
      "                 0.4047,\n",
      "                 0.5297,\n",
      "                 0.4498,\n",
      "                 0.875,\n",
      "                 0.4475,\n",
      "                 0.3904,\n",
      "                 0.9867,\n",
      "                 0.4534,\n",
      "                 0.1483,\n",
      "                 0.5851,\n",
      "                 0.172,\n",
      "                 0.5265,\n",
      "                 0.4972,\n",
      "                 0.224,\n",
      "                 0.2652,\n",
      "                 0.6586,\n",
      "                 0.3632,\n",
      "                 0.4516,\n",
      "                 0.5612,\n",
      "                 0.353,\n",
      "                 1.1989,\n",
      "                 0.2889,\n",
      "                 0.2294,\n",
      "                 0.2626,\n",
      "                 0.588,\n",
      "                 0.5921,\n",
      "                 0.7749,\n",
      "                 0.5252,\n",
      "                 0.4946,\n",
      "                 0.3809,\n",
      "                 0.1837,\n",
      "                 0.3376,\n",
      "                 0.5883,\n",
      "                 0.5256,\n",
      "                 0.7819,\n",
      "                 0.7089,\n",
      "                 0.2066,\n",
      "                 0.5023,\n",
      "                 0.1414,\n",
      "                 0.308,\n",
      "                 0.649,\n",
      "                 0.394,\n",
      "                 0.4575,\n",
      "                 0.2817,\n",
      "                 1.0138,\n",
      "                 0.4188,\n",
      "                 0.3348,\n",
      "                 0.2897,\n",
      "                 0.3181,\n",
      "                 0.8438,\n",
      "                 0.1662,\n",
      "                 0.6034,\n",
      "                 0.5626,\n",
      "                 0.2742,\n",
      "                 0.4773,\n",
      "                 0.3193,\n",
      "                 0.3002,\n",
      "                 0.6431,\n",
      "                 1.0084,\n",
      "                 0.1393,\n",
      "                 0.4649,\n",
      "                 0.6641,\n",
      "                 0.6099,\n",
      "                 0.2432,\n",
      "                 0.6728,\n",
      "                 0.3076,\n",
      "                 0.4942,\n",
      "                 0.192,\n",
      "                 0.3366,\n",
      "                 0.5514,\n",
      "                 0.4608,\n",
      "                 0.512,\n",
      "                 0.5754,\n",
      "                 0.2166,\n",
      "                 0.2835,\n",
      "                 0.6429,\n",
      "                 0.2393,\n",
      "                 0.3423,\n",
      "                 0.5506,\n",
      "                 0.4915,\n",
      "                 0.6533,\n",
      "                 0.484,\n",
      "                 0.119,\n",
      "                 0.2489,\n",
      "                 0.4781,\n",
      "                 0.1766,\n",
      "                 0.3513,\n",
      "                 0.5917,\n",
      "                 0.4026,\n",
      "                 0.3087,\n",
      "                 0.5692,\n",
      "                 0.3632,\n",
      "                 0.1582,\n",
      "                 0.9169,\n",
      "                 0.5514,\n",
      "                 0.4016,\n",
      "                 0.3554,\n",
      "                 0.1453,\n",
      "                 0.4533,\n",
      "                 0.3703,\n",
      "                 0.2219,\n",
      "                 0.6966,\n",
      "                 1.0812,\n",
      "                 0.6775,\n",
      "                 0.6749,\n",
      "                 0.2141,\n",
      "                 0.4356,\n",
      "                 0.3701,\n",
      "                 0.2243,\n",
      "                 0.2353],\n",
      "  'val_loss': [0.5792247653007507,\n",
      "               0.5759296417236328,\n",
      "               0.5709227323532104,\n",
      "               0.56279456615448,\n",
      "               0.5544620752334595,\n",
      "               0.5507764220237732,\n",
      "               0.5463311076164246,\n",
      "               0.5432448387145996,\n",
      "               0.541226863861084,\n",
      "               0.5384894013404846,\n",
      "               0.5370777249336243,\n",
      "               0.5364392995834351,\n",
      "               0.5374646186828613,\n",
      "               0.5377092361450195,\n",
      "               0.5379027128219604,\n",
      "               0.5385321974754333,\n",
      "               0.5387624502182007,\n",
      "               0.5383849143981934,\n",
      "               0.5381990671157837,\n",
      "               0.5392999053001404,\n",
      "               0.5396111607551575,\n",
      "               0.5412499308586121,\n",
      "               0.5422899127006531,\n",
      "               0.5432469248771667,\n",
      "               0.544080376625061,\n",
      "               0.544904351234436,\n",
      "               0.5435836315155029,\n",
      "               0.5436628460884094,\n",
      "               0.5439102053642273,\n",
      "               0.5438041687011719,\n",
      "               0.5435351133346558,\n",
      "               0.5430570840835571,\n",
      "               0.5428775548934937,\n",
      "               0.5429456830024719,\n",
      "               0.5430189371109009,\n",
      "               0.5429342985153198,\n",
      "               0.5430781841278076,\n",
      "               0.5432862043380737,\n",
      "               0.5433356761932373,\n",
      "               0.5433435440063477]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.4284,\n",
      "                 0.7111,\n",
      "                 1.2287,\n",
      "                 0.7858,\n",
      "                 1.0426,\n",
      "                 0.783,\n",
      "                 0.3589,\n",
      "                 1.1914,\n",
      "                 0.4593,\n",
      "                 0.6939,\n",
      "                 1.0118,\n",
      "                 1.3665,\n",
      "                 1.0192,\n",
      "                 0.2825,\n",
      "                 0.5194,\n",
      "                 0.7479,\n",
      "                 0.8356,\n",
      "                 0.3433,\n",
      "                 0.7152,\n",
      "                 0.8432,\n",
      "                 0.3081,\n",
      "                 1.3497,\n",
      "                 0.4706,\n",
      "                 1.047,\n",
      "                 0.2733,\n",
      "                 0.9979,\n",
      "                 0.657,\n",
      "                 0.7502,\n",
      "                 1.1485,\n",
      "                 0.3153,\n",
      "                 1.2749,\n",
      "                 0.2409,\n",
      "                 0.4726,\n",
      "                 0.0836,\n",
      "                 0.462,\n",
      "                 1.8304,\n",
      "                 0.5176,\n",
      "                 0.7272,\n",
      "                 0.1267,\n",
      "                 0.8602,\n",
      "                 0.2382,\n",
      "                 1.3321,\n",
      "                 0.3806,\n",
      "                 1.091,\n",
      "                 0.9258,\n",
      "                 0.326,\n",
      "                 0.5407,\n",
      "                 0.3907,\n",
      "                 0.2848,\n",
      "                 0.4581,\n",
      "                 1.3686,\n",
      "                 0.4698,\n",
      "                 0.3698,\n",
      "                 0.443,\n",
      "                 0.5873,\n",
      "                 0.9901,\n",
      "                 0.5588,\n",
      "                 0.2548,\n",
      "                 0.4583,\n",
      "                 0.704,\n",
      "                 1.3604,\n",
      "                 0.7218,\n",
      "                 0.7079,\n",
      "                 0.5543,\n",
      "                 0.4969,\n",
      "                 0.204,\n",
      "                 0.9342,\n",
      "                 0.5005,\n",
      "                 0.4302,\n",
      "                 0.2947,\n",
      "                 1.4314,\n",
      "                 0.7277,\n",
      "                 0.5672,\n",
      "                 0.2007,\n",
      "                 0.5159,\n",
      "                 0.3713,\n",
      "                 0.9177,\n",
      "                 1.6438,\n",
      "                 0.2864,\n",
      "                 0.2666,\n",
      "                 1.0227,\n",
      "                 0.3654,\n",
      "                 0.1992,\n",
      "                 0.5093,\n",
      "                 0.5697,\n",
      "                 0.3976,\n",
      "                 0.9149,\n",
      "                 0.3351,\n",
      "                 0.2407,\n",
      "                 0.0895,\n",
      "                 1.193,\n",
      "                 0.081,\n",
      "                 0.885,\n",
      "                 0.5816,\n",
      "                 0.6814,\n",
      "                 0.4189,\n",
      "                 0.4158,\n",
      "                 0.3209,\n",
      "                 1.4747,\n",
      "                 0.3546,\n",
      "                 0.4706,\n",
      "                 0.1123,\n",
      "                 0.4189,\n",
      "                 0.7939,\n",
      "                 0.5999,\n",
      "                 0.208,\n",
      "                 0.6734,\n",
      "                 1.2548,\n",
      "                 0.1142,\n",
      "                 1.3717,\n",
      "                 0.2911,\n",
      "                 0.2863,\n",
      "                 0.0534,\n",
      "                 0.2981,\n",
      "                 1.1847,\n",
      "                 1.2796,\n",
      "                 0.4299,\n",
      "                 0.0797,\n",
      "                 0.5865,\n",
      "                 0.7826,\n",
      "                 0.3493,\n",
      "                 1.3466,\n",
      "                 0.1602,\n",
      "                 0.153,\n",
      "                 0.8435,\n",
      "                 0.2344,\n",
      "                 0.2179,\n",
      "                 0.3764,\n",
      "                 1.1936,\n",
      "                 0.1711,\n",
      "                 0.2867,\n",
      "                 0.2926,\n",
      "                 0.4095,\n",
      "                 0.7873,\n",
      "                 0.678,\n",
      "                 0.2559,\n",
      "                 0.5677,\n",
      "                 0.295,\n",
      "                 1.0887,\n",
      "                 0.5925,\n",
      "                 0.5319,\n",
      "                 0.2597,\n",
      "                 0.2472,\n",
      "                 0.2284,\n",
      "                 0.4102,\n",
      "                 0.0977,\n",
      "                 0.1261,\n",
      "                 0.4724,\n",
      "                 0.2373,\n",
      "                 1.3937,\n",
      "                 0.6269,\n",
      "                 0.4587,\n",
      "                 0.414,\n",
      "                 0.366,\n",
      "                 1.3402,\n",
      "                 0.2324,\n",
      "                 0.1747,\n",
      "                 0.9463,\n",
      "                 0.68,\n",
      "                 0.078,\n",
      "                 0.7781,\n",
      "                 1.3322,\n",
      "                 0.6137,\n",
      "                 0.1633,\n",
      "                 0.3019,\n",
      "                 0.2518,\n",
      "                 0.3342,\n",
      "                 0.0547,\n",
      "                 0.3227,\n",
      "                 1.3673,\n",
      "                 0.2692,\n",
      "                 0.1469,\n",
      "                 0.2156,\n",
      "                 0.6542,\n",
      "                 0.1837,\n",
      "                 0.2189,\n",
      "                 0.1042,\n",
      "                 1.1269,\n",
      "                 0.6099,\n",
      "                 0.6713,\n",
      "                 0.1801,\n",
      "                 0.1875,\n",
      "                 0.4736,\n",
      "                 0.3585,\n",
      "                 1.4341,\n",
      "                 0.3619,\n",
      "                 0.8964,\n",
      "                 0.4245,\n",
      "                 0.1891,\n",
      "                 0.3009,\n",
      "                 0.0886,\n",
      "                 0.299,\n",
      "                 0.2303,\n",
      "                 0.4096,\n",
      "                 0.6175,\n",
      "                 0.3626,\n",
      "                 1.8232,\n",
      "                 0.2975,\n",
      "                 0.0721,\n",
      "                 0.1372,\n",
      "                 1.5258,\n",
      "                 0.6449,\n",
      "                 0.4408,\n",
      "                 0.3472,\n",
      "                 0.1052,\n",
      "                 0.4081,\n",
      "                 0.3121,\n",
      "                 0.0431,\n",
      "                 0.2432,\n",
      "                 0.5243,\n",
      "                 0.1162,\n",
      "                 0.3039,\n",
      "                 0.2697,\n",
      "                 0.2198,\n",
      "                 1.6447,\n",
      "                 0.6963,\n",
      "                 0.8306,\n",
      "                 0.3283,\n",
      "                 0.5222,\n",
      "                 0.0585,\n",
      "                 0.2176,\n",
      "                 0.2637,\n",
      "                 1.2793,\n",
      "                 0.0811,\n",
      "                 0.1879,\n",
      "                 0.0535,\n",
      "                 2.0326,\n",
      "                 0.2247,\n",
      "                 0.6354,\n",
      "                 0.0523,\n",
      "                 0.4332,\n",
      "                 0.1531,\n",
      "                 0.2162,\n",
      "                 0.5411,\n",
      "                 0.1294,\n",
      "                 1.7086,\n",
      "                 0.2633,\n",
      "                 0.1556,\n",
      "                 0.1013,\n",
      "                 0.4674,\n",
      "                 0.6591,\n",
      "                 1.26,\n",
      "                 0.3132,\n",
      "                 0.2313,\n",
      "                 0.1214,\n",
      "                 0.0431,\n",
      "                 0.0945,\n",
      "                 0.8027,\n",
      "                 0.1013,\n",
      "                 0.5065,\n",
      "                 1.3141,\n",
      "                 0.2764,\n",
      "                 0.888,\n",
      "                 0.444,\n",
      "                 0.165,\n",
      "                 0.2043,\n",
      "                 0.0423,\n",
      "                 0.3362,\n",
      "                 0.6779,\n",
      "                 1.2201,\n",
      "                 0.975,\n",
      "                 0.1266,\n",
      "                 0.1736,\n",
      "                 0.2257,\n",
      "                 0.2504,\n",
      "                 0.1676,\n",
      "                 0.0413,\n",
      "                 1.6152,\n",
      "                 0.5804,\n",
      "                 0.3496,\n",
      "                 0.0837,\n",
      "                 0.0629,\n",
      "                 1.4442,\n",
      "                 0.0383,\n",
      "                 0.2495,\n",
      "                 0.1718,\n",
      "                 0.3313,\n",
      "                 0.1873,\n",
      "                 1.3363,\n",
      "                 0.2171,\n",
      "                 0.2569,\n",
      "                 0.137,\n",
      "                 0.1063,\n",
      "                 1.1442,\n",
      "                 0.4342,\n",
      "                 0.2791,\n",
      "                 0.0598,\n",
      "                 0.8561,\n",
      "                 0.5244,\n",
      "                 1.2342,\n",
      "                 0.2015,\n",
      "                 0.0378,\n",
      "                 0.3794,\n",
      "                 0.2127,\n",
      "                 0.1498,\n",
      "                 0.1634,\n",
      "                 0.1557,\n",
      "                 0.1672,\n",
      "                 0.4174,\n",
      "                 0.0343,\n",
      "                 0.4389,\n",
      "                 1.2525,\n",
      "                 0.5318,\n",
      "                 0.0526,\n",
      "                 0.1951,\n",
      "                 0.1728,\n",
      "                 0.294,\n",
      "                 0.038,\n",
      "                 1.8984,\n",
      "                 0.8731,\n",
      "                 0.3771,\n",
      "                 0.0502,\n",
      "                 0.4136,\n",
      "                 0.2794,\n",
      "                 1.2089,\n",
      "                 0.217,\n",
      "                 0.117,\n",
      "                 0.8087,\n",
      "                 0.1421,\n",
      "                 0.4633],\n",
      "  'val_loss': [0.5795891284942627,\n",
      "               0.5864213705062866,\n",
      "               0.5952324867248535,\n",
      "               0.6110905408859253,\n",
      "               0.6226202845573425,\n",
      "               0.6334880590438843,\n",
      "               0.6563827395439148,\n",
      "               0.6640914082527161,\n",
      "               0.678905725479126,\n",
      "               0.6944554448127747,\n",
      "               0.7029497027397156,\n",
      "               0.7156745791435242,\n",
      "               0.7194629907608032,\n",
      "               0.7252444624900818,\n",
      "               0.7366815805435181,\n",
      "               0.7420104742050171,\n",
      "               0.7369962930679321,\n",
      "               0.7311080694198608,\n",
      "               0.7365973591804504,\n",
      "               0.734221875667572,\n",
      "               0.7299143671989441,\n",
      "               0.7254944443702698,\n",
      "               0.7232208251953125,\n",
      "               0.7213309407234192,\n",
      "               0.7213150262832642,\n",
      "               0.7279753684997559,\n",
      "               0.7335953712463379,\n",
      "               0.7349262833595276,\n",
      "               0.7397759556770325,\n",
      "               0.746430516242981,\n",
      "               0.7536023259162903,\n",
      "               0.7587839961051941,\n",
      "               0.7621960639953613,\n",
      "               0.7618139982223511,\n",
      "               0.7634775042533875,\n",
      "               0.7654033899307251,\n",
      "               0.7656534314155579,\n",
      "               0.7656794786453247,\n",
      "               0.7654464244842529,\n",
      "               0.7654906511306763]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.6938,\n",
      "                 0.6228,\n",
      "                 1.4939,\n",
      "                 0.7716,\n",
      "                 0.9175,\n",
      "                 1.6864,\n",
      "                 0.9549,\n",
      "                 1.4666,\n",
      "                 1.3084,\n",
      "                 0.9508,\n",
      "                 0.4544,\n",
      "                 0.9093,\n",
      "                 1.5259,\n",
      "                 1.088,\n",
      "                 0.717,\n",
      "                 0.9391,\n",
      "                 1.0112,\n",
      "                 1.4392,\n",
      "                 0.6105,\n",
      "                 1.0275,\n",
      "                 1.1769,\n",
      "                 1.0011,\n",
      "                 1.4438,\n",
      "                 0.2073,\n",
      "                 0.6407,\n",
      "                 0.9926,\n",
      "                 0.9744,\n",
      "                 0.9947,\n",
      "                 0.9716,\n",
      "                 0.4072,\n",
      "                 0.6839,\n",
      "                 1.4769,\n",
      "                 0.9859,\n",
      "                 0.9315,\n",
      "                 0.5964,\n",
      "                 0.3963,\n",
      "                 1.6602,\n",
      "                 1.2921,\n",
      "                 0.551,\n",
      "                 0.3916,\n",
      "                 1.3152,\n",
      "                 0.3873,\n",
      "                 1.2151,\n",
      "                 0.441,\n",
      "                 1.3908,\n",
      "                 0.4915,\n",
      "                 0.9351,\n",
      "                 1.1775,\n",
      "                 0.4241,\n",
      "                 1.2363,\n",
      "                 0.4369,\n",
      "                 1.1362,\n",
      "                 1.0493,\n",
      "                 0.5108,\n",
      "                 1.1999,\n",
      "                 0.5293,\n",
      "                 1.023,\n",
      "                 1.0898,\n",
      "                 0.5218,\n",
      "                 1.0524,\n",
      "                 0.1936,\n",
      "                 1.216,\n",
      "                 0.7136,\n",
      "                 1.1192,\n",
      "                 1.3267,\n",
      "                 1.2684,\n",
      "                 0.5923,\n",
      "                 1.5247,\n",
      "                 0.4952,\n",
      "                 0.4991,\n",
      "                 0.4892,\n",
      "                 1.4528,\n",
      "                 1.2514,\n",
      "                 1.3103,\n",
      "                 1.0184,\n",
      "                 1.259,\n",
      "                 0.5405,\n",
      "                 0.1874,\n",
      "                 0.9329,\n",
      "                 0.6048,\n",
      "                 0.4444,\n",
      "                 0.7543,\n",
      "                 0.5211,\n",
      "                 0.4601,\n",
      "                 0.8226,\n",
      "                 1.088,\n",
      "                 0.7816,\n",
      "                 1.216,\n",
      "                 0.6718,\n",
      "                 0.476,\n",
      "                 0.927,\n",
      "                 1.1175,\n",
      "                 0.5103,\n",
      "                 0.7345,\n",
      "                 1.5,\n",
      "                 0.6576,\n",
      "                 0.3868,\n",
      "                 1.0417,\n",
      "                 0.2299,\n",
      "                 1.218,\n",
      "                 1.3987,\n",
      "                 0.431,\n",
      "                 0.8577,\n",
      "                 0.6771,\n",
      "                 1.5186,\n",
      "                 1.1716,\n",
      "                 0.8828,\n",
      "                 0.5872,\n",
      "                 0.8922,\n",
      "                 0.5791,\n",
      "                 0.6116,\n",
      "                 0.9076,\n",
      "                 0.5585,\n",
      "                 0.3717,\n",
      "                 0.9517,\n",
      "                 0.4955,\n",
      "                 1.6136,\n",
      "                 0.6476,\n",
      "                 1.4089,\n",
      "                 0.4808,\n",
      "                 1.008,\n",
      "                 0.4951,\n",
      "                 0.5077,\n",
      "                 1.2627,\n",
      "                 1.1221,\n",
      "                 0.7399,\n",
      "                 0.8159,\n",
      "                 0.5916,\n",
      "                 0.6042,\n",
      "                 1.468,\n",
      "                 0.417,\n",
      "                 0.5097,\n",
      "                 0.7607,\n",
      "                 0.545,\n",
      "                 1.2722,\n",
      "                 0.2833,\n",
      "                 1.0047,\n",
      "                 0.5352,\n",
      "                 0.4066,\n",
      "                 0.4796,\n",
      "                 1.0191,\n",
      "                 1.1997,\n",
      "                 0.7572,\n",
      "                 1.0275,\n",
      "                 0.8271,\n",
      "                 1.2156,\n",
      "                 0.5027,\n",
      "                 0.8518,\n",
      "                 0.6004,\n",
      "                 0.2857,\n",
      "                 0.8501,\n",
      "                 0.7047,\n",
      "                 0.6749,\n",
      "                 0.5755,\n",
      "                 0.3407,\n",
      "                 0.914,\n",
      "                 1.0862,\n",
      "                 0.6137,\n",
      "                 0.7885,\n",
      "                 0.4627,\n",
      "                 0.6015,\n",
      "                 0.7149,\n",
      "                 0.3891,\n",
      "                 0.6485,\n",
      "                 1.0359,\n",
      "                 0.8056,\n",
      "                 0.7173,\n",
      "                 0.4124,\n",
      "                 1.0645,\n",
      "                 0.521,\n",
      "                 0.27,\n",
      "                 0.5773,\n",
      "                 0.8709,\n",
      "                 0.9952,\n",
      "                 0.6136,\n",
      "                 0.6316,\n",
      "                 0.6735,\n",
      "                 0.6009,\n",
      "                 0.5142,\n",
      "                 0.6369,\n",
      "                 0.6953,\n",
      "                 1.1345,\n",
      "                 1.2464,\n",
      "                 0.3206,\n",
      "                 0.5139,\n",
      "                 0.5838,\n",
      "                 0.6514,\n",
      "                 0.8807,\n",
      "                 0.4371,\n",
      "                 0.6637,\n",
      "                 0.3423,\n",
      "                 1.1021,\n",
      "                 0.4963,\n",
      "                 0.6338,\n",
      "                 1.1384,\n",
      "                 0.2483,\n",
      "                 0.5319,\n",
      "                 0.8831,\n",
      "                 0.6356,\n",
      "                 0.8496,\n",
      "                 0.3665,\n",
      "                 0.4276,\n",
      "                 0.7016,\n",
      "                 0.3308,\n",
      "                 0.3587,\n",
      "                 1.4316,\n",
      "                 0.7909,\n",
      "                 0.8979,\n",
      "                 0.9227,\n",
      "                 0.8092,\n",
      "                 0.2254,\n",
      "                 0.32,\n",
      "                 1.4879,\n",
      "                 0.7257,\n",
      "                 0.2686,\n",
      "                 0.2824,\n",
      "                 1.0077,\n",
      "                 0.4368,\n",
      "                 1.1581,\n",
      "                 0.487,\n",
      "                 0.9044,\n",
      "                 0.5328,\n",
      "                 0.4429,\n",
      "                 0.4984,\n",
      "                 0.3952,\n",
      "                 0.7401,\n",
      "                 0.555,\n",
      "                 0.3378,\n",
      "                 1.1541,\n",
      "                 0.2901,\n",
      "                 0.8789,\n",
      "                 0.5954,\n",
      "                 0.3735,\n",
      "                 0.4731,\n",
      "                 0.917,\n",
      "                 0.4443,\n",
      "                 0.578,\n",
      "                 0.9912,\n",
      "                 0.4797,\n",
      "                 0.6243,\n",
      "                 0.9354,\n",
      "                 0.347,\n",
      "                 0.2463,\n",
      "                 0.6775,\n",
      "                 0.8968,\n",
      "                 0.5664,\n",
      "                 0.6352,\n",
      "                 0.5445,\n",
      "                 0.5654,\n",
      "                 0.4231,\n",
      "                 0.7736,\n",
      "                 0.2955,\n",
      "                 0.536,\n",
      "                 1.1952,\n",
      "                 0.5242,\n",
      "                 0.3179,\n",
      "                 0.3289,\n",
      "                 0.9573,\n",
      "                 0.5651,\n",
      "                 0.407,\n",
      "                 0.4217,\n",
      "                 0.9668,\n",
      "                 0.3538,\n",
      "                 0.7646,\n",
      "                 0.8859,\n",
      "                 0.8427,\n",
      "                 0.8286,\n",
      "                 0.2943,\n",
      "                 0.6341,\n",
      "                 0.3837,\n",
      "                 0.3499,\n",
      "                 0.7438,\n",
      "                 0.3934,\n",
      "                 0.4974,\n",
      "                 0.3407,\n",
      "                 0.842,\n",
      "                 0.8206,\n",
      "                 0.6107,\n",
      "                 0.7839,\n",
      "                 0.3891,\n",
      "                 0.4043,\n",
      "                 0.3886,\n",
      "                 0.982,\n",
      "                 0.3718,\n",
      "                 0.3571,\n",
      "                 1.0589,\n",
      "                 0.3246,\n",
      "                 0.9164,\n",
      "                 0.914,\n",
      "                 0.2801,\n",
      "                 0.7194,\n",
      "                 0.7538,\n",
      "                 0.9853,\n",
      "                 0.1125,\n",
      "                 0.3233,\n",
      "                 0.5866,\n",
      "                 0.3297,\n",
      "                 0.908,\n",
      "                 0.2867,\n",
      "                 0.3852,\n",
      "                 0.5905,\n",
      "                 0.5949,\n",
      "                 0.5263,\n",
      "                 0.7029,\n",
      "                 0.5837,\n",
      "                 0.3647,\n",
      "                 0.8522,\n",
      "                 0.299,\n",
      "                 0.6826,\n",
      "                 1.1565,\n",
      "                 0.5137,\n",
      "                 0.691,\n",
      "                 0.5866,\n",
      "                 0.8177,\n",
      "                 0.6277,\n",
      "                 0.6651,\n",
      "                 0.8348,\n",
      "                 0.4095,\n",
      "                 0.6725,\n",
      "                 0.6467],\n",
      "  'val_loss': [0.5779986381530762,\n",
      "               0.5726189613342285,\n",
      "               0.5672434568405151,\n",
      "               0.5611957311630249,\n",
      "               0.5572670698165894,\n",
      "               0.5576747059822083,\n",
      "               0.5592087507247925,\n",
      "               0.5606632232666016,\n",
      "               0.5621997117996216,\n",
      "               0.5624957084655762,\n",
      "               0.5637924075126648,\n",
      "               0.56509929895401,\n",
      "               0.5667134523391724,\n",
      "               0.5668965578079224,\n",
      "               0.5690419673919678,\n",
      "               0.5720330476760864,\n",
      "               0.574988067150116,\n",
      "               0.5762372612953186,\n",
      "               0.5789700746536255,\n",
      "               0.5814751386642456,\n",
      "               0.5840965509414673,\n",
      "               0.5855680704116821,\n",
      "               0.5877582430839539,\n",
      "               0.5895400047302246,\n",
      "               0.591866135597229,\n",
      "               0.592967689037323,\n",
      "               0.5947430729866028,\n",
      "               0.596260666847229,\n",
      "               0.5973766446113586,\n",
      "               0.5978350639343262,\n",
      "               0.5986273288726807,\n",
      "               0.5987110137939453,\n",
      "               0.5988172888755798,\n",
      "               0.599635660648346,\n",
      "               0.6004294157028198,\n",
      "               0.6010777354240417,\n",
      "               0.6010838747024536,\n",
      "               0.6013306975364685,\n",
      "               0.6015653014183044,\n",
      "               0.6015810966491699]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.8991,\n",
      "                 1.1271,\n",
      "                 0.4166,\n",
      "                 0.7887,\n",
      "                 0.7142,\n",
      "                 0.8257,\n",
      "                 0.0912,\n",
      "                 0.658,\n",
      "                 0.0812,\n",
      "                 0.2413,\n",
      "                 1.3795,\n",
      "                 1.2098,\n",
      "                 0.5739,\n",
      "                 0.3828,\n",
      "                 1.0631,\n",
      "                 0.8185,\n",
      "                 0.8957,\n",
      "                 0.6942,\n",
      "                 0.9609,\n",
      "                 0.7283,\n",
      "                 0.1467,\n",
      "                 1.0626,\n",
      "                 0.1614,\n",
      "                 1.0773,\n",
      "                 0.6838,\n",
      "                 1.1285,\n",
      "                 0.3379,\n",
      "                 0.4637,\n",
      "                 1.1361,\n",
      "                 0.5489,\n",
      "                 0.8435,\n",
      "                 0.5998,\n",
      "                 0.5161,\n",
      "                 0.2193,\n",
      "                 1.268,\n",
      "                 1.0742,\n",
      "                 0.38,\n",
      "                 0.8782,\n",
      "                 0.5234,\n",
      "                 1.1155,\n",
      "                 0.5473,\n",
      "                 1.3551,\n",
      "                 0.3041,\n",
      "                 1.009,\n",
      "                 0.6334,\n",
      "                 0.4411,\n",
      "                 0.8794,\n",
      "                 0.2483,\n",
      "                 0.5106,\n",
      "                 0.2799,\n",
      "                 0.9722,\n",
      "                 0.3472,\n",
      "                 0.5114,\n",
      "                 0.9135,\n",
      "                 0.5534,\n",
      "                 0.695,\n",
      "                 0.1295,\n",
      "                 0.4576,\n",
      "                 0.6541,\n",
      "                 0.5586,\n",
      "                 1.0428,\n",
      "                 1.5863,\n",
      "                 0.464,\n",
      "                 0.3546,\n",
      "                 0.1584,\n",
      "                 0.1678,\n",
      "                 1.4762,\n",
      "                 0.6067,\n",
      "                 0.4402,\n",
      "                 0.8799,\n",
      "                 1.1004,\n",
      "                 0.6538,\n",
      "                 1.1758,\n",
      "                 0.1019,\n",
      "                 0.3812,\n",
      "                 0.7077,\n",
      "                 0.987,\n",
      "                 0.9669,\n",
      "                 1.0457,\n",
      "                 0.6861,\n",
      "                 0.8519,\n",
      "                 0.7922,\n",
      "                 0.4958,\n",
      "                 0.2019,\n",
      "                 0.9591,\n",
      "                 0.806,\n",
      "                 0.6454,\n",
      "                 0.141,\n",
      "                 0.8695,\n",
      "                 0.3486,\n",
      "                 0.8178,\n",
      "                 0.1148,\n",
      "                 0.8887,\n",
      "                 0.5157,\n",
      "                 0.4541,\n",
      "                 0.3725,\n",
      "                 0.5465,\n",
      "                 0.3561,\n",
      "                 0.7275,\n",
      "                 0.3376,\n",
      "                 0.3926,\n",
      "                 0.5073,\n",
      "                 0.2914,\n",
      "                 1.1714,\n",
      "                 0.4439,\n",
      "                 0.3767,\n",
      "                 0.8597,\n",
      "                 0.6291,\n",
      "                 0.0709,\n",
      "                 0.9074,\n",
      "                 0.3252,\n",
      "                 0.5986,\n",
      "                 0.1971,\n",
      "                 0.6608,\n",
      "                 0.827,\n",
      "                 0.4708,\n",
      "                 0.2478,\n",
      "                 0.2804,\n",
      "                 0.9345,\n",
      "                 0.7135,\n",
      "                 0.5169,\n",
      "                 0.5293,\n",
      "                 0.462,\n",
      "                 0.5221,\n",
      "                 0.4226,\n",
      "                 0.4426,\n",
      "                 0.3616,\n",
      "                 0.22,\n",
      "                 0.3644,\n",
      "                 0.2279,\n",
      "                 0.2919,\n",
      "                 0.6931,\n",
      "                 0.6041,\n",
      "                 0.9457,\n",
      "                 0.6075,\n",
      "                 0.6027,\n",
      "                 0.6291,\n",
      "                 0.3248,\n",
      "                 0.3899,\n",
      "                 0.8217,\n",
      "                 0.5351,\n",
      "                 0.2263,\n",
      "                 0.2198,\n",
      "                 0.9376,\n",
      "                 1.0142,\n",
      "                 0.4124,\n",
      "                 0.1485,\n",
      "                 0.2556,\n",
      "                 0.5794,\n",
      "                 0.7615,\n",
      "                 0.4918,\n",
      "                 0.4455,\n",
      "                 0.362,\n",
      "                 0.2999,\n",
      "                 0.7221,\n",
      "                 0.2214,\n",
      "                 1.0177,\n",
      "                 0.7475,\n",
      "                 0.2554,\n",
      "                 0.1242,\n",
      "                 0.7049,\n",
      "                 0.7447,\n",
      "                 0.4735,\n",
      "                 0.5689,\n",
      "                 0.553,\n",
      "                 0.2562,\n",
      "                 0.4254,\n",
      "                 0.1328,\n",
      "                 0.8459,\n",
      "                 0.5748,\n",
      "                 0.475,\n",
      "                 0.2898,\n",
      "                 0.2031,\n",
      "                 0.585,\n",
      "                 0.2118,\n",
      "                 0.7376,\n",
      "                 0.2079,\n",
      "                 0.6935,\n",
      "                 0.2879,\n",
      "                 0.5327,\n",
      "                 0.3638,\n",
      "                 0.5131,\n",
      "                 0.5242,\n",
      "                 0.4299,\n",
      "                 1.0242,\n",
      "                 0.3561,\n",
      "                 0.3137,\n",
      "                 0.358,\n",
      "                 0.3273,\n",
      "                 0.3962,\n",
      "                 0.163,\n",
      "                 0.4194,\n",
      "                 0.2093,\n",
      "                 0.1277,\n",
      "                 0.2149,\n",
      "                 0.247,\n",
      "                 1.1935,\n",
      "                 0.864,\n",
      "                 0.1785,\n",
      "                 0.3141,\n",
      "                 0.6393,\n",
      "                 0.3558,\n",
      "                 0.3425,\n",
      "                 0.5711,\n",
      "                 0.1799,\n",
      "                 0.2955,\n",
      "                 0.3014,\n",
      "                 0.2224,\n",
      "                 0.3661,\n",
      "                 0.0822,\n",
      "                 0.4271,\n",
      "                 0.3479,\n",
      "                 0.5179,\n",
      "                 0.1108,\n",
      "                 0.6485,\n",
      "                 0.4794,\n",
      "                 0.5163,\n",
      "                 0.6826,\n",
      "                 0.3567,\n",
      "                 0.4165,\n",
      "                 0.3911,\n",
      "                 0.468,\n",
      "                 0.543,\n",
      "                 0.2408,\n",
      "                 0.4029,\n",
      "                 0.4085,\n",
      "                 0.5383,\n",
      "                 0.3726,\n",
      "                 0.3908,\n",
      "                 0.3189,\n",
      "                 0.404,\n",
      "                 0.3788,\n",
      "                 0.1184,\n",
      "                 0.4364,\n",
      "                 0.1826,\n",
      "                 0.915,\n",
      "                 0.3852,\n",
      "                 0.2763,\n",
      "                 0.3526,\n",
      "                 0.2338,\n",
      "                 0.1677,\n",
      "                 0.5094,\n",
      "                 0.3337,\n",
      "                 0.3416,\n",
      "                 0.113,\n",
      "                 0.3629,\n",
      "                 0.2801,\n",
      "                 0.4778,\n",
      "                 0.1276,\n",
      "                 0.3044,\n",
      "                 0.5909,\n",
      "                 0.3271,\n",
      "                 0.3397,\n",
      "                 0.3967,\n",
      "                 0.5132,\n",
      "                 0.3554,\n",
      "                 0.1604,\n",
      "                 0.5781,\n",
      "                 0.4293,\n",
      "                 0.9178,\n",
      "                 0.354,\n",
      "                 0.4808,\n",
      "                 0.1684,\n",
      "                 0.2874,\n",
      "                 0.3197,\n",
      "                 0.5765,\n",
      "                 0.0678,\n",
      "                 0.5663,\n",
      "                 0.3063,\n",
      "                 0.1576,\n",
      "                 0.3934,\n",
      "                 0.5119,\n",
      "                 0.7908,\n",
      "                 0.0783,\n",
      "                 0.1801,\n",
      "                 0.3607,\n",
      "                 0.388,\n",
      "                 0.1954,\n",
      "                 0.5318,\n",
      "                 0.4493,\n",
      "                 0.2481,\n",
      "                 0.1759,\n",
      "                 0.3186,\n",
      "                 0.6044,\n",
      "                 0.3611,\n",
      "                 0.4519,\n",
      "                 0.2912,\n",
      "                 0.4603,\n",
      "                 0.7052,\n",
      "                 0.3952,\n",
      "                 0.3398,\n",
      "                 0.3332,\n",
      "                 0.3028,\n",
      "                 0.2364,\n",
      "                 0.3899,\n",
      "                 0.1381,\n",
      "                 0.2578,\n",
      "                 0.3567,\n",
      "                 0.3975,\n",
      "                 0.132,\n",
      "                 0.2308,\n",
      "                 0.8252,\n",
      "                 0.4832,\n",
      "                 0.4483,\n",
      "                 0.5855,\n",
      "                 0.2209,\n",
      "                 0.36,\n",
      "                 0.2006,\n",
      "                 0.6083,\n",
      "                 0.5687,\n",
      "                 0.3048,\n",
      "                 0.4512,\n",
      "                 0.3565,\n",
      "                 0.4439,\n",
      "                 0.5983,\n",
      "                 0.285,\n",
      "                 0.1741,\n",
      "                 0.4871,\n",
      "                 0.164,\n",
      "                 0.2117],\n",
      "  'val_loss': [0.5779100656509399,\n",
      "               0.5759962797164917,\n",
      "               0.5722674131393433,\n",
      "               0.5640902519226074,\n",
      "               0.5573773980140686,\n",
      "               0.5535929799079895,\n",
      "               0.549949049949646,\n",
      "               0.5486936569213867,\n",
      "               0.5479177236557007,\n",
      "               0.5458911657333374,\n",
      "               0.5442951917648315,\n",
      "               0.5429588556289673,\n",
      "               0.5415700078010559,\n",
      "               0.5405259132385254,\n",
      "               0.5398462414741516,\n",
      "               0.5384230613708496,\n",
      "               0.5372956395149231,\n",
      "               0.5346681475639343,\n",
      "               0.5325101017951965,\n",
      "               0.530144453048706,\n",
      "               0.5280891060829163,\n",
      "               0.5261996984481812,\n",
      "               0.5246917605400085,\n",
      "               0.5230596661567688,\n",
      "               0.5211900472640991,\n",
      "               0.5201350450515747,\n",
      "               0.519230306148529,\n",
      "               0.5186260342597961,\n",
      "               0.5179481506347656,\n",
      "               0.5175355076789856,\n",
      "               0.5169385671615601,\n",
      "               0.5164985656738281,\n",
      "               0.5162174105644226,\n",
      "               0.5159298181533813,\n",
      "               0.5156415104866028,\n",
      "               0.5153260231018066,\n",
      "               0.5151647329330444,\n",
      "               0.5148182511329651,\n",
      "               0.5146533250808716,\n",
      "               0.5146048665046692]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.6556,\n",
      "                 0.5749,\n",
      "                 0.9674,\n",
      "                 0.5481,\n",
      "                 0.8822,\n",
      "                 0.8343,\n",
      "                 0.6635,\n",
      "                 0.8348,\n",
      "                 0.913,\n",
      "                 0.9666,\n",
      "                 0.3354,\n",
      "                 0.9816,\n",
      "                 0.619,\n",
      "                 1.1765,\n",
      "                 0.4843,\n",
      "                 0.3628,\n",
      "                 0.9414,\n",
      "                 0.8753,\n",
      "                 0.4461,\n",
      "                 0.3866,\n",
      "                 0.5609,\n",
      "                 0.7392,\n",
      "                 0.8865,\n",
      "                 0.4062,\n",
      "                 0.9274,\n",
      "                 0.468,\n",
      "                 0.5455,\n",
      "                 0.8554,\n",
      "                 0.8561,\n",
      "                 0.3509,\n",
      "                 0.6906,\n",
      "                 0.9319,\n",
      "                 0.6024,\n",
      "                 0.7702,\n",
      "                 0.545,\n",
      "                 0.352,\n",
      "                 1.1239,\n",
      "                 1.1451,\n",
      "                 0.564,\n",
      "                 0.4015,\n",
      "                 1.121,\n",
      "                 0.332,\n",
      "                 0.8625,\n",
      "                 0.6747,\n",
      "                 0.8063,\n",
      "                 0.3148,\n",
      "                 0.6874,\n",
      "                 0.495,\n",
      "                 0.6072,\n",
      "                 0.8721,\n",
      "                 0.3202,\n",
      "                 1.0922,\n",
      "                 0.5652,\n",
      "                 0.4089,\n",
      "                 0.8107,\n",
      "                 0.2046,\n",
      "                 1.2669,\n",
      "                 0.5995,\n",
      "                 0.7353,\n",
      "                 0.4066,\n",
      "                 0.548,\n",
      "                 0.7596,\n",
      "                 0.5412,\n",
      "                 0.6812,\n",
      "                 0.8592,\n",
      "                 1.0242,\n",
      "                 0.2453,\n",
      "                 0.7406,\n",
      "                 0.3006,\n",
      "                 0.8574,\n",
      "                 0.5412,\n",
      "                 0.7066,\n",
      "                 0.6028,\n",
      "                 0.8025,\n",
      "                 0.6342,\n",
      "                 0.9249,\n",
      "                 0.4419,\n",
      "                 0.3057,\n",
      "                 0.4889,\n",
      "                 0.8411,\n",
      "                 0.6186,\n",
      "                 0.8926,\n",
      "                 0.9881,\n",
      "                 0.3093,\n",
      "                 0.3153,\n",
      "                 0.682,\n",
      "                 0.7697,\n",
      "                 0.9563,\n",
      "                 0.5274,\n",
      "                 0.5455,\n",
      "                 0.6419,\n",
      "                 0.6675,\n",
      "                 0.5354,\n",
      "                 0.4228,\n",
      "                 0.9908,\n",
      "                 0.5439,\n",
      "                 0.5917,\n",
      "                 1.0971,\n",
      "                 0.2294,\n",
      "                 0.5668,\n",
      "                 0.6728,\n",
      "                 0.6962,\n",
      "                 0.6211,\n",
      "                 0.1588,\n",
      "                 1.3175,\n",
      "                 0.7712,\n",
      "                 0.302,\n",
      "                 0.4215,\n",
      "                 0.9425,\n",
      "                 0.585,\n",
      "                 0.2302,\n",
      "                 1.2272,\n",
      "                 0.4826,\n",
      "                 0.4646,\n",
      "                 0.2063,\n",
      "                 0.4792,\n",
      "                 0.7183,\n",
      "                 0.7366,\n",
      "                 0.6786,\n",
      "                 0.4138,\n",
      "                 0.6618,\n",
      "                 0.5285,\n",
      "                 0.3696,\n",
      "                 0.8508,\n",
      "                 0.6613,\n",
      "                 0.9377,\n",
      "                 0.5578,\n",
      "                 0.2659,\n",
      "                 0.653,\n",
      "                 0.9357,\n",
      "                 0.2473,\n",
      "                 0.5971,\n",
      "                 0.5748,\n",
      "                 0.1728,\n",
      "                 0.8376,\n",
      "                 0.416,\n",
      "                 0.9343,\n",
      "                 0.3283,\n",
      "                 0.34,\n",
      "                 0.1813,\n",
      "                 0.3819,\n",
      "                 0.5068,\n",
      "                 0.6413,\n",
      "                 0.6999,\n",
      "                 0.8505,\n",
      "                 0.8117,\n",
      "                 0.4684,\n",
      "                 0.6842,\n",
      "                 0.6218,\n",
      "                 0.3949,\n",
      "                 0.4234,\n",
      "                 0.3996,\n",
      "                 0.4419,\n",
      "                 0.444,\n",
      "                 0.3706,\n",
      "                 0.9922,\n",
      "                 0.7605,\n",
      "                 0.2059,\n",
      "                 0.5519,\n",
      "                 0.3657,\n",
      "                 0.3013,\n",
      "                 0.6346,\n",
      "                 0.2799,\n",
      "                 0.4842,\n",
      "                 0.6534,\n",
      "                 0.4248,\n",
      "                 0.6622,\n",
      "                 0.3483,\n",
      "                 0.5285,\n",
      "                 0.6397,\n",
      "                 0.4359,\n",
      "                 0.2622,\n",
      "                 0.895,\n",
      "                 0.663,\n",
      "                 0.4928,\n",
      "                 0.4389,\n",
      "                 0.3254,\n",
      "                 0.3453,\n",
      "                 0.355,\n",
      "                 0.3734,\n",
      "                 0.3421,\n",
      "                 0.7786,\n",
      "                 0.4213,\n",
      "                 0.4755,\n",
      "                 0.5494,\n",
      "                 0.5661,\n",
      "                 0.3491,\n",
      "                 0.1922,\n",
      "                 0.5772,\n",
      "                 0.518,\n",
      "                 0.4821,\n",
      "                 1.0165,\n",
      "                 0.6674,\n",
      "                 0.8969,\n",
      "                 0.832,\n",
      "                 0.6336,\n",
      "                 0.1909,\n",
      "                 0.6586,\n",
      "                 0.4736,\n",
      "                 0.4183,\n",
      "                 0.255,\n",
      "                 0.5557,\n",
      "                 0.2671,\n",
      "                 0.3301,\n",
      "                 0.506,\n",
      "                 0.9408,\n",
      "                 0.8334,\n",
      "                 0.759,\n",
      "                 0.8303,\n",
      "                 0.4342,\n",
      "                 0.4774,\n",
      "                 0.8752,\n",
      "                 0.4929,\n",
      "                 0.7244,\n",
      "                 0.2811,\n",
      "                 0.141,\n",
      "                 0.6612,\n",
      "                 0.234,\n",
      "                 0.6398,\n",
      "                 1.0409,\n",
      "                 0.4039,\n",
      "                 0.4455,\n",
      "                 0.5299,\n",
      "                 0.8958,\n",
      "                 0.1387,\n",
      "                 0.5381,\n",
      "                 0.3457,\n",
      "                 0.4692,\n",
      "                 0.6579,\n",
      "                 0.9836,\n",
      "                 0.5606,\n",
      "                 0.7336,\n",
      "                 0.5611,\n",
      "                 0.204,\n",
      "                 0.4505,\n",
      "                 0.2167,\n",
      "                 0.6929,\n",
      "                 1.0147,\n",
      "                 0.1801,\n",
      "                 0.6582,\n",
      "                 0.5414,\n",
      "                 0.7212,\n",
      "                 0.4413,\n",
      "                 0.3529,\n",
      "                 0.6102,\n",
      "                 0.2776,\n",
      "                 0.7831,\n",
      "                 0.2342,\n",
      "                 0.4513,\n",
      "                 0.2664,\n",
      "                 0.2465,\n",
      "                 0.4413,\n",
      "                 0.4184,\n",
      "                 0.7881,\n",
      "                 0.4849,\n",
      "                 0.3425,\n",
      "                 0.2015,\n",
      "                 0.4925,\n",
      "                 0.3966,\n",
      "                 0.4674,\n",
      "                 0.2269,\n",
      "                 0.3652,\n",
      "                 0.6633,\n",
      "                 1.0002,\n",
      "                 0.5187,\n",
      "                 0.5411,\n",
      "                 0.4845,\n",
      "                 0.3869,\n",
      "                 0.5725,\n",
      "                 0.436,\n",
      "                 0.3892,\n",
      "                 0.2847,\n",
      "                 0.2972,\n",
      "                 0.4861,\n",
      "                 0.5061,\n",
      "                 0.1756,\n",
      "                 0.4811,\n",
      "                 0.9756,\n",
      "                 0.6911,\n",
      "                 0.3651,\n",
      "                 0.2851,\n",
      "                 0.5805,\n",
      "                 0.3585,\n",
      "                 0.3943,\n",
      "                 0.2712,\n",
      "                 0.5693,\n",
      "                 0.256,\n",
      "                 0.6669,\n",
      "                 0.1796,\n",
      "                 0.2274,\n",
      "                 0.631,\n",
      "                 0.5372,\n",
      "                 0.5118,\n",
      "                 0.1154,\n",
      "                 0.3861,\n",
      "                 0.6334,\n",
      "                 0.4827,\n",
      "                 0.5566,\n",
      "                 0.2853,\n",
      "                 0.4828,\n",
      "                 0.5627,\n",
      "                 0.6123,\n",
      "                 0.2672,\n",
      "                 0.8928,\n",
      "                 0.404,\n",
      "                 0.4283,\n",
      "                 0.6142,\n",
      "                 0.3313,\n",
      "                 0.1901,\n",
      "                 0.4312,\n",
      "                 0.3136,\n",
      "                 0.5271,\n",
      "                 0.3481,\n",
      "                 0.2709,\n",
      "                 0.4199,\n",
      "                 0.6471,\n",
      "                 0.5177,\n",
      "                 0.1157,\n",
      "                 1.0454,\n",
      "                 0.4089],\n",
      "  'val_loss': [0.5781622529029846,\n",
      "               0.5755650997161865,\n",
      "               0.570477306842804,\n",
      "               0.5666488409042358,\n",
      "               0.5652552843093872,\n",
      "               0.5682371854782104,\n",
      "               0.5741523504257202,\n",
      "               0.573784589767456,\n",
      "               0.576994776725769,\n",
      "               0.5810468792915344,\n",
      "               0.5792295336723328,\n",
      "               0.5754822492599487,\n",
      "               0.5763672590255737,\n",
      "               0.5791569948196411,\n",
      "               0.5775710940361023,\n",
      "               0.5773320198059082,\n",
      "               0.574817955493927,\n",
      "               0.5773640871047974,\n",
      "               0.5771891474723816,\n",
      "               0.5796284675598145,\n",
      "               0.5791186094284058,\n",
      "               0.5804052352905273,\n",
      "               0.5790531039237976,\n",
      "               0.5799471139907837,\n",
      "               0.5788903832435608,\n",
      "               0.5795086622238159,\n",
      "               0.579933762550354,\n",
      "               0.5790453553199768,\n",
      "               0.5790569186210632,\n",
      "               0.5796419978141785,\n",
      "               0.5787555575370789,\n",
      "               0.5779547691345215,\n",
      "               0.5779174566268921,\n",
      "               0.5770056843757629,\n",
      "               0.5765862464904785,\n",
      "               0.5764504075050354,\n",
      "               0.5769127011299133,\n",
      "               0.5770668983459473,\n",
      "               0.5769267678260803,\n",
      "               0.5769262313842773]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.1987,\n",
      "                 1.6899,\n",
      "                 1.2359,\n",
      "                 1.344,\n",
      "                 1.2749,\n",
      "                 1.1517,\n",
      "                 0.3393,\n",
      "                 0.2489,\n",
      "                 0.2334,\n",
      "                 0.6325,\n",
      "                 1.1528,\n",
      "                 1.0976,\n",
      "                 0.2849,\n",
      "                 1.697,\n",
      "                 1.3866,\n",
      "                 0.5666,\n",
      "                 1.0577,\n",
      "                 0.1501,\n",
      "                 1.2083,\n",
      "                 0.8204,\n",
      "                 1.031,\n",
      "                 0.9538,\n",
      "                 0.3254,\n",
      "                 1.4519,\n",
      "                 0.9526,\n",
      "                 1.3171,\n",
      "                 0.4556,\n",
      "                 1.1236,\n",
      "                 1.6053,\n",
      "                 0.1421,\n",
      "                 1.3935,\n",
      "                 0.1866,\n",
      "                 0.1967,\n",
      "                 0.8104,\n",
      "                 1.2158,\n",
      "                 0.7378,\n",
      "                 0.7199,\n",
      "                 1.0516,\n",
      "                 0.2961,\n",
      "                 1.839,\n",
      "                 0.1771,\n",
      "                 0.9966,\n",
      "                 1.4368,\n",
      "                 1.3725,\n",
      "                 1.0006,\n",
      "                 0.0729,\n",
      "                 0.7047,\n",
      "                 0.3208,\n",
      "                 0.8198,\n",
      "                 0.7687,\n",
      "                 0.617,\n",
      "                 1.3971,\n",
      "                 0.4936,\n",
      "                 1.1377,\n",
      "                 0.2522,\n",
      "                 0.9134,\n",
      "                 0.4551,\n",
      "                 0.1853,\n",
      "                 0.1766,\n",
      "                 1.5181,\n",
      "                 1.2478,\n",
      "                 1.6957,\n",
      "                 0.8413,\n",
      "                 0.6182,\n",
      "                 0.8281,\n",
      "                 1.3605,\n",
      "                 1.3168,\n",
      "                 0.2065,\n",
      "                 0.0475,\n",
      "                 0.927,\n",
      "                 1.4089,\n",
      "                 0.8149,\n",
      "                 1.0307,\n",
      "                 0.3751,\n",
      "                 1.6509,\n",
      "                 0.0933,\n",
      "                 1.6776,\n",
      "                 0.771,\n",
      "                 0.34,\n",
      "                 0.7566,\n",
      "                 1.7919,\n",
      "                 1.0329,\n",
      "                 1.4634,\n",
      "                 0.1382,\n",
      "                 0.6834,\n",
      "                 0.1187,\n",
      "                 1.2102,\n",
      "                 0.5585,\n",
      "                 1.0869,\n",
      "                 0.2118,\n",
      "                 1.0673,\n",
      "                 0.2946,\n",
      "                 1.3287,\n",
      "                 0.3742,\n",
      "                 0.6551,\n",
      "                 0.8713,\n",
      "                 0.7581,\n",
      "                 0.6126,\n",
      "                 0.4916,\n",
      "                 0.9688,\n",
      "                 1.0585,\n",
      "                 0.6259,\n",
      "                 0.2806,\n",
      "                 0.8113,\n",
      "                 0.9277,\n",
      "                 0.7613,\n",
      "                 0.5231,\n",
      "                 0.9095,\n",
      "                 0.9641,\n",
      "                 0.9316,\n",
      "                 0.3049,\n",
      "                 0.4089,\n",
      "                 0.8103,\n",
      "                 0.8288,\n",
      "                 0.9712,\n",
      "                 0.9074,\n",
      "                 0.7019,\n",
      "                 0.1863,\n",
      "                 0.8301,\n",
      "                 0.8391,\n",
      "                 0.5304,\n",
      "                 1.3406,\n",
      "                 0.622,\n",
      "                 0.6101,\n",
      "                 1.106,\n",
      "                 1.0962,\n",
      "                 0.3766,\n",
      "                 0.0609,\n",
      "                 1.6887,\n",
      "                 0.2749,\n",
      "                 0.1921,\n",
      "                 0.5422,\n",
      "                 0.1472,\n",
      "                 0.5159,\n",
      "                 0.6865,\n",
      "                 1.2421,\n",
      "                 1.439,\n",
      "                 0.0619,\n",
      "                 0.8798,\n",
      "                 0.6847,\n",
      "                 0.6233,\n",
      "                 1.1423,\n",
      "                 0.9022,\n",
      "                 0.2908,\n",
      "                 0.6716,\n",
      "                 1.8057,\n",
      "                 0.2559,\n",
      "                 0.883,\n",
      "                 0.8581,\n",
      "                 1.6418,\n",
      "                 0.4822,\n",
      "                 0.1204,\n",
      "                 0.8934,\n",
      "                 1.5836,\n",
      "                 1.9654,\n",
      "                 0.2802,\n",
      "                 0.3344,\n",
      "                 1.3001,\n",
      "                 0.2813,\n",
      "                 0.0635,\n",
      "                 0.9312,\n",
      "                 1.0039,\n",
      "                 0.3635,\n",
      "                 1.0147,\n",
      "                 1.4644,\n",
      "                 0.2029,\n",
      "                 0.9242,\n",
      "                 0.1745,\n",
      "                 0.3846,\n",
      "                 0.8077,\n",
      "                 1.6325,\n",
      "                 0.0857,\n",
      "                 1.0279,\n",
      "                 0.9237,\n",
      "                 0.4074,\n",
      "                 0.5041,\n",
      "                 0.1063,\n",
      "                 0.996,\n",
      "                 0.8874,\n",
      "                 0.9724,\n",
      "                 1.0591,\n",
      "                 0.2085,\n",
      "                 0.5477,\n",
      "                 1.0467,\n",
      "                 1.0801,\n",
      "                 1.0574,\n",
      "                 0.7543,\n",
      "                 0.4094,\n",
      "                 0.7831,\n",
      "                 0.9892,\n",
      "                 0.104,\n",
      "                 0.1298,\n",
      "                 0.7321,\n",
      "                 0.3087,\n",
      "                 0.1368,\n",
      "                 0.7973,\n",
      "                 1.5075,\n",
      "                 0.7181,\n",
      "                 0.1371,\n",
      "                 0.9345,\n",
      "                 0.7284,\n",
      "                 0.4068,\n",
      "                 0.4706,\n",
      "                 1.0452,\n",
      "                 0.9218,\n",
      "                 0.4656,\n",
      "                 0.6864,\n",
      "                 0.2415,\n",
      "                 0.1339,\n",
      "                 0.1592,\n",
      "                 1.0844,\n",
      "                 1.5717,\n",
      "                 0.4033,\n",
      "                 0.3187,\n",
      "                 0.825,\n",
      "                 0.4641,\n",
      "                 0.6836,\n",
      "                 0.5017,\n",
      "                 0.085,\n",
      "                 0.9505,\n",
      "                 0.3234,\n",
      "                 0.7028,\n",
      "                 0.8757,\n",
      "                 0.4383,\n",
      "                 0.266,\n",
      "                 0.1348,\n",
      "                 0.6945,\n",
      "                 1.02,\n",
      "                 0.3417,\n",
      "                 1.4162,\n",
      "                 0.2677,\n",
      "                 0.813,\n",
      "                 0.6246,\n",
      "                 0.2416,\n",
      "                 0.2161,\n",
      "                 1.0227,\n",
      "                 0.619,\n",
      "                 1.3903,\n",
      "                 0.1801,\n",
      "                 0.3059,\n",
      "                 0.1159,\n",
      "                 0.689,\n",
      "                 1.3815,\n",
      "                 0.5982,\n",
      "                 0.2703,\n",
      "                 0.1295,\n",
      "                 1.6646,\n",
      "                 0.9769,\n",
      "                 0.1839,\n",
      "                 0.3443,\n",
      "                 0.826,\n",
      "                 1.3678,\n",
      "                 1.7612,\n",
      "                 0.3949,\n",
      "                 0.2179,\n",
      "                 0.3496,\n",
      "                 0.1341,\n",
      "                 0.4691,\n",
      "                 0.1733,\n",
      "                 0.7371,\n",
      "                 0.3222,\n",
      "                 0.2742,\n",
      "                 1.6221,\n",
      "                 0.4939,\n",
      "                 0.1173,\n",
      "                 0.8008,\n",
      "                 0.1554,\n",
      "                 0.8855,\n",
      "                 0.7618,\n",
      "                 0.6724,\n",
      "                 1.1828,\n",
      "                 0.1447,\n",
      "                 1.2575,\n",
      "                 0.2337,\n",
      "                 0.8033,\n",
      "                 0.1865,\n",
      "                 0.0625,\n",
      "                 0.7377,\n",
      "                 0.5322,\n",
      "                 0.4645,\n",
      "                 0.4211,\n",
      "                 1.75,\n",
      "                 0.1546,\n",
      "                 0.8652,\n",
      "                 0.3028,\n",
      "                 0.496,\n",
      "                 0.3719,\n",
      "                 0.6658,\n",
      "                 0.6786,\n",
      "                 1.1459,\n",
      "                 0.6925,\n",
      "                 0.2315,\n",
      "                 0.5011,\n",
      "                 0.5674,\n",
      "                 0.8838,\n",
      "                 0.2085,\n",
      "                 0.5912,\n",
      "                 0.1462,\n",
      "                 0.8888,\n",
      "                 0.0677,\n",
      "                 0.2588,\n",
      "                 0.8715,\n",
      "                 0.4313,\n",
      "                 0.8584,\n",
      "                 0.5081,\n",
      "                 1.1994,\n",
      "                 0.3321,\n",
      "                 0.1182,\n",
      "                 0.775,\n",
      "                 1.0599,\n",
      "                 0.5911,\n",
      "                 0.2084,\n",
      "                 0.646,\n",
      "                 0.4111,\n",
      "                 0.8542,\n",
      "                 0.8483,\n",
      "                 0.9095,\n",
      "                 0.5609,\n",
      "                 0.1545,\n",
      "                 0.6096],\n",
      "  'val_loss': [0.5769524574279785,\n",
      "               0.571523904800415,\n",
      "               0.5654837489128113,\n",
      "               0.5601164102554321,\n",
      "               0.5549647212028503,\n",
      "               0.5524051785469055,\n",
      "               0.554253876209259,\n",
      "               0.5581508278846741,\n",
      "               0.5617882013320923,\n",
      "               0.5645330548286438,\n",
      "               0.5664270520210266,\n",
      "               0.5703614354133606,\n",
      "               0.5722862482070923,\n",
      "               0.5744714736938477,\n",
      "               0.574924647808075,\n",
      "               0.5769888162612915,\n",
      "               0.5806692838668823,\n",
      "               0.5845947265625,\n",
      "               0.587591826915741,\n",
      "               0.5884376764297485,\n",
      "               0.5907518267631531,\n",
      "               0.5906890630722046,\n",
      "               0.5922492146492004,\n",
      "               0.594488263130188,\n",
      "               0.5958887338638306,\n",
      "               0.5960021018981934,\n",
      "               0.5948814153671265,\n",
      "               0.5953174233436584,\n",
      "               0.5949594378471375,\n",
      "               0.5949690937995911,\n",
      "               0.594192624092102,\n",
      "               0.5948413610458374,\n",
      "               0.5959823131561279,\n",
      "               0.5958853960037231,\n",
      "               0.5957350134849548,\n",
      "               0.5963733792304993,\n",
      "               0.5968917012214661,\n",
      "               0.5969058871269226,\n",
      "               0.5969979166984558,\n",
      "               0.5970695614814758]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [0.659,\n",
      "                 1.1929,\n",
      "                 1.0536,\n",
      "                 1.5913,\n",
      "                 2.2156,\n",
      "                 0.9219,\n",
      "                 0.9931,\n",
      "                 1.2457,\n",
      "                 0.7819,\n",
      "                 1.905,\n",
      "                 1.166,\n",
      "                 1.2131,\n",
      "                 1.1189,\n",
      "                 2.4542,\n",
      "                 1.1825,\n",
      "                 0.4302,\n",
      "                 0.8368,\n",
      "                 0.7151,\n",
      "                 1.7273,\n",
      "                 0.9969,\n",
      "                 2.2439,\n",
      "                 1.4457,\n",
      "                 0.9064,\n",
      "                 0.9553,\n",
      "                 0.9284,\n",
      "                 0.48,\n",
      "                 0.914,\n",
      "                 1.5913,\n",
      "                 1.0614,\n",
      "                 0.8297,\n",
      "                 1.8552,\n",
      "                 1.2072,\n",
      "                 0.4791,\n",
      "                 1.3332,\n",
      "                 1.1599,\n",
      "                 1.295,\n",
      "                 1.4576,\n",
      "                 0.7251,\n",
      "                 1.486,\n",
      "                 1.1606,\n",
      "                 1.0171,\n",
      "                 1.4675,\n",
      "                 1.7966,\n",
      "                 1.0948,\n",
      "                 0.554,\n",
      "                 1.0638,\n",
      "                 0.6869,\n",
      "                 1.0303,\n",
      "                 1.5206,\n",
      "                 0.9702,\n",
      "                 0.7125,\n",
      "                 1.1812,\n",
      "                 0.5894,\n",
      "                 1.4223,\n",
      "                 0.8186,\n",
      "                 1.5516,\n",
      "                 0.9171,\n",
      "                 1.174,\n",
      "                 0.736,\n",
      "                 1.7653,\n",
      "                 1.3317,\n",
      "                 0.8371,\n",
      "                 1.5721,\n",
      "                 0.7547,\n",
      "                 0.981,\n",
      "                 1.5434,\n",
      "                 1.1861,\n",
      "                 0.7789,\n",
      "                 1.1579,\n",
      "                 1.273,\n",
      "                 1.1516,\n",
      "                 0.7745,\n",
      "                 0.6688,\n",
      "                 0.4647,\n",
      "                 1.2931,\n",
      "                 0.8996,\n",
      "                 1.3994,\n",
      "                 1.2228,\n",
      "                 1.0205,\n",
      "                 1.2324,\n",
      "                 1.808,\n",
      "                 1.093,\n",
      "                 1.1792,\n",
      "                 1.396,\n",
      "                 0.6779,\n",
      "                 0.7655,\n",
      "                 1.1299,\n",
      "                 0.9332,\n",
      "                 1.0378,\n",
      "                 1.3243,\n",
      "                 0.8613,\n",
      "                 0.5214,\n",
      "                 1.1933,\n",
      "                 0.4211,\n",
      "                 1.1843,\n",
      "                 1.4375,\n",
      "                 0.7819,\n",
      "                 0.6751,\n",
      "                 1.4212,\n",
      "                 0.8557,\n",
      "                 1.0121,\n",
      "                 0.9015,\n",
      "                 0.4094,\n",
      "                 1.0274,\n",
      "                 0.9965,\n",
      "                 1.2285,\n",
      "                 0.4634,\n",
      "                 0.9347,\n",
      "                 1.3887,\n",
      "                 0.958,\n",
      "                 1.1713,\n",
      "                 0.9813,\n",
      "                 1.8017,\n",
      "                 1.1006,\n",
      "                 0.8851,\n",
      "                 0.7144,\n",
      "                 1.315,\n",
      "                 0.8996,\n",
      "                 0.628,\n",
      "                 0.765,\n",
      "                 0.5114,\n",
      "                 1.4613,\n",
      "                 0.9197,\n",
      "                 0.4533,\n",
      "                 1.2934,\n",
      "                 0.8065,\n",
      "                 0.5154,\n",
      "                 1.1415,\n",
      "                 1.4113,\n",
      "                 0.8982,\n",
      "                 1.1017,\n",
      "                 0.4183,\n",
      "                 0.5239,\n",
      "                 0.8574,\n",
      "                 1.153,\n",
      "                 0.9535,\n",
      "                 0.8351,\n",
      "                 0.9037,\n",
      "                 0.6942,\n",
      "                 0.475,\n",
      "                 0.9587,\n",
      "                 1.1111,\n",
      "                 1.434,\n",
      "                 0.7363,\n",
      "                 0.7716,\n",
      "                 0.972,\n",
      "                 1.1366,\n",
      "                 0.8156,\n",
      "                 0.7839,\n",
      "                 1.1256,\n",
      "                 0.6522,\n",
      "                 0.5083,\n",
      "                 0.7606,\n",
      "                 1.3527,\n",
      "                 0.9908,\n",
      "                 0.8745,\n",
      "                 0.5226,\n",
      "                 0.8406,\n",
      "                 0.9333,\n",
      "                 0.8159,\n",
      "                 0.8941,\n",
      "                 0.9919,\n",
      "                 0.2751,\n",
      "                 0.9498,\n",
      "                 1.3217,\n",
      "                 0.6838,\n",
      "                 0.823,\n",
      "                 1.1074,\n",
      "                 0.2871,\n",
      "                 1.0429,\n",
      "                 0.823,\n",
      "                 1.1666,\n",
      "                 1.1253,\n",
      "                 0.8795,\n",
      "                 0.636,\n",
      "                 0.5376,\n",
      "                 0.9214,\n",
      "                 1.2703,\n",
      "                 0.5466,\n",
      "                 0.7983,\n",
      "                 1.21,\n",
      "                 0.7923,\n",
      "                 0.608,\n",
      "                 0.7328,\n",
      "                 0.5601,\n",
      "                 0.8875,\n",
      "                 0.896,\n",
      "                 0.4885,\n",
      "                 0.9963,\n",
      "                 1.3236,\n",
      "                 0.8844,\n",
      "                 0.816,\n",
      "                 0.778,\n",
      "                 0.6479,\n",
      "                 0.5834,\n",
      "                 0.4385,\n",
      "                 0.9178,\n",
      "                 0.3967,\n",
      "                 0.9619,\n",
      "                 0.7646,\n",
      "                 0.4872,\n",
      "                 0.6334,\n",
      "                 0.5424,\n",
      "                 0.9693,\n",
      "                 1.4352,\n",
      "                 0.5722,\n",
      "                 0.8809,\n",
      "                 0.9019,\n",
      "                 0.8261,\n",
      "                 0.8089,\n",
      "                 0.6098,\n",
      "                 1.1013,\n",
      "                 0.3837,\n",
      "                 0.8489,\n",
      "                 0.552,\n",
      "                 0.9436,\n",
      "                 1.4185,\n",
      "                 0.3671,\n",
      "                 0.4971,\n",
      "                 1.0311,\n",
      "                 0.828,\n",
      "                 0.8989,\n",
      "                 1.1921,\n",
      "                 0.5067,\n",
      "                 1.1283,\n",
      "                 0.5162,\n",
      "                 0.7008,\n",
      "                 0.7502,\n",
      "                 1.0253,\n",
      "                 1.0584,\n",
      "                 0.919,\n",
      "                 0.5825,\n",
      "                 0.7508,\n",
      "                 0.2614,\n",
      "                 0.396,\n",
      "                 0.6995,\n",
      "                 0.8503,\n",
      "                 0.9366,\n",
      "                 1.1415,\n",
      "                 0.63,\n",
      "                 0.4194,\n",
      "                 0.492,\n",
      "                 0.6081,\n",
      "                 0.8424,\n",
      "                 0.9339,\n",
      "                 0.8342,\n",
      "                 0.8275,\n",
      "                 0.8633,\n",
      "                 0.7811,\n",
      "                 0.4234,\n",
      "                 0.711,\n",
      "                 0.9075,\n",
      "                 1.2845,\n",
      "                 0.5421,\n",
      "                 0.9681,\n",
      "                 0.5577,\n",
      "                 0.7291,\n",
      "                 0.5001,\n",
      "                 0.4754,\n",
      "                 0.6582,\n",
      "                 0.485,\n",
      "                 0.6159,\n",
      "                 0.9805,\n",
      "                 0.6704,\n",
      "                 0.5051,\n",
      "                 0.7523,\n",
      "                 0.5462,\n",
      "                 0.4969,\n",
      "                 1.0893,\n",
      "                 0.5488,\n",
      "                 1.305,\n",
      "                 0.8952,\n",
      "                 1.1809,\n",
      "                 0.8678,\n",
      "                 0.671,\n",
      "                 1.1902,\n",
      "                 0.5953,\n",
      "                 0.6492,\n",
      "                 0.8049,\n",
      "                 0.8967,\n",
      "                 0.5749,\n",
      "                 0.9845,\n",
      "                 0.8205,\n",
      "                 0.6061,\n",
      "                 0.2594,\n",
      "                 0.7496,\n",
      "                 0.8889,\n",
      "                 0.8783,\n",
      "                 1.1103,\n",
      "                 0.501,\n",
      "                 1.0438,\n",
      "                 0.7528,\n",
      "                 0.2751,\n",
      "                 0.8201,\n",
      "                 0.9892,\n",
      "                 0.4909,\n",
      "                 0.5682,\n",
      "                 0.4638,\n",
      "                 0.6615,\n",
      "                 0.8002,\n",
      "                 0.6387,\n",
      "                 0.3811,\n",
      "                 0.643,\n",
      "                 1.0277,\n",
      "                 0.6908,\n",
      "                 1.0988,\n",
      "                 0.5146,\n",
      "                 0.8408,\n",
      "                 0.6025,\n",
      "                 0.6788,\n",
      "                 0.489,\n",
      "                 0.573,\n",
      "                 0.4116,\n",
      "                 0.6745,\n",
      "                 0.7278,\n",
      "                 0.6541,\n",
      "                 0.9552,\n",
      "                 0.9046,\n",
      "                 0.4966,\n",
      "                 0.616],\n",
      "  'val_loss': [0.5788252353668213,\n",
      "               0.5811716318130493,\n",
      "               0.5825923681259155,\n",
      "               0.5805476903915405,\n",
      "               0.5817404985427856,\n",
      "               0.5807759165763855,\n",
      "               0.5814650058746338,\n",
      "               0.5851989388465881,\n",
      "               0.5847878456115723,\n",
      "               0.5811498761177063,\n",
      "               0.5795871019363403,\n",
      "               0.5848042368888855,\n",
      "               0.5862478613853455,\n",
      "               0.5838801264762878,\n",
      "               0.5844146609306335,\n",
      "               0.5821942090988159,\n",
      "               0.5890134572982788,\n",
      "               0.5918096303939819,\n",
      "               0.5977479219436646,\n",
      "               0.5972703695297241,\n",
      "               0.5966426134109497,\n",
      "               0.5948887467384338,\n",
      "               0.5970975160598755,\n",
      "               0.5930511951446533,\n",
      "               0.596218466758728,\n",
      "               0.5964746475219727,\n",
      "               0.6003320813179016,\n",
      "               0.6040111780166626,\n",
      "               0.6090005040168762,\n",
      "               0.6102169156074524,\n",
      "               0.613667368888855,\n",
      "               0.615081250667572,\n",
      "               0.617088794708252,\n",
      "               0.6178765296936035,\n",
      "               0.6185828447341919,\n",
      "               0.619225800037384,\n",
      "               0.6193019151687622,\n",
      "               0.6195178627967834,\n",
      "               0.6198326945304871,\n",
      "               0.6200647354125977]},\n",
      " {'model_name': 'opt-350m',\n",
      "  'sample_size': 16,\n",
      "  'train_loss': [1.5208,\n",
      "                 1.2238,\n",
      "                 0.8949,\n",
      "                 0.8002,\n",
      "                 0.3429,\n",
      "                 1.9946,\n",
      "                 0.9858,\n",
      "                 1.915,\n",
      "                 1.6351,\n",
      "                 2.151,\n",
      "                 2.0867,\n",
      "                 0.7422,\n",
      "                 1.5228,\n",
      "                 0.2941,\n",
      "                 1.0697,\n",
      "                 0.838,\n",
      "                 1.3036,\n",
      "                 1.9965,\n",
      "                 0.7202,\n",
      "                 0.5975,\n",
      "                 1.0026,\n",
      "                 0.8469,\n",
      "                 1.7119,\n",
      "                 1.0038,\n",
      "                 2.0627,\n",
      "                 0.8719,\n",
      "                 0.3675,\n",
      "                 1.0462,\n",
      "                 0.7726,\n",
      "                 1.7711,\n",
      "                 0.3986,\n",
      "                 2.2436,\n",
      "                 0.9132,\n",
      "                 1.2709,\n",
      "                 0.8741,\n",
      "                 0.7353,\n",
      "                 1.5555,\n",
      "                 0.5692,\n",
      "                 2.0691,\n",
      "                 1.1803,\n",
      "                 1.8836,\n",
      "                 0.2821,\n",
      "                 0.5821,\n",
      "                 1.2137,\n",
      "                 0.5983,\n",
      "                 1.5942,\n",
      "                 0.697,\n",
      "                 1.0241,\n",
      "                 1.9089,\n",
      "                 0.5446,\n",
      "                 1.3352,\n",
      "                 0.6236,\n",
      "                 1.1359,\n",
      "                 0.6174,\n",
      "                 1.4209,\n",
      "                 0.6767,\n",
      "                 0.8921,\n",
      "                 1.7102,\n",
      "                 1.5205,\n",
      "                 0.2086,\n",
      "                 1.1056,\n",
      "                 1.056,\n",
      "                 0.455,\n",
      "                 0.9681,\n",
      "                 1.4285,\n",
      "                 0.1326,\n",
      "                 0.8575,\n",
      "                 1.819,\n",
      "                 1.7419,\n",
      "                 1.3813,\n",
      "                 0.219,\n",
      "                 0.5897,\n",
      "                 1.0818,\n",
      "                 1.0105,\n",
      "                 0.1783,\n",
      "                 1.7947,\n",
      "                 0.0973,\n",
      "                 0.5959,\n",
      "                 1.3047,\n",
      "                 0.9068,\n",
      "                 0.1771,\n",
      "                 1.1691,\n",
      "                 1.0624,\n",
      "                 1.2588,\n",
      "                 0.5231,\n",
      "                 2.2396,\n",
      "                 0.7152,\n",
      "                 0.508,\n",
      "                 0.4825,\n",
      "                 1.5443,\n",
      "                 0.6188,\n",
      "                 0.4446,\n",
      "                 0.841,\n",
      "                 1.0443,\n",
      "                 1.6073,\n",
      "                 0.8106,\n",
      "                 0.6199,\n",
      "                 1.2057,\n",
      "                 0.851,\n",
      "                 0.3976,\n",
      "                 1.4379,\n",
      "                 1.574,\n",
      "                 0.4834,\n",
      "                 0.3008,\n",
      "                 0.7845,\n",
      "                 0.6323,\n",
      "                 0.8763,\n",
      "                 0.7556,\n",
      "                 0.1232,\n",
      "                 0.9874,\n",
      "                 0.3726,\n",
      "                 1.7552,\n",
      "                 0.5074,\n",
      "                 0.9979,\n",
      "                 0.1925,\n",
      "                 0.651,\n",
      "                 1.5723,\n",
      "                 1.0546,\n",
      "                 1.1078,\n",
      "                 0.8607,\n",
      "                 1.6446,\n",
      "                 0.1511,\n",
      "                 1.6126,\n",
      "                 1.0666,\n",
      "                 0.364,\n",
      "                 0.9072,\n",
      "                 0.1087,\n",
      "                 1.4018,\n",
      "                 0.1064,\n",
      "                 1.8746,\n",
      "                 0.5733,\n",
      "                 0.493,\n",
      "                 2.1826,\n",
      "                 0.7402,\n",
      "                 0.7057,\n",
      "                 0.5011,\n",
      "                 0.6193,\n",
      "                 1.1029,\n",
      "                 0.14,\n",
      "                 0.3781,\n",
      "                 0.6769,\n",
      "                 0.6212,\n",
      "                 0.6761,\n",
      "                 2.0776,\n",
      "                 1.5417,\n",
      "                 0.5814,\n",
      "                 0.9394,\n",
      "                 0.3464,\n",
      "                 1.1056,\n",
      "                 0.0892,\n",
      "                 0.1003,\n",
      "                 1.1076,\n",
      "                 0.6976,\n",
      "                 0.1203,\n",
      "                 0.561,\n",
      "                 1.3502,\n",
      "                 1.6342,\n",
      "                 0.3099,\n",
      "                 1.1106,\n",
      "                 0.4217,\n",
      "                 0.1338,\n",
      "                 0.3437,\n",
      "                 1.4365,\n",
      "                 0.8154,\n",
      "                 0.5113,\n",
      "                 0.7433,\n",
      "                 1.322,\n",
      "                 0.6701,\n",
      "                 1.1855,\n",
      "                 0.1814,\n",
      "                 0.5362,\n",
      "                 1.2,\n",
      "                 1.2501,\n",
      "                 0.358,\n",
      "                 0.0926,\n",
      "                 1.1571,\n",
      "                 0.3824,\n",
      "                 0.2287,\n",
      "                 0.9487,\n",
      "                 0.3608,\n",
      "                 0.2318,\n",
      "                 2.3315,\n",
      "                 1.3327,\n",
      "                 0.4678,\n",
      "                 0.5386,\n",
      "                 0.8041,\n",
      "                 0.6356,\n",
      "                 0.7625,\n",
      "                 0.4408,\n",
      "                 0.174,\n",
      "                 0.3561,\n",
      "                 1.7797,\n",
      "                 0.857,\n",
      "                 0.1951,\n",
      "                 2.0727,\n",
      "                 0.5151,\n",
      "                 0.1181,\n",
      "                 0.9869,\n",
      "                 0.7919,\n",
      "                 0.6985,\n",
      "                 0.8941,\n",
      "                 0.1449,\n",
      "                 0.5566,\n",
      "                 0.1495,\n",
      "                 0.4678,\n",
      "                 1.4121,\n",
      "                 0.3402,\n",
      "                 1.0115,\n",
      "                 1.8088,\n",
      "                 0.9694,\n",
      "                 0.5464,\n",
      "                 0.0452,\n",
      "                 1.1479,\n",
      "                 0.1984,\n",
      "                 0.1084,\n",
      "                 0.3922,\n",
      "                 0.203,\n",
      "                 0.2408,\n",
      "                 1.6455,\n",
      "                 0.7057,\n",
      "                 0.849,\n",
      "                 0.5941,\n",
      "                 0.3352,\n",
      "                 0.4684,\n",
      "                 0.5943,\n",
      "                 0.7139,\n",
      "                 0.9686,\n",
      "                 0.0758,\n",
      "                 1.4023,\n",
      "                 0.4974,\n",
      "                 0.8086,\n",
      "                 0.7557,\n",
      "                 0.06,\n",
      "                 1.0236,\n",
      "                 0.868,\n",
      "                 0.0875,\n",
      "                 0.6337,\n",
      "                 0.4777,\n",
      "                 1.0849,\n",
      "                 1.2604,\n",
      "                 1.9583,\n",
      "                 0.0959,\n",
      "                 0.1643,\n",
      "                 1.0315,\n",
      "                 0.4292,\n",
      "                 1.2334,\n",
      "                 0.4719,\n",
      "                 0.121,\n",
      "                 0.3715,\n",
      "                 0.9488,\n",
      "                 0.7334,\n",
      "                 0.4744,\n",
      "                 0.0901,\n",
      "                 1.5597,\n",
      "                 0.8822,\n",
      "                 0.3639,\n",
      "                 0.4594,\n",
      "                 0.5943,\n",
      "                 1.1733,\n",
      "                 0.2401,\n",
      "                 0.0869,\n",
      "                 1.0631,\n",
      "                 0.035,\n",
      "                 1.4994,\n",
      "                 1.6517,\n",
      "                 0.689,\n",
      "                 0.414,\n",
      "                 0.1379,\n",
      "                 0.1914,\n",
      "                 0.812,\n",
      "                 0.3619,\n",
      "                 1.2202,\n",
      "                 0.0836,\n",
      "                 0.4229,\n",
      "                 0.1259,\n",
      "                 0.731,\n",
      "                 1.2744,\n",
      "                 0.7124,\n",
      "                 1.0217,\n",
      "                 0.2856,\n",
      "                 0.2286,\n",
      "                 0.0887,\n",
      "                 1.263,\n",
      "                 0.14,\n",
      "                 0.8343,\n",
      "                 0.5604,\n",
      "                 0.7647,\n",
      "                 1.1336,\n",
      "                 0.562,\n",
      "                 0.2618,\n",
      "                 0.4895,\n",
      "                 0.9937,\n",
      "                 1.861,\n",
      "                 0.3435,\n",
      "                 0.0977,\n",
      "                 1.1942,\n",
      "                 0.6099,\n",
      "                 1.4943,\n",
      "                 0.0706,\n",
      "                 0.557,\n",
      "                 0.9601,\n",
      "                 0.5375,\n",
      "                 0.1196,\n",
      "                 0.7505,\n",
      "                 0.3116,\n",
      "                 0.0505,\n",
      "                 1.128,\n",
      "                 0.3841,\n",
      "                 0.5253,\n",
      "                 0.5613,\n",
      "                 1.1203,\n",
      "                 1.19,\n",
      "                 0.27,\n",
      "                 0.8163,\n",
      "                 0.5366,\n",
      "                 0.5166,\n",
      "                 0.4261,\n",
      "                 0.3892,\n",
      "                 1.0531,\n",
      "                 1.1089],\n",
      "  'val_loss': [0.5765315294265747,\n",
      "               0.5717545747756958,\n",
      "               0.5700541138648987,\n",
      "               0.5613851547241211,\n",
      "               0.5577799081802368,\n",
      "               0.5547692775726318,\n",
      "               0.5516780614852905,\n",
      "               0.5485532283782959,\n",
      "               0.5458609461784363,\n",
      "               0.5441464185714722,\n",
      "               0.542737603187561,\n",
      "               0.5419324040412903,\n",
      "               0.5408005118370056,\n",
      "               0.5393007397651672,\n",
      "               0.5393214225769043,\n",
      "               0.5374678373336792,\n",
      "               0.5357578992843628,\n",
      "               0.5350523591041565,\n",
      "               0.5351155400276184,\n",
      "               0.5343121886253357,\n",
      "               0.5339641571044922,\n",
      "               0.5337988138198853,\n",
      "               0.5337890386581421,\n",
      "               0.5333872437477112,\n",
      "               0.5338934063911438,\n",
      "               0.5346373319625854,\n",
      "               0.5351365208625793,\n",
      "               0.5357660055160522,\n",
      "               0.5352624654769897,\n",
      "               0.5339688062667847,\n",
      "               0.5333048105239868,\n",
      "               0.5328525304794312,\n",
      "               0.5330957770347595,\n",
      "               0.5331546068191528,\n",
      "               0.5332411527633667,\n",
      "               0.5333898663520813,\n",
      "               0.5334507822990417,\n",
      "               0.5332896709442139,\n",
      "               0.5331941843032837,\n",
      "               0.533146858215332]}]\n"
     ]
    }
   ],
   "source": [
    "from src.finetuners.fewshot_lora import batch_fine_tune\n",
    "\n",
    "metrics, training_histories = batch_fine_tune(model_names=['opt-125m', 'opt-350m'], \n",
    "                                              train_datasets=train_datasets, \n",
    "                                              eval_dataset_in=eval_dataset_in, \n",
    "                                              eval_dataset_out=eval_dataset_out, \n",
    "                                              exp_label='final', \n",
    "                                              save_trials=False)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)\n",
    "print(\"Training histories:\")\n",
    "pprint.pprint(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-context learning (ICL)\n",
    "\n",
    "ICL is performed similarly to zero-shot evaluation, using the `generate` method. Context (labeled training examples) is pre-pended to each evaluation example. Model parameters are not updated using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.incontext import batch_evaluate\n",
    "\n",
    "metrics = batch_evaluate(model_names=['opt-125m', 'opt-350m'], \n",
    "                         train_datasets=train_datasets, \n",
    "                         eval_dataset_in=eval_dataset_in, \n",
    "                         eval_dataset_out=eval_dataset_out, \n",
    "                         exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-distillation fine-tuning\n",
    "TODO: add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82c4273c4c4441c90e1ccd2ed35fc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-125m 16-shot:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb135073a343499c93c359c73c8112e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-125m 4096-shot:   0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28ea08d0ed641188c7d8f3825c93085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 16-shot:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959f04b7bac2479fa30e710f4fd4b2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 4096-shot:   0%|          | 0/2049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.finetuners.context_distillation import batch_context_distillation\n",
    "from src.data.data import get_context_distillation_datasets\n",
    "\n",
    "context_distillation_datasets = get_context_distillation_datasets(dataset=in_domain_train,\n",
    "                                                                  train_datasets=train_datasets,\n",
    "                                                                  fewshot_sample_size=16,\n",
    "                                                                  large_sample_size=4096)\n",
    "\n",
    "metrics = batch_context_distillation(model_names=['opt-125m', 'opt-350m'],\n",
    "                                     in_domain_dataset=in_domain_train,\n",
    "                                     train_datasets=context_distillation_datasets,\n",
    "                                     eval_dataset_in=eval_dataset_in,\n",
    "                                     eval_dataset_out=eval_dataset_out,\n",
    "                                     exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive context-distillation fine-tuning\n",
    "TODO: add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ba4db7994a46a381858928b0d79616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 16-shot:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecbf2f188f347f083d518e53b264fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 4096-shot:   0%|          | 0/2049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "{'eval_in_accuracy': 0.56,\n",
      " 'eval_in_loss': 0.7520318841934204,\n",
      " 'eval_in_peak_memory_gb': 7.291062355041504,\n",
      " 'eval_in_runtime': 19.606728076934814,\n",
      " 'eval_in_samples_per_second': 2.5501450218417405,\n",
      " 'eval_out_accuracy': 0.52,\n",
      " 'eval_out_loss': 0.6991815215349197,\n",
      " 'eval_out_peak_memory_gb': 7.291062355041504,\n",
      " 'eval_out_runtime': 18.990807056427002,\n",
      " 'eval_out_samples_per_second': 2.6328528246027676,\n",
      " 'model_name': 'opt-350m',\n",
      " 'sample_size': '50'}\n"
     ]
    }
   ],
   "source": [
    "from src.finetuners.context_distillation_recursive import batch_recursive_context_distillation\n",
    "from src.data.data import get_context_distillation_datasets\n",
    "\n",
    "context_distillation_datasets = get_context_distillation_datasets(dataset=in_domain_train,\n",
    "                                                                  train_datasets=train_datasets,\n",
    "                                                                  fewshot_sample_size=16,\n",
    "                                                                  large_sample_size=4096)\n",
    "\n",
    "batch_recursive_context_distillation(model_names=['opt-350m'],\n",
    "                                     in_domain_dataset=in_domain_train,\n",
    "                                     train_datasets=context_distillation_datasets,\n",
    "                                     eval_dataset_in=eval_dataset_in,\n",
    "                                     eval_dataset_out=eval_dataset_out,\n",
    "                                     exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot in-domain vs. out-of-domain metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plot import plot_in_out_domain_subplots\n",
    "\n",
    "plot_in_out_domain_subplots(logfiles=['fewshot_metrics_final.csv',\n",
    "                                      'icl_metrics_final.csv',\n",
    "                                      'zeroshot_metrics_final.csv',\n",
    "                                      'fewshot_lora_metrics_final.csv',\n",
    "                                      'context_distillation_metrics_final.csv',\n",
    "                                      'recursive_context_distillation_metrics_final.csv'],\n",
    "                            metrics=['accuracy', 'runtime', 'peak_memory_gb', 'loss'],\n",
    "                            group_by='model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plot import plot_learning_curves\n",
    "\n",
    "plot_learning_curves(logfile='fewshot_training_history_final.csv', subplot=True)\n",
    "plot_learning_curves(logfile='fewshot_lora_training_history_final.csv', subplot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
