{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment notebook for \"Context is All You Need\"\n",
    "In this notebook we conduct experiments involving few-shot fine-tuning, in-context learning (ICL), and a novel implementation of context distillation.\n",
    "\n",
    "To install the development environment, run the following:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate fine-tuning\n",
    "```\n",
    "\n",
    "Run ```pip install -e .``` if module importing isn't working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the check below fails, verify your pytorch installation by following the steps at https://pytorch.org/get-started/locally/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\harri\\OneDrive\\Documents\\OMSCS\\CS 7643 DL\\Projects\\Group Project\\Efficient_LLM_Few-Example_Fine-Tuning\\experiments\\final_experiments.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda_check\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cuda_check()\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\utils.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mUtility functions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\__init__.py:123\u001b[0m\n\u001b[0;32m    121\u001b[0m is_loaded \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 123\u001b[0m     res \u001b[39m=\u001b[39m kernel32\u001b[39m.\u001b[39;49mLoadLibraryExW(dll, \u001b[39mNone\u001b[39;49;00m, \u001b[39m0x00001100\u001b[39;49m)\n\u001b[0;32m    124\u001b[0m     last_error \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mget_last_error()\n\u001b[0;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m last_error \u001b[39m!=\u001b[39m \u001b[39m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.utils import cuda_check\n",
    "\n",
    "cuda_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets\n",
    "\n",
    "Import datasets using methods from `src/data/data.py`. Datasets are downloaded from huggingface and stored in `/data`. Once downloaded, datasets are loaded locally.\n",
    "\n",
    "Our in domain dataset is [MNLI](https://huggingface.co/datasets/glue). Our out of domain dataset is [HANS](https://huggingface.co/datasets/hans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In domain (MNLI):\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 261802\n",
      "})\n",
      "{'hypothesis': 'A member of my team will execute your orders with immense '\n",
      "               'precision.',\n",
      " 'idx': 2,\n",
      " 'label': 0,\n",
      " 'premise': 'One of our number will carry out your instructions minutely.'}\n",
      "\n",
      "Out of domain (HANS):\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'binary_parse_hypothesis': '( ( The athlete ) ( ( avoided ( the president ) ) '\n",
      "                            '. ) )',\n",
      " 'binary_parse_premise': '( ( The president ) ( ( avoided ( the athlete ) ) . '\n",
      "                         ') )',\n",
      " 'heuristic': 'lexical_overlap',\n",
      " 'hypothesis': 'The athlete avoided the president .',\n",
      " 'label': 1,\n",
      " 'parse_hypothesis': '(ROOT (S (NP (DT The) (NN athlete)) (VP (VBD avoided) '\n",
      "                     '(NP (DT the) (NN president))) (. .)))',\n",
      " 'parse_premise': '(ROOT (S (NP (DT The) (NN president)) (VP (VBD avoided) (NP '\n",
      "                  '(DT the) (NN athlete))) (. .)))',\n",
      " 'premise': 'The president avoided the athlete .',\n",
      " 'subcase': 'ln_subject/object_swap',\n",
      " 'template': 'temp1'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from src.data.data import get_in_domain, get_out_domain\n",
    "\n",
    "in_domain_train, in_domain_test = get_in_domain()\n",
    "out_domain = get_out_domain()\n",
    "\n",
    "print(f\"In domain (MNLI):\\n{in_domain_train}\")\n",
    "pprint.pprint(in_domain_train[1])\n",
    "\n",
    "print(f\"\\nOut of domain (HANS):\\n{out_domain}\")\n",
    "pprint.pprint(out_domain[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and evaluation datasets\n",
    "\n",
    "The `get_random_subsets` method from `src/data/data.py` creates a dictionary of training and evaluation data organized by sample size. Each sample size will contain 10 randomly generated trials of that sample size. Evaluation sets contain 50 samples and are randomly generated a single time to ensure consistant comparison across fine-tuning methods.\n",
    "\n",
    "Before generating our datasets, we set random seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets:\n",
      "{2: [...], 4: [...], 8: [...], 16: [...]}\n",
      "\n",
      "Eval datasets:\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from src.data.data import get_random_subsets\n",
    "\n",
    "# Seed random generators\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Generate training and evaluation datasets\n",
    "train_datasets, eval_dataset_in, eval_dataset_out = get_random_subsets(train_dataset=in_domain_train, \n",
    "                                                                       eval_dataset_in=in_domain_test, \n",
    "                                                                       eval_dataset_out=out_domain, \n",
    "                                                                       train_sample_sizes=[2, 4, 8, 16],\n",
    "                                                                       num_trials=10,\n",
    "                                                                       eval_sample_size=50)\n",
    "\n",
    "print(\"Train datasets:\")\n",
    "pprint.pprint(train_datasets, depth=1)\n",
    "print(\"\\nEval datasets:\")\n",
    "pprint.pprint(eval_dataset_in, depth=1)\n",
    "pprint.pprint(eval_dataset_out, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n",
    "\n",
    "Import models using methods from `src/models/opt.py`. Models are downloaded from huggingface and stored in `/models/pretrained`. Once downloaded, models are loaded locally. In the following experiments, model loading is handled in the scope of each fine-tuning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model.model import get_model\n",
    "\n",
    "# # Get SequenceClassification models\n",
    "# model_opt125, tokenizer_opt125 = get_model(model_name='opt-125m', model_type='SequenceClassification', pretrained=True)\n",
    "# model_opt350, tokenizer_opt350 = get_model(model_name='opt-350m', model_type='SequenceClassification', pretrained=True)\n",
    "\n",
    "# # Get CasualLM models\n",
    "# model_opt125_causal, tokenizer_opt125_causal = get_model(model_name='opt-125m', model_type='CausalLM', pretrained=True)\n",
    "# model_opt350_causal, tokenizer_opt350_causal = get_model(model_name='opt-350m', model_type='CausalLM', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot baseline\n",
    "\n",
    "We evaluate both models on in and out of domain eval sets with no training or context using the `generate` method. These results serve as a baseline for comparison to other fine-tuning methods. Model parameters are not updated using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c2a8778bdb4435b77ad93dfa2b3cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f68f774dd1a416faaf83574d341f4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8384481ba71a4463823ae0e7a4c6eab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d980f8f78cc45aeb95506af023027de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "{'eval_in_accuracy': 0.56,\n",
      " 'eval_in_loss': 0.7520318841934204,\n",
      " 'eval_in_peak_memory_gb': 7.291062355041504,\n",
      " 'eval_in_runtime': 600.9849765300751,\n",
      " 'eval_in_samples_per_second': 0.08319675524783747,\n",
      " 'eval_out_accuracy': 0.52,\n",
      " 'eval_out_loss': 0.6991815215349197,\n",
      " 'eval_out_peak_memory_gb': 7.291062355041504,\n",
      " 'eval_out_runtime': 591.9345364570618,\n",
      " 'eval_out_samples_per_second': 0.0844688000454708,\n",
      " 'model_name': 'opt-350m',\n",
      " 'sample_size': '50'}\n"
     ]
    }
   ],
   "source": [
    "from src.finetuners.zeroshot import batch_evaluate\n",
    "\n",
    "metrics = batch_evaluate(model_names=['opt-125m', 'opt-350m'], \n",
    "                         eval_dataset_in=eval_dataset_in, \n",
    "                         eval_dataset_out=eval_dataset_out, \n",
    "                         exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot fine-tuning\n",
    "\n",
    "We fine-tune both models on 10 trials of training data and evaluate on in and out of domain eval sets. This method updates all model parameters. Fine-tuned models can be saved locally to `models/finetuned/` by setting the `save_trials` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcfebcbbc054bb691dd6b0805451732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-125m 2-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87110d142d084477bc3aa2c96c8a8270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-125m 4-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3331cd8cc143259e6d988b5eb71fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-125m 8-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cd60780fe24471b439293be34c9c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-125m 16-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789e2f86ea3b410497dfce89dbda8dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 2-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd437e1299bb42028ec1cc37c5e257e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "opt-350m 4-shot:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\harri\\OneDrive\\Documents\\OMSCS\\CS 7643 DL\\Projects\\Group Project\\Efficient_LLM_Few-Example_Fine-Tuning\\experiments\\final_experiments.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfinetuners\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfewshot\u001b[39;00m \u001b[39mimport\u001b[39;00m batch_fine_tune\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m metrics, training_histories \u001b[39m=\u001b[39m batch_fine_tune(model_names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mopt-125m\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mopt-350m\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                               train_datasets\u001b[39m=\u001b[39;49mtrain_datasets, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                               eval_dataset_in\u001b[39m=\u001b[39;49meval_dataset_in, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                               eval_dataset_out\u001b[39m=\u001b[39;49meval_dataset_out, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                               exp_label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfinal\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                               save_trials\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMetrics:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/harri/OneDrive/Documents/OMSCS/CS%207643%20DL/Projects/Group%20Project/Efficient_LLM_Few-Example_Fine-Tuning/experiments/final_experiments.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pprint\u001b[39m.\u001b[39mpprint(metrics)\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\fewshot.py:103\u001b[0m, in \u001b[0;36mbatch_fine_tune\u001b[1;34m(model_names, train_datasets, eval_dataset_in, eval_dataset_out, exp_label, save_trials)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m trial_num, dataset \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(progress_bar):\n\u001b[0;32m    102\u001b[0m     model, tokenizer \u001b[39m=\u001b[39m get_model(model_name, \u001b[39m'\u001b[39m\u001b[39mSequenceClassification\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Load original model from disk\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     metrics_trial, full_training_history \u001b[39m=\u001b[39m fine_tune(model\u001b[39m=\u001b[39;49mmodel, \n\u001b[0;32m    104\u001b[0m                                                      tokenizer\u001b[39m=\u001b[39;49mtokenizer, \n\u001b[0;32m    105\u001b[0m                                                      train_dataset\u001b[39m=\u001b[39;49mdataset, \n\u001b[0;32m    106\u001b[0m                                                      eval_dataset_in\u001b[39m=\u001b[39;49meval_dataset_in, \n\u001b[0;32m    107\u001b[0m                                                      eval_dataset_out\u001b[39m=\u001b[39;49meval_dataset_out, \n\u001b[0;32m    108\u001b[0m                                                      val_in_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[0;32m    109\u001b[0m                                                      verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m# Fine-tune\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     metrics_trial \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m: model_name,\n\u001b[0;32m    112\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39msample_size\u001b[39m\u001b[39m'\u001b[39m: sample_size,\n\u001b[0;32m    113\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmetrics_trial}\n\u001b[0;32m    114\u001b[0m     metrics\u001b[39m.\u001b[39mappend(metrics_trial)\n",
      "File \u001b[1;32mc:\\users\\harri\\onedrive\\documents\\omscs\\cs 7643 dl\\projects\\group project\\efficient_llm_few-example_fine-tuning\\src\\finetuners\\fewshot.py:75\u001b[0m, in \u001b[0;36mfine_tune\u001b[1;34m(model, tokenizer, train_dataset, eval_dataset_in, eval_dataset_out, val_in_training, verbose, disable_tqdm)\u001b[0m\n\u001b[0;32m     72\u001b[0m     trainer\u001b[39m.\u001b[39mremove_callback(PrinterCallback)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Train on in domain\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m train_output \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     76\u001b[0m train_metrics \u001b[39m=\u001b[39m train_output\u001b[39m.\u001b[39mmetrics\n\u001b[0;32m     78\u001b[0m \u001b[39m# Evaluate on in domain\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1898\u001b[0m ):\n\u001b[0;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\transformers\\trainer.py:2787\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m   2786\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2787\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m   2789\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\accelerate\\accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1989\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\harri\\anaconda3\\envs\\fine-tuning\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.finetuners.fewshot import batch_fine_tune\n",
    "\n",
    "metrics, training_histories = batch_fine_tune(model_names=['opt-125m', 'opt-350m'], \n",
    "                                              train_datasets=train_datasets, \n",
    "                                              eval_dataset_in=eval_dataset_in, \n",
    "                                              eval_dataset_out=eval_dataset_out, \n",
    "                                              exp_label='final', \n",
    "                                              save_trials=False)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)\n",
    "print(\"Training histories:\")\n",
    "pprint.pprint(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot with LoRA (Low Rank Adaptation)\n",
    "\n",
    "We fine-tune both models identically to few-shot fine-tuning, but with each model converted to a `LoraModel` for Parameter-Efficient Fine-tuning (PEFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.fewshot_lora import batch_fine_tune\n",
    "\n",
    "metrics, training_histories = batch_fine_tune(model_names=['opt-125m', 'opt-350m'], \n",
    "                                              train_datasets=train_datasets, \n",
    "                                              eval_dataset_in=eval_dataset_in, \n",
    "                                              eval_dataset_out=eval_dataset_out, \n",
    "                                              exp_label='final', \n",
    "                                              save_trials=False)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)\n",
    "print(\"Training histories:\")\n",
    "pprint.pprint(training_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-context learning (ICL)\n",
    "\n",
    "ICL is performed similarly to zero-shot evaluation, using the `generate` method. Context (labeled training examples) is pre-pended to each evaluation example. Model parameters are not updated using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.incontext import batch_evaluate\n",
    "\n",
    "metrics = batch_evaluate(model_names=['opt-125m', 'opt-350m'], \n",
    "                         train_datasets=train_datasets, \n",
    "                         eval_dataset_in=eval_dataset_in, \n",
    "                         eval_dataset_out=eval_dataset_out, \n",
    "                         exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-distillation fine-tuning\n",
    "TODO: add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.context_distillation import batch_context_distillation\n",
    "\n",
    "metrics = batch_context_distillation(model_names=['opt-125m', 'opt-350m'],\n",
    "                                     train_dataset=train_datasets[0], #TODO: need to sample again for this?\n",
    "                                     eval_dataset_in=eval_dataset_in, \n",
    "                                     eval_dataset_out=eval_dataset_out, \n",
    "                                     exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive context-distillation fine-tuning\n",
    "TODO: add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuners.context_distillation_recursive import batch_recursive_context_distillation\n",
    "\n",
    "metrics = batch_recursive_context_distillation(model_names=['opt-125m', 'opt-350m'],\n",
    "                                               train_dataset=train_datasets[0], #TODO: need to sample again for this?\n",
    "                                               eval_dataset_in=eval_dataset_in, \n",
    "                                               eval_dataset_out=eval_dataset_out, \n",
    "                                               exp_label='final')\n",
    "\n",
    "print(\"Metrics:\")\n",
    "pprint.pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot in-domain vs. out-of-domain metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plot import plot_in_out_domain_subplots\n",
    "\n",
    "plot_in_out_domain_subplots(logfiles=['fewshot_metrics_final.csv',\n",
    "                                      'icl_metrics_final.csv',\n",
    "                                      'zeroshot_metrics_final.csv',\n",
    "                                      'fewshot_lora_metrics_final.csv',\n",
    "                                      'context_distillation_metrics_final.csv',\n",
    "                                      'recursive_context_distillation_metrics.csv'],\n",
    "                            metrics=['accuracy', 'runtime', 'peak_memory_gb', 'loss'],\n",
    "                            group_by='model_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plot import plot_learning_curves\n",
    "\n",
    "plot_learning_curves(logfile='fewshot_training_history_final.csv', subplot=True)\n",
    "plot_learning_curves(logfile='fewshot_lora_training_history_final.csv', subplot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
